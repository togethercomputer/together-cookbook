{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Contextual RAG Workflow with Together AI on Union\n",
    "\n",
    "This notebook walks you through building a Contextual RAG (Retrieval-Augmented Generation) workflow on Union using Together's embedding, reranker, and chat models. It ties together web scraping, embedding generation, and serving into one cohesive application.\n",
    "\n",
    "[Union](https://www.union.ai/) is a production-grade AI orchestrator that unifies data, models, and compute workflows into a single, intuitive platform. It makes scalable, enterprise-ready AI accessible to all teams.\n",
    "\n",
    "In this notebook, we take the [existing Contextual RAG Together app](https://docs.together.ai/docs/how-to-implement-contextual-rag-from-anthropic) and make it \"production-grade\" with Union ‚Äî ready for enterprise deployment.\n",
    "\n",
    "<img src=\"../images/Union_serving.png\" alt=\"Description\" width=\"850\" height=\"600\">\n",
    "\n",
    "## Workflow overview\n",
    "\n",
    "The workflow follows these steps:\n",
    "\n",
    "1. Fetches all links to Paul Graham's essays.\n",
    "2. Scrapes web content to retrieve the full text of the essays.\n",
    "3. Splits the text into smaller chunks for processing.\n",
    "4. Appends context from the relevant essay to each chunk.\n",
    "5. Generates embeddings and stores them in a hosted vector database.\n",
    "6. Creates a keyword index for efficient retrieval.\n",
    "7. Serves a FastAPI app to expose the RAG functionality.\n",
    "8. Provides a Gradio app, using the FastAPI endpoint, for an easy-to-use RAG interface.\n",
    "\n",
    "Later in the notebook, we‚Äôll dive into the Union features that simplify this entire process.\n",
    "\n",
    "## Execution approach\n",
    "\n",
    "This workflow is designed for local execution first, allowing you to test and validate it before deploying and scaling it on a Union cluster. This staged approach ensures smooth transitions from development to production.\n",
    "\n",
    "Before running the workflow, make sure to install `union`:\n",
    "\n",
    "```\n",
    "pip install union\n",
    "```\n",
    "\n",
    "### Local execution\n",
    "\n",
    "First, we import the required dependencies to ensure the workflow runs smoothly. Next, we define an actor environment, as the workflow relies on actor tasks throughout the process.\n",
    "\n",
    "[Union Actors](https://docs.union.ai/serverless/user-guide/core-concepts/actors) let us reuse a container and its environment across tasks, avoiding the overhead of starting a new container for each task. In this workflow, we define a single actor and reuse it consistently since the underlying components don‚Äôt require independent scaling or separate environments.\n",
    "\n",
    "Within the actor environment, we specify the `ImageSpec`, which defines the container image that tasks in the workflow will use. With Union, every task runs in its own dedicated container, requiring a container image. Instead of manually creating a Dockerfile, we define the image specification in Python. When run on Union Serverless, the container image is built remotely, simplifying the setup.\n",
    "\n",
    "We also configure the actor‚Äôs replica count to 10, meaning 10 workers are provisioned to handle tasks, allowing up to 10 tasks to run in parallel, provided sufficient resources. The TTL (time to live) is set to 120 seconds, ensuring the actor remains active for this period when no tasks are being processed.\n",
    "\n",
    "Finally, we create a Pydantic `BaseModel` named `Document` to capture metadata for each document used by the RAG app. This model ensures consistent data structuring and smooth integration throughout the workflow.\n",
    "\n",
    "NOTE: Add your Together AI API key (`TOGETHER_API_KEY`) to the `.env` file before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Optional\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import flytekit as fl\n",
    "import numpy as np\n",
    "import requests\n",
    "import union\n",
    "from flytekit.core.artifact import Artifact\n",
    "from flytekit.exceptions.base import FlyteRecoverableException\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from flytekit.types.file import FlyteFile\n",
    "from pydantic import BaseModel\n",
    "from union.actor import ActorEnvironment\n",
    "\n",
    "actor = ActorEnvironment(\n",
    "    name=\"contextual-rag\",\n",
    "    replica_count=10,\n",
    "    ttl_seconds=120,\n",
    "    container_image=union.ImageSpec(\n",
    "        name=\"contextual-rag\",\n",
    "        packages=[\n",
    "            \"together==1.3.10\",\n",
    "            \"beautifulsoup4==4.12.3\",\n",
    "            \"bm25s==0.2.5\",\n",
    "            \"pydantic>2\",\n",
    "            \"pymilvus>=2.5.4\",\n",
    "            \"union>=0.1.139\",\n",
    "            \"flytekit>=1.15.0b5\",\n",
    "        ],\n",
    "    ),\n",
    "    secret_requests=[\n",
    "        fl.Secret(\n",
    "            key=\"together-api-key\", \n",
    "            env_var=\"TOGETHER_API_KEY\",\n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR,\n",
    "        ),\n",
    "        fl.Secret(\n",
    "            key=\"milvus-uri\",\n",
    "            env_var=\"MILVUS_URI\",\n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR,\n",
    "        ),\n",
    "        fl.Secret(\n",
    "            key=\"milvus-token\",\n",
    "            env_var=\"MILVUS_TOKEN\",\n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "class Document(BaseModel):\n",
    "    idx: int\n",
    "    title: str\n",
    "    url: str\n",
    "    content: Optional[str] = None\n",
    "    chunks: Optional[list[str]] = None\n",
    "    prompts: Optional[list[str]] = None\n",
    "    contextual_chunks: Optional[list[str]] = None\n",
    "    tokens: Optional[list[list[int]]] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by defining an actor task to parse the main page of Paul Graham's essays. This task extracts a list of document titles and their respective URLs. Since actor tasks run within the shared actor environment we set up earlier, they efficiently reuse the same container and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task\n",
    "def parse_main_page(\n",
    "    base_url: str, articles_url: str, local: bool = False\n",
    ") -> list[Document]:\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    assert base_url.endswith(\"/\"), f\"Base URL must end with a slash: {base_url}\"\n",
    "    response = requests.get(urljoin(base_url, articles_url))\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    td_cells = soup.select(\"table > tr > td > table > tr > td\")\n",
    "    documents = []\n",
    "\n",
    "    idx = 0\n",
    "    for td in td_cells:\n",
    "        img = td.find(\"img\")\n",
    "        if img and int(img.get(\"width\", 0)) <= 15 and int(img.get(\"height\", 0)) <= 15:\n",
    "            a_tag = td.find(\"font\").find(\"a\") if td.find(\"font\") else None\n",
    "            if a_tag:\n",
    "                documents.append(\n",
    "                    Document(\n",
    "                        idx=idx, title=a_tag.text, url=urljoin(base_url, a_tag[\"href\"])\n",
    "                    )\n",
    "                )\n",
    "                idx += 1\n",
    "\n",
    "    if local:\n",
    "        return documents[:3]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an actor task to scrape the content of each document. Using the list of URLs gathered in the previous step, this task extracts the full text of the essays, ensuring that all relevant content is retrieved for further processing.\n",
    "\n",
    "We also set `retries` to `3`, meaning the task will be retried three times before the error is propagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task(retries=3)\n",
    "def scrape_pg_essays(document: Document) -> Document:\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    try:\n",
    "        response = requests.get(document.url)\n",
    "    except Exception as e:\n",
    "        raise FlyteRecoverableException(f\"Failed to scrape {document.url}: {str(e)}\")\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    content = soup.find(\"font\")\n",
    "\n",
    "    text = None\n",
    "    if content:\n",
    "        text = \" \".join(content.get_text().split())\n",
    "    document.content = text\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define an actor task to create chunks for each document. Chunks are necessary because we need to append context to each chunk, ensuring the RAG app can process the information effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task(cache=True, cache_version=\"0.2\")\n",
    "def create_chunks(document: Document, chunk_size: int, overlap: int) -> Document:\n",
    "    if document.content:\n",
    "        content_chunks = [\n",
    "            document.content[i : i + chunk_size]\n",
    "            for i in range(0, len(document.content), chunk_size - overlap)\n",
    "        ]\n",
    "        document.chunks = content_chunks\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use Together AI to generate context for each chunk of text, using the secret we initialized earlier. The system retrieves relevant context based on the entire document, ensuring accurate and meaningful outputs.\n",
    "\n",
    "Notice that we set [`cache`](https://docs.union.ai/serverless/user-guide/core-concepts/caching) to `True` for this task to avoid re-running the execution for the same inputs. This ensures that if the document and model remain unchanged, the outputs are retrieved directly from the cache, improving efficiency.\n",
    "\n",
    "Once the context is generated, we map the chunks back to their respective documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task(cache=True, cache_version=\"0.4\")\n",
    "def generate_context(document: Document, model: str) -> Document:\n",
    "    from together import Together\n",
    "\n",
    "    CONTEXTUAL_RAG_PROMPT = \"\"\"\n",
    "Given the document below, we want to explain what the chunk captures in the document.\n",
    "\n",
    "{WHOLE_DOCUMENT}\n",
    "\n",
    "Here is the chunk we want to explain:\n",
    "\n",
    "{CHUNK_CONTENT}\n",
    "\n",
    "Answer ONLY with a succinct explanation of the meaning of the chunk in the context of the whole document above.\n",
    "\"\"\"\n",
    "\n",
    "    client = Together(api_key=os.getenv(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "    contextual_chunks = [\n",
    "        f\"{response.choices[0].message.content} {chunk}\"\n",
    "        for chunk in (document.chunks or [])\n",
    "        for response in [\n",
    "            client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": CONTEXTUAL_RAG_PROMPT.format(\n",
    "                            WHOLE_DOCUMENT=document.content,\n",
    "                            CHUNK_CONTENT=chunk,\n",
    "                        ),\n",
    "                    }\n",
    "                ],\n",
    "                temperature=1,\n",
    "            )\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Assign the contextual chunks back to the document\n",
    "    document.contextual_chunks = contextual_chunks if contextual_chunks else None\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an embedding function to generate embeddings for each chunk. This function converts the chunks into vector representations, which we can store in a vector database for efficient retrieval and processing.\n",
    "\n",
    "Next, we create a vector index and store the embeddings in the [Milvus](https://milvus.io/) vector database. For each embedding, we store the ID, document, and document title. These details ensure the embeddings are ready for efficient retrieval during the RAG process.\n",
    "\n",
    "By setting `cache` to `True`, we avoid redundant upserts or inserts for the same document. Instead, we can add new records or update existing ones only if the content has changed. This approach keeps the vector database up-to-date efficiently, minimizing resource usage while maintaining accuracy.\n",
    "\n",
    "Note: We're using the Milvus hosted vector database to store the embeddings. However, you can replace it with any vector database of your choice based on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "def get_embedding(chunk: str, model_api_string: str):\n",
    "    client = Together(\n",
    "        api_key=os.getenv(\"TOGETHER_API_KEY\")\n",
    "    )\n",
    "    outputs = client.embeddings.create(\n",
    "        input=chunk,\n",
    "        model=model_api_string,\n",
    "    )\n",
    "    return outputs.data[0].embedding\n",
    "\n",
    "\n",
    "@actor.task(cache=True, cache_version=\"0.19\", retries=5)\n",
    "def create_vector_index(\n",
    "    document: Document, model_api_string: str, local: bool = False\n",
    ") -> Document:\n",
    "    from pymilvus import DataType, MilvusClient\n",
    "\n",
    "    if local:\n",
    "        client = MilvusClient(\"test_milvus.db\")\n",
    "    else:\n",
    "        try:\n",
    "            client = MilvusClient(uri=os.getenv(\"MILVUS_URI\"), token=os.getenv(\"MILVUS_TOKEN\"))\n",
    "        except Exception as e:\n",
    "            raise FlyteRecoverableException(\n",
    "                f\"Failed to connect to Milvus: {e}\"\n",
    "            )\n",
    "\n",
    "    collection_name = \"paul_graham_collection\"\n",
    "\n",
    "    if not client.has_collection(collection_name):\n",
    "        schema = client.create_schema()\n",
    "        schema.add_field(\n",
    "            \"id\", DataType.INT64, is_primary=True, auto_id=True\n",
    "        )\n",
    "        schema.add_field(\"document_index\", DataType.VARCHAR, max_length=255)\n",
    "        schema.add_field(\"embedding\", DataType.FLOAT_VECTOR, dim=1024)\n",
    "        schema.add_field(\"title\", DataType.VARCHAR, max_length=255)\n",
    "        index_params = client.prepare_index_params()\n",
    "        index_params.add_index(\"embedding\", metric_type=\"COSINE\")\n",
    "\n",
    "        client.create_collection(collection_name, dimension=512, schema=schema, index_params=index_params)\n",
    "\n",
    "    if not document.contextual_chunks:\n",
    "        return document  # Exit early if there are no contextual chunks\n",
    "    \n",
    "    # Generate embeddings for chunks\n",
    "    embeddings = [get_embedding(chunk[:512], model_api_string) for chunk in document.contextual_chunks] # NOTE: Trimming the chunk for the embedding model's context window\n",
    "    embeddings_np = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "    ids = [\n",
    "        f\"id{document.idx}_{chunk_idx}\"\n",
    "        for chunk_idx, _ in enumerate(document.contextual_chunks)\n",
    "    ]\n",
    "    titles = [document.title] * len(document.contextual_chunks)\n",
    "\n",
    "    client.upsert(\n",
    "        collection_name,\n",
    "        [\n",
    "            {\"id\": index, \"document_index\": document_index, \"embedding\": embedding, \"title\": title}\n",
    "            for index, (document_index, embedding, title) in enumerate(zip(ids, embeddings_np.tolist(), titles))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create a BM25S keyword index to organize the document chunks. This index is great for keyword-based searches and works well alongside vector indexing. We also store a mapping between document IDs and their corresponding contextual chunk data, making it easier to retrieve content during the RAG process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@actor.task(cache=True, cache_version=\"0.5\")\n",
    "def create_bm25s_index(documents: list[Document]) -> tuple[FlyteDirectory, FlyteFile]:\n",
    "    import json\n",
    "    import bm25s\n",
    "\n",
    "    # Prepare data for JSON\n",
    "    data = {\n",
    "        f\"id{doc_idx}_{chunk_idx}\": contextual_chunk\n",
    "        for doc_idx, document in enumerate(documents)\n",
    "        if document.contextual_chunks\n",
    "        for chunk_idx, contextual_chunk in enumerate(document.contextual_chunks)\n",
    "    }\n",
    "\n",
    "    retriever = bm25s.BM25(corpus=list(data.values()))\n",
    "    retriever.index(bm25s.tokenize(list(data.values())))\n",
    "\n",
    "    ctx = union.current_context()\n",
    "    working_dir = Path(ctx.working_directory)\n",
    "    bm25s_index_dir = working_dir / \"bm25s_index\"\n",
    "    contextual_chunks_json = working_dir / \"contextual_chunks.json\"\n",
    "\n",
    "    retriever.save(str(bm25s_index_dir))\n",
    "\n",
    "    # Write the data to a JSON file\n",
    "    with open(contextual_chunks_json, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return FlyteDirectory(path=bm25s_index_dir), FlyteFile(contextual_chunks_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a [standard workflow](https://docs.union.ai/serverless/user-guide/core-concepts/workflows/standard-workflows) to execute these tasks in sequence. By using [map tasks](https://docs.union.ai/serverless/user-guide/core-concepts/tasks/task-types#map-tasks), we run operations in parallel while respecting the resource constraints of each task. This approach **significantly improves execution speed**. We set the concurrency to 2, meaning two tasks will run in parallel. Note that the replica count for actors is set to 10, but this can be overridden at the map task level. We're doing this because having too many parallel clients could cause server availability issues.\n",
    "\n",
    "The final output of this workflow includes the BM25S keyword index and the contextual chunks mapping file, both returned as [Union Artifacts](https://docs.union.ai/serverless/user-guide/core-concepts/artifacts). The Artifact Service automatically indexes and assigns semantic meaning to all outputs from Union tasks and workflow executions, such as models, files, or other data. This makes it easy to track, access, and orchestrate pipelines directly through their outputs. In this case, the keyword index and file artifacts are directly used during app serving.\n",
    "\n",
    "We also set up a retrieval task to fetch embeddings for local execution. Once everything‚Äôs in place, we run the workflow and the retrieval task locally, producing a set of relevant chunks.\n",
    "\n",
    "One advantage of running locally is that all tasks and workflows are Python functions, making it easy to test everything before moving to production. This approach allows you to experiment locally and then deploy the same workflow in a production environment, ensuring it‚Äôs production-ready. You get the flexibility to test and refine your workflow without compromising on the capabilities needed for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Ensure the secret (together API key) is present in the .env file\n",
    "\n",
    "BM25Index = Artifact(name=\"bm25s-index\")\n",
    "ContextualChunksJSON = Artifact(name=\"contextual-chunks-json\")\n",
    "\n",
    "\n",
    "@union.workflow\n",
    "def build_indices_wf(\n",
    "    base_url: str = \"https://paulgraham.com/\",\n",
    "    articles_url: str = \"articles.html\",\n",
    "    model_api_string: str = \"BAAI/bge-large-en-v1.5\",\n",
    "    chunk_size: int = 250,\n",
    "    overlap: int = 30,\n",
    "    model: str = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\",\n",
    "    local: bool = True,\n",
    ") -> tuple[\n",
    "    Annotated[FlyteDirectory, BM25Index], Annotated[FlyteFile, ContextualChunksJSON]\n",
    "]:\n",
    "    tocs = parse_main_page(base_url=base_url, articles_url=articles_url, local=local)\n",
    "    scraped_content = union.map_task(scrape_pg_essays, concurrency=2)(document=tocs)\n",
    "    chunks = union.map_task(\n",
    "        functools.partial(create_chunks, chunk_size=chunk_size, overlap=overlap)\n",
    "    )(document=scraped_content)\n",
    "    contextual_chunks = union.map_task(functools.partial(generate_context, model=model))(\n",
    "        document=chunks\n",
    "    )\n",
    "    union.map_task(\n",
    "        functools.partial(\n",
    "            create_vector_index, model_api_string=model_api_string, local=local\n",
    "        ), \n",
    "        concurrency=2\n",
    "    )(document=contextual_chunks)\n",
    "    bm25s_index, contextual_chunks_json_file = create_bm25s_index(\n",
    "        documents=contextual_chunks\n",
    "    )\n",
    "    return bm25s_index, contextual_chunks_json_file\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResults:\n",
    "    vector_results: list[list[str]]\n",
    "    bm25s_results: list[list[str]]\n",
    "\n",
    "\n",
    "@union.task\n",
    "def retrieve(\n",
    "    bm25s_index: FlyteDirectory,\n",
    "    contextual_chunks_data: FlyteFile,\n",
    "    model_api_string: str = \"BAAI/bge-large-en-v1.5\",\n",
    "    queries: list[str] = [\n",
    "        \"What to do in the face of uncertainty?\",\n",
    "        \"Why won't people write?\",\n",
    "    ],\n",
    ") -> RetrievalResults:\n",
    "    import json\n",
    "\n",
    "    import bm25s\n",
    "    import numpy as np\n",
    "    from pymilvus import MilvusClient\n",
    "\n",
    "    client = MilvusClient(\"test_milvus.db\")    \n",
    "    \n",
    "    # Generate embeddings for the queries using Together\n",
    "    query_embeddings = [\n",
    "        get_embedding(query, model_api_string) for query in queries\n",
    "    ]\n",
    "    query_embeddings_np = np.array(query_embeddings, dtype=np.float32)\n",
    "\n",
    "    collection_name = \"paul_graham_collection\" \n",
    "    results = client.search(\n",
    "        collection_name, \n",
    "        query_embeddings_np, \n",
    "        limit=5, \n",
    "        search_params={\"metric_type\": \"COSINE\"}, \n",
    "        anns_field=\"embedding\",\n",
    "        output_fields=[\"document_index\", \"title\"]\n",
    "    )\n",
    "\n",
    "    # Load BM25S index\n",
    "    retriever = bm25s.BM25()\n",
    "    bm25_index = retriever.load(save_dir=bm25s_index.download())\n",
    "\n",
    "    # Load contextual chunk data\n",
    "    with open(contextual_chunks_data, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        contextual_chunks_data_dict = json.load(json_file)\n",
    "\n",
    "    # Perform BM25S-based retrieval\n",
    "    bm25s_idx_result = bm25_index.retrieve(\n",
    "        query_tokens=bm25s.tokenize(queries),\n",
    "        k=5,\n",
    "        corpus=np.array(list(contextual_chunks_data_dict.values())),\n",
    "    )\n",
    "\n",
    "    # Return results as a dataclass\n",
    "    return RetrievalResults(\n",
    "        vector_results=results,\n",
    "        bm25s_results=bm25s_idx_result.documents.tolist(),\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bm25s_index, contextual_chunks_data = build_indices_wf()\n",
    "    results = retrieve(\n",
    "        bm25s_index=bm25s_index, contextual_chunks_data=contextual_chunks_data\n",
    "    )\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote execution\n",
    "\n",
    "To provide the Together AI API key to the actor during remote execution, we send it as a [secret](https://docs.union.ai/serverless/user-guide/development-cycle/managing-secrets#creating-secrets). We can create this secret using the Union CLI before running the workflow. Simply run the following commands:\n",
    "```\n",
    "union create secret together-api-key\n",
    "```\n",
    "\n",
    "To run the workflow remotely on a Union cluster, we start by logging into the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê \u001b[33mConfiguration saved to \u001b[0m\u001b[33m/Users/samhitaalla/.union/\u001b[0m\u001b[33mconfig.yaml\u001b[0m\n",
      "Login successful into \u001b[1;32mserverless\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!union create login --serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we initialize a Union remote object to execute the workflow on the cluster. The [UnionRemote](https://docs.union.ai/serverless/user-guide/development-cycle/union-remote) Python API supports functionality similar to that of the Union CLI, enabling you to manage Union workflows, tasks, launch plans and artifacts from within your Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">15:40:33.909493 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> remote.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">291</span> - Jupyter notebook and interactive task  \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                </span>         support is still alpha.                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m15:40:33.909493\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m remote.py:\u001b[1;36m291\u001b[0m - Jupyter notebook and interactive task  \n",
       "\u001b[2;36m                \u001b[0m         support is still alpha.                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from union.remote import UnionRemote\n",
    "\n",
    "remote = UnionRemote(default_project=\"default\", default_domain=\"development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_execution = remote.execute(build_indices_wf, inputs={\"local\": False})\n",
    "print(indices_execution.execution_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a launch plan to run the workflow daily. A [launch plan](https://docs.union.ai/serverless/user-guide/core-concepts/launch-plans/) serves as a template for invoking the workflow. \n",
    "\n",
    "The scheduled launch plan ensures that the vector database and keyword index are regularly updated, keeping the data fresh and synchronized.\n",
    "\n",
    "Be sure to note the `version` field when registering the launch plan. Each Union entity (task, workflow, launch plan) is automatically versioned, as every entity is associated with a version by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">15:40:38.502891 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> remote.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">291</span> - Jupyter notebook and interactive task  \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                </span>         support is still alpha.                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m15:40:38.502891\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m remote.py:\u001b[1;36m291\u001b[0m - Jupyter notebook and interactive task  \n",
       "\u001b[2;36m                \u001b[0m         support is still alpha.                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mImage contextual-rag:wSXajFjO_IzJEs1M5oN_Gw found. Skip building.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lp = fl.LaunchPlan.get_or_create(\n",
    "    workflow=build_indices_wf,\n",
    "    name=\"vector_db_ingestion_activate\",\n",
    "    schedule=fl.CronSchedule(\n",
    "        schedule=\"0 1 * * *\"\n",
    "    ),  # Run every day to update the databases\n",
    "    auto_activate=True,\n",
    ")\n",
    "\n",
    "registered_lp = remote.register_launch_plan(entity=lp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy apps\n",
    "\n",
    "We deploy the FastAPI and Gradio applications to serve the RAG app with Union. Union allows us to build and serve our own web apps, though it's worth noting that serving on Union is still an experimental feature. FastAPI is used to define the endpoint for serving the app, while Gradio is used to create the user interface.\n",
    "\n",
    "When defining the app, we can specify inputs, images (using `ImageSpec`), resources to assign to the app, secrets, replicas, and more. We can organize the app specs into separate files. The FastAPI app spec is available in the `fastapi_app.py` file, and the Gradio app spec is in the `gradio_app.py` file.\n",
    "\n",
    "We retrieve the artifacts and send them as inputs to the FastAPI app. We can then retrieve the app's endpoint to use in the other app. Finally, we either create the app if it doesn't already exist or update it if it does.\n",
    "\n",
    "While we‚Äôre using FastAPI and Gradio here, you can use any Python-based front-end and API frameworks to define your apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from union.app import App, Input\n",
    "\n",
    "\n",
    "fastapi_app = App(\n",
    "    name=\"contextual-rag-fastapi\",\n",
    "    inputs=[\n",
    "        Input(\n",
    "            name=\"bm25s_index\",\n",
    "            value=BM25Index.query(),\n",
    "            download=True,\n",
    "            env_var=\"BM25S_INDEX\",\n",
    "        ),\n",
    "        Input(\n",
    "            name=\"contextual_chunks_json\",\n",
    "            value=ContextualChunksJSON.query(),\n",
    "            download=True,\n",
    "            env_var=\"CONTEXTUAL_CHUNKS_JSON\",\n",
    "        ),\n",
    "    ],\n",
    "    container_image=union.ImageSpec(\n",
    "        name=\"contextual-rag-fastapi\",\n",
    "        packages=[\n",
    "            \"together\",\n",
    "            \"bm25s\",\n",
    "            \"pymilvus\",\n",
    "            \"uvicorn[standard]\",\n",
    "            \"fastapi[standard]\",\n",
    "            \"union-runtime>=0.1.10\",\n",
    "            \"flytekit>=1.15.0b5\",\n",
    "        ],\n",
    "    ),\n",
    "    limits=union.Resources(cpu=\"1\", mem=\"3Gi\"),\n",
    "    port=8080,\n",
    "    include=[\"fastapi_app.py\"],\n",
    "    args=[\"uvicorn\", \"fastapi_app:app\", \"--port\", \"8080\"],\n",
    "    min_replicas=1,\n",
    "    max_replicas=1,\n",
    "    secrets=[\n",
    "        fl.Secret(\n",
    "            key=\"together-api-key\", \n",
    "            env_var=\"TOGETHER_API_KEY\", \n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR\n",
    "        ),\n",
    "        fl.Secret(\n",
    "            key=\"milvus-uri\",\n",
    "            env_var=\"MILVUS_URI\",\n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR,\n",
    "        ),\n",
    "        fl.Secret(\n",
    "            key=\"milvus-token\",\n",
    "            env_var=\"MILVUS_TOKEN\",\n",
    "            mount_requirement=union.Secret.MountType.ENV_VAR,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "gradio_app = App(\n",
    "    name=\"contextual-rag-gradio\",\n",
    "    inputs=[\n",
    "        Input(\n",
    "            name=\"fastapi_endpoint\",\n",
    "            value=fastapi_app.query_endpoint(public=False),\n",
    "            env_var=\"FASTAPI_ENDPOINT\",\n",
    "        )\n",
    "    ],\n",
    "    container_image=union.ImageSpec(\n",
    "        name=\"contextual-rag-gradio\",\n",
    "        packages=[\"gradio\", \"union-runtime>=0.1.5\"],\n",
    "    ),\n",
    "    limits=union.Resources(cpu=\"1\", mem=\"1Gi\"),\n",
    "    port=8080,\n",
    "    include=[\"gradio_app.py\"],\n",
    "    args=[\n",
    "        \"python\",\n",
    "        \"gradio_app.py\",\n",
    "    ],\n",
    "    min_replicas=1,\n",
    "    max_replicas=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">13:38:54.680630 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> remote.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">291</span> - Jupyter notebook and interactive task  \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                </span>         support is still alpha.                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m13:38:54.680630\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m remote.py:\u001b[1;36m291\u001b[0m - Jupyter notebook and interactive task  \n",
       "\u001b[2;36m                \u001b[0m         support is still alpha.                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® Deploying Application: <a href=\"https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-fastapi\" target=\"_blank\">contextual-rag-fastapi</a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® Deploying Application: \u001b]8;id=74331;https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-fastapi\u001b\\contextual-rag-fastapi\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üîé Console URL: \n",
       "<a href=\"https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-fastapi\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-fastapi</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üîé Console URL: \n",
       "\u001b]8;id=608337;https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-fastapi\u001b\\\u001b[4;94mhttps://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-fastapi\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> OutOfDate: The Configuration is still working to reflect the latest desired specification.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m OutOfDate: The Configuration is still working to reflect the latest desired specification.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> TrafficNotMigrated: Traffic is not yet migrated to the latest revision.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m TrafficNotMigrated: Traffic is not yet migrated to the latest revision.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> IngressNotConfigured: Ingress has not yet been reconciled.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m IngressNotConfigured: Ingress has not yet been reconciled.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> Uninitialized: Waiting for load balancer to be ready\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m Uninitialized: Waiting for load balancer to be ready\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Started:</span> Service is ready\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mStarted:\u001b[0m Service is ready\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üöÄ Deployed Endpoint: <a href=\"https://quiet-hill-86295.apps.serverless-1.us-east-2.s.union.ai\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://quiet-hill-86295.apps.serverless-1.us-east-2.s.union.ai</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üöÄ Deployed Endpoint: \u001b]8;id=704177;https://quiet-hill-86295.apps.serverless-1.us-east-2.s.union.ai\u001b\\\u001b[4;94mhttps://quiet-hill-86295.apps.serverless-1.us-east-2.s.union.ai\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® Deploying Application: <a href=\"https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-gradio\" target=\"_blank\">contextual-rag-gradio</a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® Deploying Application: \u001b]8;id=954595;https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-gradio\u001b\\contextual-rag-gradio\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üîé Console URL: \n",
       "<a href=\"https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-gradio\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-gradio</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üîé Console URL: \n",
       "\u001b]8;id=814440;https://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-gradio\u001b\\\u001b[4;94mhttps://serverless.union.ai/org/samhita-alla/projects/default/domains/development/apps/contextual-rag-gradio\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> OutOfDate: The Configuration is still working to reflect the latest desired specification.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m OutOfDate: The Configuration is still working to reflect the latest desired specification.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> IngressNotConfigured: Ingress has not yet been reconciled.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m IngressNotConfigured: Ingress has not yet been reconciled.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Pending:</span> Uninitialized: Waiting for load balancer to be ready\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mPending:\u001b[0m Uninitialized: Waiting for load balancer to be ready\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[Status]</span> <span style=\"font-style: italic\">Started:</span> Service is ready\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1mStatus\u001b[0m\u001b[1m]\u001b[0m \u001b[3mStarted:\u001b[0m Service is ready\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üöÄ Deployed Endpoint: <a href=\"https://steep-feather-4e234.apps.serverless-1.us-east-2.s.union.ai\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://steep-feather-4e234.apps.serverless-1.us-east-2.s.union.ai</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üöÄ Deployed Endpoint: \u001b]8;id=411877;https://steep-feather-4e234.apps.serverless-1.us-east-2.s.union.ai\u001b\\\u001b[4;94mhttps://steep-feather-4e234.apps.serverless-1.us-east-2.s.union.ai\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from union.remote._app_remote import AppRemote\n",
    "\n",
    "app_remote = AppRemote(project=\"default\", domain=\"development\")\n",
    "\n",
    "app_remote.create_or_update(fastapi_app)\n",
    "app_remote.create_or_update(gradio_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The apps will be deployed at the URLs provided in the output, which you can access. Below are some example queries to test the Gradio application:\n",
    "\n",
    "- What did Paul Graham do growing up?\n",
    "- What did the author do during their time in art school?\n",
    "- Can you give me a summary of the author's life?\n",
    "- What did the author do during their time at Yale?\n",
    "- What did the author do during their time at YC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to stop the apps, here‚Äôs how you can do it:\n",
    "# app_remote.stop(name=\"contextual-rag-fastapi-app\")\n",
    "# app_remote.stop(name=\"contextual-rag-gradio-app\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contextual-rag-example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
