{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEPA Summarization Optimization with LLM Judge Evaluation\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to optimize summarization prompts using GEPA (Generate, Evaluate, Propose, Adapt) with the our Evaluations API. We'll:\n",
    "\n",
    "1. Load the CNN/DailyMail dataset containing news articles\n",
    "2. Start with a baseline summarization prompt\n",
    "3. Use an optimizer LLM to iteratively improve the prompt\n",
    "4. Compare prompts head-to-head using a judge model\n",
    "5. Track improvement over multiple iterations\n",
    "\n",
    "**Concepts Covered:**\n",
    "- **GEPA Optimization**: Iterative prompt engineering using LLM feedback\n",
    "- **LLM-as-a-Judge**: Using a language model to evaluate and compare outputs\n",
    "- **Batch Evaluation**: Efficient comparison of multiple summaries\n",
    "- **Prompt Engineering**: Systematic improvement of instruction prompts"
   ],
   "id": "9bed21b9f21cadb7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Installation"
   ],
   "id": "c044d292f626f2f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU together dspy-ai datasets tqdm"
   ],
   "id": "cf56ca26c1b94222"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import together\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import dspy\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ],
   "id": "1c293b491e894110"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Set up your API key and configure the models we'll use:\n",
    "- **Summarizer Model**: Generates the summaries\n",
    "- **Judge Model**: Evaluates which summary is better\n",
    "- **Optimizer Model**: Proposes improvements to the prompt"
   ],
   "id": "8e71863c8ff3faa6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = together.Client()\n",
    "\n",
    "# Model configuration\n",
    "SUMMARIZER_MODEL = \"openai/gpt-oss-20b\"\n",
    "JUDGE_MODEL = \"deepseek-ai/DeepSeek-V3\"\n",
    "OPTIMIZER_MODEL = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"\n",
    "\n",
    "# Data splits\n",
    "TRAIN_SIZE = 150\n",
    "VAL_SIZE = 300\n",
    "TEST_SIZE = 300\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"‚úì Configuration complete\")"
   ],
   "id": "3d21616fa03c0145"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Baseline and Judge Prompts\n",
    "\n",
    "We start with a simple baseline prompt for summarization. The GEPA process will iteratively improve this prompt based on performance feedback."
   ],
   "id": "d9378d341fb8389d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_PROMPT = \"\"\"Summarize this news article in 3-5 key points.\n",
    "\n",
    "Write a brief summary covering:\n",
    "- The main news event\n",
    "- Key people or organizations involved\n",
    "- Important details or outcomes\n",
    "- Any significant context\n",
    "\n",
    "Keep it to 3-5 sentences total.\"\"\"\n",
    "\n",
    "JUDGE_PROMPT = \"\"\"Compare these two summaries of the same news article.\n",
    "\n",
    "Which summary better:\n",
    "- Captures the main news story\n",
    "- Includes important details\n",
    "- Is clear and concise\n",
    "- Avoids unnecessary information\n",
    "\n",
    "Choose A or B and explain why briefly.\"\"\"\n",
    "\n",
    "print(\"Baseline Prompt:\")\n",
    "print(BASELINE_PROMPT)\n",
    "print(\"\\nJudge Prompt:\")\n",
    "print(JUDGE_PROMPT)"
   ],
   "id": "263940c8c55eb1dd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Loading the CNN/DailyMail Dataset\n",
    "\n",
    "The CNN/DailyMail dataset contains news articles paired with human-written highlights. We'll use the articles as our source text and split the data into train, validation, and test sets.\n",
    "\n",
    "**Dataset Structure:**\n",
    "- `article`: The full news article text\n",
    "- `highlights`: Human-written bullet-point summary\n",
    "- We'll use the articles for summarization and evaluate our generated summaries"
   ],
   "id": "c0a86293e7b95dd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data():\n",
    "    \"\"\"Load CNN/DailyMail dataset for summarization.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìÇ LOADING DATA\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"Loading CNN/DailyMail dataset...\")\n",
    "    dataset = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", trust_remote_code=True)\n",
    "    data = dataset['test']\n",
    "\n",
    "    print(f\"‚úì Loaded {len(data)} examples\")\n",
    "    print(f\"  Sample article: {data[0]['article'][:100]}...\")\n",
    "    print(f\"  Sample highlights: {data[0]['highlights'][:100]}...\")\n",
    "\n",
    "    all_data = []\n",
    "    for i, item in enumerate(data):\n",
    "        all_data.append({\n",
    "            'id': f\"cnn_{i}\",\n",
    "            'text': item['article'],\n",
    "            'reference_summary': item['highlights']\n",
    "        })\n",
    "\n",
    "    print(f\"‚úì Converted to {len(all_data)} items\")\n",
    "\n",
    "    # Shuffle and split\n",
    "    random.seed(RANDOM_SEED)\n",
    "    random.shuffle(all_data)\n",
    "\n",
    "    train_data = all_data[:TRAIN_SIZE]\n",
    "    val_data = all_data[TRAIN_SIZE:TRAIN_SIZE + VAL_SIZE]\n",
    "    test_data = all_data[TRAIN_SIZE + VAL_SIZE:TRAIN_SIZE + VAL_SIZE + TEST_SIZE]\n",
    "\n",
    "    print(f\"‚úì Split: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")\n",
    "\n",
    "    # Verify\n",
    "    assert len(val_data) > 0, \"Val data is empty!\"\n",
    "    assert len(test_data) > 0, \"Test data is empty!\"\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Load the data\n",
    "train_data, val_data, test_data = load_and_split_data()"
   ],
   "id": "7dcc2d8d5c706df4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Summarization Module\n",
    "\n",
    "We create a DSPy module that wraps our summarization task. This module can be configured with different instruction prompts, which is key to the GEPA optimization process."
   ],
   "id": "d1b9222690db8449"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(dspy.Signature):\n",
    "    \"\"\"Generate a summary.\"\"\"\n",
    "    text = dspy.InputField()\n",
    "    summary = dspy.OutputField()\n",
    "\n",
    "\n",
    "class SummarizationModule(dspy.Module):\n",
    "    \"\"\"Summarization module.\"\"\"\n",
    "\n",
    "    def __init__(self, instructions=None):\n",
    "        super().__init__()\n",
    "        self.instructions = instructions or BASELINE_PROMPT\n",
    "\n",
    "        if instructions:\n",
    "            class CustomSummarizer(dspy.Signature):\n",
    "                __doc__ = instructions\n",
    "                text = dspy.InputField()\n",
    "                summary = dspy.OutputField()\n",
    "\n",
    "            self.predictor = dspy.Predict(CustomSummarizer)\n",
    "        else:\n",
    "            self.predictor = dspy.Predict(Summarizer)\n",
    "\n",
    "    def forward(self, text):\n",
    "        return self.predictor(text=text)\n",
    "\n",
    "print(\"‚úì Summarization module defined\")"
   ],
   "id": "b8ca2917024c326e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Batch Summary Generation\n",
    "\n",
    "This function generates summaries for a batch of articles using a given prompt. It includes error handling and progress tracking."
   ],
   "id": "590d6b9c625ca2cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries_batch(\n",
    "        summarizer: SummarizationModule,\n",
    "        data: List[Dict],\n",
    "        desc: str = \"Generating\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Generate summaries for a batch of texts.\"\"\"\n",
    "    results = []\n",
    "    errors = 0\n",
    "    error_details = []\n",
    "\n",
    "    # Print the prompt being used (first item only)\n",
    "    if len(data) > 0:\n",
    "        print(f\"  Using prompt: {summarizer.instructions[:100]}...\")\n",
    "\n",
    "    for item in tqdm(data, desc=desc):\n",
    "        try:\n",
    "            pred = summarizer(text=item['text'][:5000])\n",
    "\n",
    "            if pred is None:\n",
    "                raise ValueError(\"Model returned None\")\n",
    "\n",
    "            if hasattr(pred, 'summary') and pred.summary:\n",
    "                summary = pred.summary\n",
    "            elif isinstance(pred, str):\n",
    "                summary = pred\n",
    "            else:\n",
    "                print(f\"\\n  DEBUG: pred type={type(pred)}, hasattr summary={hasattr(pred, 'summary')}\")\n",
    "                raise ValueError(f\"Cannot extract summary from {type(pred)}\")\n",
    "\n",
    "            summary = summary.strip()\n",
    "            if len(summary) < 20:\n",
    "                raise ValueError(\"Summary too short\")\n",
    "\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            error_details.append(str(e)[:100])\n",
    "\n",
    "            if errors <= 5:\n",
    "                print(f\"\\n‚ö†Ô∏è  Error: {str(e)[:80]}\")\n",
    "\n",
    "            summary = \"Error generating summary.\"\n",
    "\n",
    "        results.append({\n",
    "            'id': item['id'],\n",
    "            'text': item['text'],\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "    if errors > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Total errors: {errors}/{len(data)} ({errors / len(data) * 100:.1f}%)\")\n",
    "        from collections import Counter\n",
    "        common_errors = Counter(error_details).most_common(3)\n",
    "        print(f\"  Most common errors:\")\n",
    "        for err, count in common_errors:\n",
    "            print(f\"    - {err[:60]}... ({count}x)\")\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"‚úì Batch generation function defined\")"
   ],
   "id": "270abdde73d2ca72"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Optimizer LLM Wrapper\n",
    "\n",
    "This wrapper allows us to use an LLM to propose improvements to our summarization prompt based on current performance."
   ],
   "id": "2cfe63f485894d7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleOptimizerLM:\n",
    "    \"\"\"Wrapper for optimizer LLM.\"\"\"\n",
    "\n",
    "    def __init__(self, model: str, api_key: str):\n",
    "        self.client = together.Client(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, prompt: str) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=4000\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "print(\"‚úì Optimizer LLM wrapper defined\")"
   ],
   "id": "d11af9ff91f442df"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Reflection and Prompt Improvement\n",
    "\n",
    "This function uses the optimizer LLM to analyze the current prompt and performance, then propose an improved version.\n",
    "\n",
    "**Key Constraints:**\n",
    "- Keep prompts under 150 words for clarity\n",
    "- Focus on simple, direct instructions\n",
    "- Target 4-6 sentence summaries\n",
    "- Avoid overly complex requirements"
   ],
   "id": "67a224aff87d2f5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect_and_improve_prompt(\n",
    "        current_prompt: str,\n",
    "        current_score: float,\n",
    "        optimizer_lm: SimpleOptimizerLM,\n",
    "        iteration: int\n",
    ") -> str:\n",
    "    \"\"\"Use LLM to propose improved prompt.\"\"\"\n",
    "\n",
    "    print(f\"\\nü§î REFLECTION (Iteration {iteration})\")\n",
    "\n",
    "    reflection_prompt = f\"\"\"You are optimizing a summarization prompt for CNN/DailyMail news articles.\n",
    "\n",
    "Current Prompt:\n",
    "```\n",
    "{current_prompt}\n",
    "```\n",
    "\n",
    "Current Performance: {current_score:.1%} win rate\n",
    "\n",
    "Your task: Propose a SIMPLE improved version that generates better summaries.\n",
    "\n",
    "CRITICAL CONSTRAINTS:\n",
    "- Keep the prompt under 150 words\n",
    "- Make it clear and direct (NOT overly complex)\n",
    "- Target 4-6 sentence summaries\n",
    "- Avoid excessive instructions or formatting requirements\n",
    "- The prompt should be easy for the model to follow\n",
    "\n",
    "Focus on:\n",
    "- Should it emphasize different aspects (accuracy, brevity, completeness)?\n",
    "- Are the current guidelines clear?\n",
    "- Is anything missing or unnecessary?\n",
    "\n",
    "Output ONLY the improved prompt within ``` blocks. Keep it simple and clear.\"\"\"\n",
    "\n",
    "    response = optimizer_lm(reflection_prompt)\n",
    "\n",
    "    # Extract prompt\n",
    "    match = re.search(r'```(.*?)```', response, re.DOTALL)\n",
    "    if match:\n",
    "        new_prompt = match.group(1).strip()\n",
    "        # Remove language tags\n",
    "        for tag in ['markdown', 'text', 'python', 'plaintext']:\n",
    "            if new_prompt.startswith(f'{tag}\\n'):\n",
    "                new_prompt = '\\n'.join(new_prompt.split('\\n')[1:])\n",
    "\n",
    "        # Validate length (reject if too long)\n",
    "        word_count = len(new_prompt.split())\n",
    "        if word_count > 200:\n",
    "            print(f\"  ‚ö†Ô∏è  Generated prompt too long ({word_count} words), using current\")\n",
    "            return current_prompt\n",
    "\n",
    "        print(f\"‚úì Generated new prompt ({word_count} words)\")\n",
    "        return new_prompt\n",
    "\n",
    "    print(\"‚ö†Ô∏è  Could not extract prompt\")\n",
    "    return current_prompt\n",
    "\n",
    "print(\"‚úì Reflection function defined\")"
   ],
   "id": "1186e66cab3ea1f1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Head-to-Head Prompt Comparison\n",
    "\n",
    "This function compares two prompts by:\n",
    "1. Generating summaries with both prompts\n",
    "2. Creating a comparison dataset\n",
    "3. Using the Together AI evaluation API with a judge model\n",
    "4. Computing win rates\n",
    "\n",
    "The evaluation uses a two-pass approach to eliminate position bias."
   ],
   "id": "a2fbbd02f5054425"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_prompts_on_batch(\n",
    "        data: List[Dict],\n",
    "        prompt_a: str,\n",
    "        prompt_b: str,\n",
    "        summarizer_lm: dspy.LM,\n",
    "        eval_name: str\n",
    ") -> Tuple[float, float, Dict]:\n",
    "    \"\"\"\n",
    "    Compare two summarization prompts.\n",
    "\n",
    "    1. Generate summaries with prompt A\n",
    "    2. Generate summaries with prompt B\n",
    "    3. Use judge to compare them\n",
    "    4. Return win rate for prompt A\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"üîÑ COMPARING PROMPTS: {eval_name}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "    # Step 1: Generate with both prompts\n",
    "    dspy.configure(lm=summarizer_lm)\n",
    "\n",
    "    summarizer_a = SummarizationModule(prompt_a)\n",
    "    summarizer_b = SummarizationModule(prompt_b)\n",
    "\n",
    "    print(\"Generating summaries with Prompt A...\")\n",
    "    summaries_a = generate_summaries_batch(summarizer_a, data, \"Prompt A\")\n",
    "\n",
    "    print(\"Generating summaries with Prompt B...\")\n",
    "    summaries_b = generate_summaries_batch(summarizer_b, data, \"Prompt B\")\n",
    "\n",
    "    # Step 2: Prepare comparison data\n",
    "    temp_file = f\"temp_compare_{eval_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\"\n",
    "\n",
    "    with open(temp_file, 'w') as f:\n",
    "        for summary_a, summary_b in zip(summaries_a, summaries_b):\n",
    "            formatted = {\n",
    "                \"prompt\": f\"Source article: {summary_a['text'][:5000]}\",\n",
    "                \"model_a_output\": summary_a['summary'],\n",
    "                \"model_b_output\": summary_b['summary'],\n",
    "                \"id\": summary_a['id']\n",
    "            }\n",
    "            f.write(json.dumps(formatted) + '\\n')\n",
    "\n",
    "    # Step 3: Upload and evaluate\n",
    "    print(\"üì§ Uploading for comparison...\")\n",
    "    file_response = client.files.upload(file=temp_file, purpose=\"eval\")\n",
    "    file_id = file_response.id\n",
    "\n",
    "    print(\"üöÄ Launching comparison...\")\n",
    "    eval_response = client.evaluation.create(\n",
    "        type=\"compare\",\n",
    "        input_data_file_path=file_id,\n",
    "        judge_model=JUDGE_MODEL,\n",
    "        judge_model_source=\"serverless\",\n",
    "        judge_system_template=JUDGE_PROMPT,\n",
    "        model_a=\"model_a_output\",\n",
    "        model_b=\"model_b_output\"\n",
    "    )\n",
    "\n",
    "    # Step 4: Wait and get results\n",
    "    print(f\"‚è≥ Waiting (ID: {eval_response.workflow_id})...\")\n",
    "    while True:\n",
    "        status = client.evaluation.status(eval_response.workflow_id)\n",
    "        if status.status.value == \"completed\":\n",
    "            break\n",
    "        elif status.status.value == \"failed\":\n",
    "            raise Exception(\"Evaluation failed\")\n",
    "        time.sleep(30)\n",
    "\n",
    "    a_wins = status.results.get('A_wins', 0)\n",
    "    b_wins = status.results.get('B_wins', 0)\n",
    "    ties = status.results.get('Ties', 0)\n",
    "\n",
    "    # Win rate for prompt A\n",
    "    decisive_total = a_wins + b_wins\n",
    "    if decisive_total > 0:\n",
    "        a_win_rate = a_wins / decisive_total\n",
    "        b_win_rate = b_wins / decisive_total\n",
    "    else:\n",
    "        a_win_rate = b_win_rate = 0.5\n",
    "\n",
    "    print(f\"‚úì Results: Prompt A wins={a_wins}, Prompt B wins={b_wins}, Ties={ties}\")\n",
    "    print(f\"‚úì Prompt A win rate: {a_win_rate:.2%}\")\n",
    "\n",
    "    os.remove(temp_file)\n",
    "\n",
    "    return a_win_rate, b_win_rate, {\n",
    "        'a_wins': a_wins,\n",
    "        'b_wins': b_wins,\n",
    "        'ties': ties,\n",
    "        'a_win_rate': a_win_rate\n",
    "    }\n",
    "\n",
    "print(\"‚úì Comparison function defined\")"
   ],
   "id": "5a1b2d5116f3731f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß¨ GEPA Optimization Loop\n",
    "\n",
    "This is the main optimization loop that implements the GEPA algorithm:\n",
    "\n",
    "1. **Generate**: Create summaries with current prompt\n",
    "2. **Evaluate**: Compare against baseline using judge model\n",
    "3. **Propose**: Use optimizer LLM to suggest improvements\n",
    "4. **Adapt**: Accept improvements that increase win rate\n",
    "\n",
    "The process repeats for multiple iterations, tracking the best prompt found."
   ],
   "id": "6657d33b050676ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_manual_gepa(\n",
    "        train_data: List[Dict],\n",
    "        val_data: List[Dict],\n",
    "        test_data: List[Dict],\n",
    "        summarizer_lm: dspy.LM,\n",
    "        optimizer_lm: SimpleOptimizerLM,\n",
    "        max_iterations: int = 5\n",
    "):\n",
    "    \"\"\"Manual GEPA-style optimization.\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üß¨ MANUAL GEPA OPTIMIZATION\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Track best prompt\n",
    "    best_prompt = BASELINE_PROMPT\n",
    "    best_val_score = 0.5  # Start at 50% (neutral)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"ITERATION {i + 1}/{max_iterations}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "\n",
    "        if i == 0:\n",
    "            print(\"Iteration 0: Establishing baseline (no comparison yet)\")\n",
    "            continue\n",
    "\n",
    "        # Generate new candidate prompt\n",
    "        new_prompt = reflect_and_improve_prompt(\n",
    "            best_prompt,\n",
    "            best_val_score,\n",
    "            optimizer_lm,\n",
    "            i\n",
    "        )\n",
    "\n",
    "        if new_prompt == best_prompt:\n",
    "            print(\"‚ö†Ô∏è  No change in prompt, stopping\")\n",
    "            break\n",
    "\n",
    "        print(f\"‚úì Generated candidate prompt ({len(new_prompt)} chars)\")\n",
    "\n",
    "        # Compare best_prompt vs new_prompt on validation set\n",
    "        baseline_win_rate, new_prompt_win_rate, metrics = compare_two_prompts_on_batch(\n",
    "            val_data,\n",
    "            prompt_a=best_prompt,\n",
    "            prompt_b=new_prompt,\n",
    "            summarizer_lm=summarizer_lm,\n",
    "            eval_name=f\"iter{i}_val\"\n",
    "        )\n",
    "\n",
    "        new_prompt_win_rate = 1.0 - baseline_win_rate\n",
    "\n",
    "        print(f\"\\n  Current best: {baseline_win_rate:.2%}\")\n",
    "        print(f\"  New candidate: {new_prompt_win_rate:.2%}\")\n",
    "\n",
    "        if new_prompt_win_rate > best_val_score:\n",
    "            improvement = new_prompt_win_rate - best_val_score\n",
    "            print(f\"  üéâ New best! (+{improvement * 100:.2f}pp)\")\n",
    "            best_prompt = new_prompt\n",
    "            best_val_score = new_prompt_win_rate\n",
    "        else:\n",
    "            print(f\"  No improvement\")\n",
    "\n",
    "    # Calculate total time\n",
    "    total_time = time.time() - start_time\n",
    "    hours = int(total_time // 3600)\n",
    "    minutes = int((total_time % 3600) // 60)\n",
    "    seconds = int(total_time % 60)\n",
    "\n",
    "    # Final test evaluation\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä FINAL TEST EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\n‚è±Ô∏è  OPTIMIZATION TIME:\")\n",
    "    if hours > 0:\n",
    "        print(f\"  Total: {hours}h {minutes}m {seconds}s\")\n",
    "    elif minutes > 0:\n",
    "        print(f\"  Total: {minutes}m {seconds}s\")\n",
    "    else:\n",
    "        print(f\"  Total: {seconds}s\")\n",
    "\n",
    "    baseline_test_win_rate, optimized_test_win_rate, _ = compare_two_prompts_on_batch(\n",
    "        test_data,\n",
    "        prompt_a=BASELINE_PROMPT,\n",
    "        prompt_b=best_prompt,\n",
    "        summarizer_lm=summarizer_lm,\n",
    "        eval_name=\"final_test\"\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üéâ FINAL RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\nTEST SET:\")\n",
    "    print(f\"  Baseline prompt:  {baseline_test_win_rate:.2%}\")\n",
    "    print(f\"  Optimized prompt: {optimized_test_win_rate:.2%}\")\n",
    "    print(f\"  Improvement:      {(optimized_test_win_rate - 0.5) * 100:+.2f}pp from neutral\")\n",
    "\n",
    "    # Save results\n",
    "    output_dir = Path(\"results\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    with open(output_dir / f\"prompts_{timestamp}.txt\", 'w') as f:\n",
    "        f.write(\"BASELINE:\\n\" + \"=\" * 80 + \"\\n\")\n",
    "        f.write(BASELINE_PROMPT)\n",
    "        f.write(\"\\n\\nOPTIMIZED:\\n\" + \"=\" * 80 + \"\\n\")\n",
    "        f.write(best_prompt)\n",
    "        f.write(f\"\\n\\nRESULTS:\\n\" + \"=\" * 80 + \"\\n\")\n",
    "        f.write(f\"Baseline: {baseline_test_win_rate:.2%}\\n\")\n",
    "        f.write(f\"Optimized: {optimized_test_win_rate:.2%}\\n\")\n",
    "\n",
    "    print(f\"\\nüíæ Saved to: results/prompts_{timestamp}.txt\")\n",
    "\n",
    "    return {\n",
    "        'baseline_test': baseline_test_win_rate,\n",
    "        'optimized_test': optimized_test_win_rate,\n",
    "        'best_prompt': best_prompt\n",
    "    }\n",
    "\n",
    "print(\"‚úì GEPA optimization function defined\")"
   ],
   "id": "c7100da955cfb3b5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run the Optimization\n",
    "\n",
    "Now we'll execute the full GEPA optimization process. This will:\n",
    "1. Set up the summarizer and optimizer models\n",
    "2. Run multiple iterations of prompt improvement\n",
    "3. Evaluate the final optimized prompt on the test set\n",
    "4. Display comprehensive results"
   ],
   "id": "4839066f78acf10d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéØ GEPA SUMMARIZATION - TOGETHER AI BATCH EVAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not TOGETHER_API_KEY or TOGETHER_API_KEY == 'your_api_key_here':\n",
    "    print(\"‚ùå Set TOGETHER_API_KEY\")\n",
    "else:\n",
    "    # Setup models\n",
    "    summarizer_lm = dspy.LM(\n",
    "        f\"together_ai/{SUMMARIZER_MODEL}\",\n",
    "        api_key=TOGETHER_API_KEY,\n",
    "        temperature=0.5,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "\n",
    "    optimizer_lm = SimpleOptimizerLM(\n",
    "        model=OPTIMIZER_MODEL,\n",
    "        api_key=TOGETHER_API_KEY\n",
    "    )\n",
    "\n",
    "    # Run optimization\n",
    "    results = run_manual_gepa(\n",
    "        train_data,\n",
    "        val_data,\n",
    "        test_data,\n",
    "        summarizer_lm,\n",
    "        optimizer_lm,\n",
    "        max_iterations=5\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ Complete!\")"
   ],
   "id": "51f60931bec8f490"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Analyzing the Results\n",
    "\n",
    "Let's examine the optimized prompt and compare it to the baseline."
   ],
   "id": "2be0f2bb00a13ff6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üìù PROMPT COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nBASELINE PROMPT:\")\n",
    "print(\"-\" * 80)\n",
    "print(BASELINE_PROMPT)\n",
    "\n",
    "print(\"\\n\\nOPTIMIZED PROMPT:\")\n",
    "print(\"-\" * 80)\n",
    "print(results['best_prompt'])\n",
    "\n",
    "print(\"\\n\\nPERFORMANCE COMPARISON:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Baseline Win Rate:  {results['baseline_test']:.2%}\")\n",
    "print(f\"Optimized Win Rate: {results['optimized_test']:.2%}\")\n",
    "print(f\"Improvement:        {(results['optimized_test'] - 0.5) * 100:+.2f} percentage points from neutral\")"
   ],
   "id": "bc461eee131bd49f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Key Findings\n",
    "\n",
    "**GEPA Optimization Process:**\n",
    "- Iteratively improves prompts through LLM-guided reflection\n",
    "- Uses head-to-head comparisons with a judge model\n",
    "- Tracks and accepts only improvements over baseline\n",
    "\n",
    "**Benefits of This Approach:**\n",
    "1. **Automated**: No manual prompt engineering required\n",
    "2. **Data-driven**: Decisions based on actual performance metrics\n",
    "3. **Scalable**: Can optimize for any task with appropriate data\n",
    "4. **Transparent**: Clear tracking of improvements across iterations\n",
    "\n",
    "**Next Steps:**\n",
    "- Try with different datasets or domains\n",
    "- Experiment with different judge criteria\n",
    "- Adjust the optimizer's reflection prompt\n",
    "- Increase iterations for potentially better results"
   ],
   "id": "8b606f57d491feb6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
