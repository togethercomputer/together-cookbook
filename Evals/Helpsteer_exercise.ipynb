{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa2dc6b",
   "metadata": {},
   "source": [
    "# Model Comparison on Summarization Tasks\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Evals/Compare_Evals.ipynb)\n",
    "\n",
    "<img src=\"../images/compare_eval.png\" width=\"750\">\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to compare quality of a small base model, fine-tuned model and a proprietary model using Together Evaluations service on a conversational dataset. We'll showcase:\n",
    "- Fine-tuning a base model on the HelpSteer3 dataset\n",
    "- Using external models (GPT from OpenAI, gemini from Google) for comparison\n",
    "- Leveraging GPT-5.2 as an LLM-as-a-Judge to evaluate model outputs\n",
    "- Comparing against a proprietary baseline model (google/gemini-2.5-flash)\n",
    "\n",
    "You can also find out more about the Evaluations API in the [docs](https://docs.together.ai/docs/ai-evaluations)!\n",
    "\n",
    "The full list of supported models can be found [here](https://docs.together.ai/docs/evaluations-supported-models).\n",
    "\n",
    "\n",
    "**Concepts Covered:**\n",
    "- **LLM-as-a-Judge**: Using GPT-4o as a judge model to evaluate and compare outputs from other models\n",
    "- **Compare Evaluation**: Head-to-head comparison between multiple models to determine which performs better\n",
    "- **Fine-tuning**: Training a base model on domain-specific data to improve performance\n",
    "- **External Model Integration**: Using models from different providers (OpenAI, Google) alongside Together AI models\n",
    "- **Multi-Model Comparison**: Evaluating a fine-tuned model against both small baseline models (Gemini Flash 1.5) and proprietary models (GPT-4o-mini)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f43be5",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To set up the environment:\n",
    "1. Navigate to the same folder as this notebook\n",
    "2. Run the installation script: `bash install.sh`\n",
    "   - This will create a virtual environment called `env_cookbook_evals`\n",
    "   - It will install all dependencies from `requirements.txt`\n",
    "3. Activate the environment: `source env_cookbook_evals/bin/activate`\n",
    "4. You're ready to run this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "JUDGE_MODEL = \"moonshotai/Kimi-K2-Instruct-0905\"\n",
    "PROPRIETARY_BASE_MODEL = \"openai/gpt-5-mini\"\n",
    "# You can try using Qwen/Qwen3-Next-80B-A3B-Instruct if you have a lot of time.\n",
    "# It is available for FT and on serverless inference, but will take a couple of hours for fine-tuning,\n",
    "# It will also require a DE endpoint to be launched manually for FT version.\n",
    "BASE_OSS_MODEL_FOR_INFERENCE = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "# We use this model because it is available on LoRA serverless\n",
    "# but we can also use DE models if we launch an endpoint manually\n",
    "BASE_OSS_MODEL_FOR_FT = \"Qwen/Qwen3-Next-80B-A3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3daef6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import together\n",
    "\n",
    "together_client = together.Client(api_key=TOGETHER_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d307fe1",
   "metadata": {},
   "source": [
    "## üìä Understanding the HelpSteer3 Edit Dataset\n",
    "\n",
    "The HelpSteer3 Edit dataset contains conversational contexts paired with multiple response options that can be compared and evaluated.\n",
    "\n",
    "In our setup we will use only a particular column \"edited_response\", that represents the ideal response that a user potentially wants (\"golden_response\"). It can be a human-provided column or a strong proprietary model (ex. GPT-4) in other settings.\n",
    "\n",
    "For our evaluation, we'll use this dataset to compare how different models respond to the same prompts and assess which produces higher-quality outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e13d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vprov/work/together/together-cookbook/env_cookbook/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "hs3_edit = load_dataset(\"nvidia/HelpSteer3\", \"edit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84442044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample context:\n",
      "[{'role': 'user', 'content': 'Good morning! Can you please write me a simple html and javascript that creates a quiz for students, using the variables I have defined below? The quiz will need to calculate the correct response, which is the average of 7 random numbers. Students will enter their response which will be checked against the calculated response, then a gif image will be displayed for \"correct\" or \"incorrect\".'}, {'role': 'assistant', 'content': 'Sure, here\\'s an example of a simple HTML and JavaScript quiz that meets your requirements:\\n\\n```html\\n\\nSimple Quiz\\n\\nSimple Quiz\\n===========\\n\\nEnter your response:\\n\\n  \\n\\n\\nCheck Answer\\n\\nfunction checkAnswer() {\\n  var response = document.getElementById(\"response\").value;\\n  var correctResponse = (Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7) / 7;\\n\\n  if (response == correctResponse) {\\n    document.getElementById(\"result\").innerHTML = \"<img src=\\'correct.gif\\'>\";\\n  } else {\\n    document.getElementById(\"result\").innerHTML = \"<img src=\\'incorrect.gif\\'>\";\\n  }\\n}\\n```\\n\\nIn this example, the `checkAnswer()` function is called when the \"Check Answer\" button is clicked. It retrieves the student\\'s response from the input field, calculates the correct response using the average of 7 random numbers, and compares the two values. If the student\\'s response matches the correct response, it displays a \"correct\" gif image. Otherwise, it displays an \"incorrect\" gif image.\\n\\nNote that you\\'ll need to replace the `correct.gif` and `incorrect.gif` with the actual paths to your own correct and incorrect gif images.'}, {'role': 'user', 'content': 'Thank you! Are you able to add some CSS so it looks a bit nicer?'}, {'role': 'assistant', 'content': 'Sure, here\\'s an updated version of the HTML and JavaScript code with some basic CSS styling:\\n\\n```html\\n\\nSimple Quiz\\n\\nSimple Quiz\\n===========\\n\\nEnter your response:\\n\\n  \\n\\n\\nCheck Answer\\n\\nfunction checkAnswer() {\\n  var response = document.getElementById(\"response\").value;\\n  var correctResponse = (Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7) / 7;\\n\\n  if (response == correctResponse) {\\n    document.getElementById(\"result\").innerHTML = \"<img src=\\'correct.gif\\' style=\\'width: 200px; height: 200px;\\'>\";\\n  } else {\\n    document.getElementById(\"result\").innerHTML = \"<img src=\\'incorrect.gif\\' style=\\'width: 200px; height: 200px;\\'>\";\\n  }\\n}\\nbody {\\n  font-family: Arial, sans-serif;\\n  text-align: center;\\n}\\n\\nh1 {\\n  color: #333;\\n}\\n\\ninput[type=\"text\"] {\\n  padding: 10px;\\n  font-size: 16px;\\n  border-radius: 5px;\\n  border: 1px solid #ccc;\\n  margin-bottom: 10px;\\n}\\n\\nbutton {\\n  padding: 10px 20px;\\n  font-size: 16px;\\n  border-radius: 5px;\\n  border: none;\\n  background-color: #4CAF50;\\n  color: white;\\n  cursor: pointer;\\n}\\n\\nbutton:hover {\\n  background-color: #45a049;\\n}\\n\\n#result {\\n  margin-top: 20px;\\n}\\n```\\n\\nIn this updated version, the CSS styles are added to the `style` tags in the `head` section of the HTML document. The styles include setting the font family and text alignment for the body, setting the color for the `h1` element, setting the padding, font size, border radius, and border for the input field, setting the padding, font size, border radius, border, background color, and text color for the button, and setting the margin for the result element.\\n\\nFeel free to adjust the styles to your liking.'}, {'role': 'user', 'content': 'Great! Can you please add a function so that if the answer is incorrect, the student is given some text with a link to find more information? This text and the link URL need to be stored in a variable.'}]\n",
      "\n",
      "Sample edited_response:\n",
      "Certainly! Here's the updated code with a function that displays additional information and a link when the answer is incorrect:\n",
      "\n",
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "  <style>\n",
      "    body {\n",
      "      font-family: Arial, sans-serif;\n",
      "      text-align: center;\n",
      "    }\n",
      "\n",
      "    h1 {\n",
      "      color: #333;\n",
      "    }\n",
      "\n",
      "    input[type=\"text\"] {\n",
      "      padding: 10px;\n",
      "      font-size: 16px;\n",
      "      border-radius: 5px;\n",
      "      border: 1px solid #ccc;\n",
      "      margin-bottom: 10px;\n",
      "    }\n",
      "\n",
      "    button {\n",
      "      padding: 10px 20px;\n",
      "      font-size: 16px;\n",
      "      border-radius: 5px;\n",
      "      border: none;\n",
      "      background-color: #4CAF50;\n",
      "      color: white;\n",
      "      cursor: pointer;\n",
      "    }\n",
      "\n",
      "    button:hover {\n",
      "      background-color: #45a049;\n",
      "    }\n",
      "\n",
      "    #result {\n",
      "      margin-top: 20px;\n",
      "    }\n",
      "\n",
      "    .incorrect-message {\n",
      "      margin-top: 20px;\n",
      "      color: red;\n",
      "    }\n",
      "  </style>\n",
      "</head>\n",
      "<body>\n",
      "\n",
      "  <h1>Simple Quiz</h1>\n",
      "\n",
      "  <p>Enter your response:</p>\n",
      "\n",
      "  <input type=\"text\" id=\"response\">\n",
      "\n",
      "  <br>\n",
      "\n",
      "  <button onclick=\"checkAnswer()\">Check Answer</button>\n",
      "\n",
      "  <div id=\"result\"></div>\n",
      "\n",
      "  <script>\n",
      "    // Variables for additional information and link\n",
      "    var additionalInfo = \"Sorry, that's not correct. Here's a link to help you learn more:\";\n",
      "    var linkURL = \"https://www.example.com/learn-more\";\n",
      "\n",
      "    // Generate the correct answer by averaging 7 random numbers\n",
      "    var total = 0;\n",
      "    for (var i = 0; i < 7; i++) {\n",
      "      total += Math.random() * 7;\n",
      "    }\n",
      "    var correctResponse = Math.round(total / 7);\n",
      "\n",
      "    // Function to check the user's answer\n",
      "    function checkAnswer() {\n",
      "      // Get and parse the user's input\n",
      "      var response = parseFloat(document.getElementById(\"response\").value);\n",
      "      console.log(response,correctResponse);\n",
      "      // Validate the input to ensure it's a number\n",
      "      if(isNaN(response)){\n",
      "         document.getElementById(\"result\").innerHTML =\"<p>Please enter a valid number!</p>\"\n",
      "         return; // Exit the function if input is invalid\n",
      "       }\n",
      "     // Check if the response matches the correct answer\n",
      "     else if (response === correctResponse) {\n",
      "        document.getElementById(\"result\").innerHTML = \"<img src='correct.gif' style='width: 200px; height: 200px;' alt='correct answer'>\"; \n",
      "        document.getElementById(\"result\").classList.remove(\"incorrect-message\");\n",
      "      }\n",
      "      // Otherwise check if the response is incorrect and provide a link for more information \n",
      "      else {\n",
      "        document.getElementById(\"result\").innerHTML = \"<img src='incorrect.gif' style='width: 200px; height: 200px;' alt='wrong answer'><div class='incorrect-message'>\" + additionalInfo + \" <a href='\" + linkURL + \"' target='_blank'>Learn More</a></div>\"; \n",
      "      }\n",
      "    }\n",
      "  </script>\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "```\n",
      "\n",
      "In this updated version, I've added two variables: `additionalInfo` for the text message and `linkURL` for the URL of the link. When the answer is incorrect, the `checkAnswer()` function now displays the incorrect GIF along with the additional information and a link. The link opens in a new tab (`target='_blank'`).\n",
      "\n",
      "Additionally, I've added a CSS class `.incorrect-message` to style the additional information text in red.\n"
     ]
    }
   ],
   "source": [
    "# Print a sample to understand the structure\n",
    "sample = hs3_edit['train'][0]\n",
    "print(\"Sample context:\")\n",
    "print(sample['context'])\n",
    "print(\"\\nSample edited_response:\")\n",
    "print(sample['edited_response'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64afcb1e",
   "metadata": {},
   "source": [
    "## üîß SFT Fine-tuning with HelpSteer3 Edit Dataset\n",
    "\n",
    "We'll use the `context` and `edited_response` columns to create training data for Supervised Fine-Tuning (SFT). The context already contains conversation messages, and we'll append the edited_response as the final assistant message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33fdf5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "together_client = Together(api_key=TOGETHER_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ece54808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed dataset size: 13740\n",
      "Sample messages count: 6\n"
     ]
    }
   ],
   "source": [
    "def map_to_sft_format(row):\n",
    "    \"\"\"\n",
    "    Convert HelpSteer3 edit row to SFT (Supervised Fine-Tuning) chat format.\n",
    "    \n",
    "    This function transforms a single row from the HelpSteer3 edit dataset into the \n",
    "    chat message format required for fine-tuning language models.\n",
    "    \n",
    "    Args:\n",
    "        row: A dictionary containing 'context' and 'edited_response' keys from the dataset\n",
    "        \n",
    "    Process:\n",
    "        1. Extracts the 'context' field, which contains a list of conversation messages\n",
    "           (typically alternating between user and assistant roles)\n",
    "        2. Creates a copy of these context messages to avoid modifying the original data\n",
    "        3. Appends the 'edited_response' as a new assistant message to complete the conversation\n",
    "           This edited_response represents the ideal/golden response for training\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with a single 'messages' key containing the complete conversation,\n",
    "        formatted as a list of message dictionaries with 'role' and 'content' fields\n",
    "        \n",
    "    Example:\n",
    "        Input row: {\n",
    "            'context': [\n",
    "                {'role': 'user', 'content': 'Hello'},\n",
    "                {'role': 'assistant', 'content': 'Hi there'}\n",
    "            ],\n",
    "            'edited_response': 'How can I help you today?'\n",
    "        }\n",
    "        \n",
    "        Output: {\n",
    "            'messages': [\n",
    "                {'role': 'user', 'content': 'Hello'},\n",
    "                {'role': 'assistant', 'content': 'Hi there'},\n",
    "                {'role': 'assistant', 'content': 'How can I help you today?'}\n",
    "            ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    messages = list(row['context'])  # Copy the context messages\n",
    "    messages.append({\"role\": \"assistant\", \"content\": row['edited_response']})\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# Apply transformation to the dataset\n",
    "train_sft = hs3_edit['train'].map(map_to_sft_format, remove_columns=hs3_edit['train'].column_names)\n",
    "print(f\"Transformed dataset size: {len(train_sft)}\")\n",
    "print(f\"Sample messages count: {len(train_sft[0]['messages'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9843db4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset format validation passed\n"
     ]
    }
   ],
   "source": [
    "# Validate the dataset format\n",
    "assert 'messages' in train_sft.column_names, \"Dataset must contain 'messages' column\"\n",
    "assert len(train_sft) > 0, \"Dataset must not be empty\"\n",
    "assert isinstance(train_sft[0]['messages'], list), \"Messages must be a list\"\n",
    "assert all('role' in msg and 'content' in msg for msg in train_sft[0]['messages']), \"Each message must have 'role' and 'content'\"\n",
    "print(\"‚úì Dataset format validation passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c38e6465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Good morning! Can you please write me a simple html and javascript that creates a quiz for students, using the variables I have defined below? The quiz will need to calculate the correct response, which is the average of 7 random numbers. Students will enter their response which will be checked against the calculated response, then a gif image will be displayed for \"correct\" or \"incorrect\".',\n",
       "  'role': 'user'},\n",
       " {'content': 'Sure, here\\'s an example of a simple HTML and JavaScript quiz that meets your requirements:\\n\\n```html\\n\\nSimple Quiz\\n\\nSimple Quiz\\n===========\\n\\nEnter your response:\\n\\n  \\n\\n\\nCheck Answer\\n\\nfunction checkAnswer() {\\n  var response = document.getElementById(\"response\").value;\\n  var correctResponse = (Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7) / 7;\\n\\n  if (response == correctResponse) {\\n    document.getElementById(\"result\").innerHTML = \"<img src=\\'correct.gif\\'>\";\\n  } else {\\n    document.getElementById(\"result\").innerHTML = \"<img src=\\'incorrect.gif\\'>\";\\n  }\\n}\\n```\\n\\nIn this example, the `checkAnswer()` function is called when the \"Check Answer\" button is clicked. It retrieves the student\\'s response from the input field, calculates the correct response using the average of 7 random numbers, and compares the two values. If the student\\'s response matches the correct response, it displays a \"correct\" gif image. Otherwise, it displays an \"incorrect\" gif image.\\n\\nNote that you\\'ll need to replace the `correct.gif` and `incorrect.gif` with the actual paths to your own correct and incorrect gif images.',\n",
       "  'role': 'assistant'},\n",
       " {'content': 'Thank you! Are you able to add some CSS so it looks a bit nicer?',\n",
       "  'role': 'user'},\n",
       " {'content': 'Sure, here\\'s an updated version of the HTML and JavaScript code with some basic CSS styling:\\n\\n```html\\n\\nSimple Quiz\\n\\nSimple Quiz\\n===========\\n\\nEnter your response:\\n\\n  \\n\\n\\nCheck Answer\\n\\nfunction checkAnswer() {\\n  var response = document.getElementById(\"response\").value;\\n  var correctResponse = (Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7 + Math.random() * 7) / 7;\\n\\n  if (response == correctResponse) {\\n    document.getElementById(\"result\").innerHTML = \"<img src=\\'correct.gif\\' style=\\'width: 200px; height: 200px;\\'>\";\\n  } else {\\n    document.getElementById(\"result\").innerHTML = \"<img src=\\'incorrect.gif\\' style=\\'width: 200px; height: 200px;\\'>\";\\n  }\\n}\\nbody {\\n  font-family: Arial, sans-serif;\\n  text-align: center;\\n}\\n\\nh1 {\\n  color: #333;\\n}\\n\\ninput[type=\"text\"] {\\n  padding: 10px;\\n  font-size: 16px;\\n  border-radius: 5px;\\n  border: 1px solid #ccc;\\n  margin-bottom: 10px;\\n}\\n\\nbutton {\\n  padding: 10px 20px;\\n  font-size: 16px;\\n  border-radius: 5px;\\n  border: none;\\n  background-color: #4CAF50;\\n  color: white;\\n  cursor: pointer;\\n}\\n\\nbutton:hover {\\n  background-color: #45a049;\\n}\\n\\n#result {\\n  margin-top: 20px;\\n}\\n```\\n\\nIn this updated version, the CSS styles are added to the `style` tags in the `head` section of the HTML document. The styles include setting the font family and text alignment for the body, setting the color for the `h1` element, setting the padding, font size, border radius, and border for the input field, setting the padding, font size, border radius, border, background color, and text color for the button, and setting the margin for the result element.\\n\\nFeel free to adjust the styles to your liking.',\n",
       "  'role': 'assistant'},\n",
       " {'content': 'Great! Can you please add a function so that if the answer is incorrect, the student is given some text with a link to find more information? This text and the link URL need to be stored in a variable.',\n",
       "  'role': 'user'},\n",
       " {'content': 'Certainly! Here\\'s the updated code with a function that displays additional information and a link when the answer is incorrect:\\n\\n```html\\n<!DOCTYPE html>\\r\\n<html>\\r\\n<head>\\r\\n  <style>\\r\\n    body {\\r\\n      font-family: Arial, sans-serif;\\r\\n      text-align: center;\\r\\n    }\\r\\n\\r\\n    h1 {\\r\\n      color: #333;\\r\\n    }\\r\\n\\r\\n    input[type=\"text\"] {\\r\\n      padding: 10px;\\r\\n      font-size: 16px;\\r\\n      border-radius: 5px;\\r\\n      border: 1px solid #ccc;\\r\\n      margin-bottom: 10px;\\r\\n    }\\r\\n\\r\\n    button {\\r\\n      padding: 10px 20px;\\r\\n      font-size: 16px;\\r\\n      border-radius: 5px;\\r\\n      border: none;\\r\\n      background-color: #4CAF50;\\r\\n      color: white;\\r\\n      cursor: pointer;\\r\\n    }\\r\\n\\r\\n    button:hover {\\r\\n      background-color: #45a049;\\r\\n    }\\r\\n\\r\\n    #result {\\r\\n      margin-top: 20px;\\r\\n    }\\r\\n\\r\\n    .incorrect-message {\\r\\n      margin-top: 20px;\\r\\n      color: red;\\r\\n    }\\r\\n  </style>\\r\\n</head>\\r\\n<body>\\r\\n\\r\\n  <h1>Simple Quiz</h1>\\r\\n\\r\\n  <p>Enter your response:</p>\\r\\n\\r\\n  <input type=\"text\" id=\"response\">\\r\\n\\r\\n  <br>\\r\\n\\r\\n  <button onclick=\"checkAnswer()\">Check Answer</button>\\r\\n\\r\\n  <div id=\"result\"></div>\\r\\n\\r\\n  <script>\\r\\n    // Variables for additional information and link\\r\\n    var additionalInfo = \"Sorry, that\\'s not correct. Here\\'s a link to help you learn more:\";\\r\\n    var linkURL = \"https://www.example.com/learn-more\";\\r\\n\\r\\n    // Generate the correct answer by averaging 7 random numbers\\r\\n    var total = 0;\\r\\n    for (var i = 0; i < 7; i++) {\\r\\n      total += Math.random() * 7;\\r\\n    }\\r\\n    var correctResponse = Math.round(total / 7);\\r\\n\\r\\n    // Function to check the user\\'s answer\\r\\n    function checkAnswer() {\\r\\n      // Get and parse the user\\'s input\\r\\n      var response = parseFloat(document.getElementById(\"response\").value);\\r\\n      console.log(response,correctResponse);\\r\\n      // Validate the input to ensure it\\'s a number\\r\\n      if(isNaN(response)){\\r\\n         document.getElementById(\"result\").innerHTML =\"<p>Please enter a valid number!</p>\"\\r\\n         return; // Exit the function if input is invalid\\r\\n       }\\r\\n     // Check if the response matches the correct answer\\r\\n     else if (response === correctResponse) {\\r\\n        document.getElementById(\"result\").innerHTML = \"<img src=\\'correct.gif\\' style=\\'width: 200px; height: 200px;\\' alt=\\'correct answer\\'>\"; \\r\\n        document.getElementById(\"result\").classList.remove(\"incorrect-message\");\\r\\n      }\\r\\n      // Otherwise check if the response is incorrect and provide a link for more information \\n      else {\\r\\n        document.getElementById(\"result\").innerHTML = \"<img src=\\'incorrect.gif\\' style=\\'width: 200px; height: 200px;\\' alt=\\'wrong answer\\'><div class=\\'incorrect-message\\'>\" + additionalInfo + \" <a href=\\'\" + linkURL + \"\\' target=\\'_blank\\'>Learn More</a></div>\"; \\r\\n      }\\r\\n    }\\r\\n  </script>\\r\\n\\r\\n</body>\\r\\n</html>\\n```\\n\\nIn this updated version, I\\'ve added two variables: `additionalInfo` for the text message and `linkURL` for the URL of the link. When the answer is incorrect, the `checkAnswer()` function now displays the incorrect GIF along with the additional information and a link. The link opens in a new tab (`target=\\'_blank\\'`).\\n\\nAdditionally, I\\'ve added a CSS class `.incorrect-message` to style the additional information text in red.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sft[0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06401658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 29.96ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training data to helpsteer3_sft_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save to JSONL file\n",
    "SFT_TRAIN_FILE = \"helpsteer3_sft_train.jsonl\"\n",
    "train_sft.to_json(SFT_TRAIN_FILE)\n",
    "print(f\"Saved training data to {SFT_TRAIN_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a10cb418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating file: 13740 lines [00:00, 58238.33 lines/s]\n",
      "Uploading file helpsteer3_sft_train.jsonl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79.0M/79.0M [00:05<00:00, 13.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file ID: file-d4186e06-621d-48e2-aa4e-f6d0c2afa325\n"
     ]
    }
   ],
   "source": [
    "# Upload file to Together AI\n",
    "train_file_resp = together_client.files.upload(SFT_TRAIN_FILE, purpose='fine-tune', check=True)\n",
    "print(f\"Uploaded file ID: {train_file_resp.id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a675019",
   "metadata": {},
   "source": [
    "### Launch Fine-tuning Job\n",
    "\n",
    "Configure and start the SFT fine-tuning job using the uploaded HelpSteer3 data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f8c1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_id = train_file_resp.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eace1ff",
   "metadata": {},
   "source": [
    "### Training Takes about 15 mins, so we can start it, and then proceed to Evaluations part without waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31bb05d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job ID: ft-2e1fb7c5-2e58\n",
      "Status: pending\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ft_resp = together_client.fine_tuning.create(\n",
    "    training_file=train_file_resp.id,\n",
    "    model=BASE_OSS_MODEL_FOR_FT,\n",
    "    train_on_inputs=False,\n",
    "    n_epochs=1,\n",
    "    n_checkpoints=1,\n",
    "    wandb_api_key=WANDB_API_KEY if WANDB_API_KEY else None,\n",
    "    lora=True,\n",
    "    suffix=\"helpsteer3-sft\",\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job ID: {ft_resp.id}\")\n",
    "print(f\"Status: {ft_resp.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbeea0",
   "metadata": {},
   "source": [
    "### Monitor Fine-tuning Progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc6dfea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: pending\n",
      "Fine-tuning request created\n",
      "ft-2e1fb7c5-2e58\n"
     ]
    }
   ],
   "source": [
    "# Check job status\n",
    "job_status = together_client.fine_tuning.retrieve(ft_resp.id)\n",
    "print(f\"Status: {job_status.status}\")\n",
    "\n",
    "# List events/logs\n",
    "for event in together_client.fine_tuning.list_events(id=ft_resp.id).data:\n",
    "    print(event.message)\n",
    "\n",
    "print(ft_resp.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac232bd",
   "metadata": {},
   "source": [
    "### Inference with Fine-tuned Model (when fine-tuning is finished)\n",
    "\n",
    "Once the job completes, test the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5066ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"ft-179f58d2-6079\",\n",
      "  \"training_file\": \"file-04348668-ed71-4526-9e77-4fcc824ab9bd\",\n",
      "  \"validation_file\": \"file-ef6acfbc-1eaa-42cc-9623-277ff9bd32b0\",\n",
      "  \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Reference\",\n",
      "  \"output_name\": \"ivprov/Meta-Llama-3.1-8B-Instruct-Reference-talkspace-dpo-8B-aa4a26b5\",\n",
      "  \"adapter_output_name\": null,\n",
      "  \"n_epochs\": 1,\n",
      "  \"n_checkpoints\": 1,\n",
      "  \"n_evals\": 10,\n",
      "  \"batch_size\": 8,\n",
      "  \"learning_rate\": 1e-05,\n",
      "  \"lr_scheduler\": {\n",
      "    \"lr_scheduler_type\": \"cosine\",\n",
      "    \"lr_scheduler_args\": {\n",
      "      \"min_lr_ratio\": 0.0,\n",
      "      \"num_cycles\": 0.5\n",
      "    }\n",
      "  },\n",
      "  \"warmup_ratio\": 0.0,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"weight_decay\": 0.0,\n",
      "  \"eval_steps\": 0,\n",
      "  \"training_type\": {\n",
      "    \"type\": \"Lora\"\n",
      "  },\n",
      "  \"created_at\": \"2025-11-04T12:57:17.129Z\",\n",
      "  \"updated_at\": \"2025-11-04T15:11:47.746Z\",\n",
      "  \"status\": \"completed\",\n",
      "  \"job_id\": \"ft-179f58d2-6079\",\n",
      "  \"token_count\": 11579857,\n",
      "  \"param_count\": 8030261248,\n",
      "  \"total_price\": 21368624400,\n",
      "  \"total_steps\": 5471,\n",
      "  \"steps_completed\": 5471,\n",
      "  \"epochs_completed\": 1,\n",
      "  \"evals_completed\": 10,\n",
      "  \"queue_depth\": null,\n",
      "  \"wandb_base_url\": \"\",\n",
      "  \"wandb_project_name\": \"talkspace\",\n",
      "  \"wandb_name\": \"\",\n",
      "  \"wandb_url\": \"https://wandb.ai/vprov/talkspace/runs/fqow8e49\",\n",
      "  \"training_file_num_lines\": null,\n",
      "  \"training_file_size\": null,\n",
      "  \"train_on_inputs\": \"auto\",\n",
      "  \"from_checkpoint\": null,\n",
      "  \"progress\": null,\n",
      "  \"adapter_output_path\": \"s3://together-dev/finetune/67573d8a7f3f0de92d0489ed/ivprov/Meta-Llama-3.1-8B-Instruct-Reference-talkspace-dpo-8B-aa4a26b5/ft-179f58d2-6079_adapter-2025-11-04-15-09-00\",\n",
      "  \"checkpoints\": [],\n",
      "  \"continued_checkpoint\": \"\",\n",
      "  \"eval_price\": 7472796000,\n",
      "  \"eval_token_count\": 622733,\n",
      "  \"evals_paid_for\": 10,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"max_steps\": -1,\n",
      "  \"merge_parent_adapter\": false,\n",
      "  \"model_output_path\": \"s3://together-dev/finetune/67573d8a7f3f0de92d0489ed/ivprov/Meta-Llama-3.1-8B-Instruct-Reference-talkspace-dpo-8B-aa4a26b5/ft-179f58d2-6079-2025-11-04-15-09-00\",\n",
      "  \"multimodal_params\": {},\n",
      "  \"parent_ft_id\": \"\",\n",
      "  \"random_seed\": \"null\",\n",
      "  \"refund_amount\": 0,\n",
      "  \"save_steps\": 0,\n",
      "  \"started_at\": null,\n",
      "  \"steps_paid_for\": 5471,\n",
      "  \"suffix\": \"talkspace-dpo-8B\",\n",
      "  \"train_price\": 13895828400,\n",
      "  \"training_files\": null,\n",
      "  \"training_method\": {\n",
      "    \"dpo_beta\": 0.1,\n",
      "    \"method\": \"dpo\"\n",
      "  },\n",
      "  \"try_byoa_upload\": true,\n",
      "  \"user_id\": \"67573d8a7f3f0de92d0489ed\",\n",
      "  \"validation_files\": null,\n",
      "  \"validation_split_ratio\": 0,\n",
      "  \"warmup_steps\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "job_status = together_client.fine_tuning.retrieve(ft_resp.id)\n",
    "status_dict = job_status.model_dump()\n",
    "status_dict.pop('events', None)\n",
    "print(json.dumps(status_dict, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61978009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model: ivprov/Meta-Llama-3.1-8B-Instruct-Reference-helpsteer3-sft-d5865876\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = job_status.output_name\n",
    "print(f\"Fine-tuned model: {finetuned_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e41ba666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Factorial Function in Python**\n",
      "================================\n",
      "\n",
      "Here's a simple Python function to calculate the factorial of a given number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a given number.\n",
      "\n",
      "    Args:\n",
      "        n (int): The number to calculate the factorial of.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If n is a negative number.\n",
      "    \"\"\"\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Input must be a non-negative integer.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "```\n",
      "\n",
      "**Example Use Cases**\n",
      "--------------------\n",
      "\n",
      "```python\n",
      "print(factorial(5))  # Output: 120\n",
      "print(factorial(0))  # Output: 1\n",
      "print(factorial(1))  # Output: 1\n",
      "```\n",
      "\n",
      "**Alternative Recursive Implementation**\n",
      "--------------------------------------\n",
      "\n",
      "If you prefer a more concise recursive implementation, you can use the following function:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a given number.\n",
      "\n",
      "    Args:\n",
      "        n (int): The number to calculate the factorial of.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If n is a negative number.\n",
      "    \"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Input must be a non-negative integer.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "```\n",
      "\n",
      "**Iterative Implementation**\n",
      "---------------------------\n",
      "\n",
      "If you prefer an iterative implementation, you can use the following function:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a given number.\n",
      "\n",
      "    Args:\n",
      "        n (int): The number to calculate the factorial of.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If n is a negative number.\n",
      "    \"\"\"\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Input must be a non-negative integer.\")\n",
      "    result = 1\n",
      "    for i in range(1, n+1):\n",
      "        result *= i\n",
      "    return result\n",
      "```\n",
      "\n",
      "Note that the iterative implementation is generally more efficient than the recursive implementation, especially for large values of n.\n"
     ]
    }
   ],
   "source": [
    "# Test with a sample prompt, LoRA adapter takes time to load\n",
    "response = together_client.chat.completions.create(\n",
    "    model=finetuned_model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a Python function to calculate factorial.\"}],\n",
    "    max_tokens=512,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c33fd0",
   "metadata": {},
   "source": [
    "## üîÑ Preparing Data for Evaluation\n",
    "\n",
    "We'll create a test subset from the HelpSteer3 dataset and prepare it for evaluation. The evaluation will compare:\n",
    "1. Proprietary model (claude) vs Golden Answer\n",
    "2. Base OSS model vs Golden answer (edited_response) - as a baseline comparison \n",
    "3. Fine-tuned OSS model vs Golden answer (edited_response) - to measure fine-tuning effectiveness\n",
    "\n",
    "We will use \"openai/gpt-5.2\" as a Judge for comparisons.\n",
    "\n",
    "We need to:\n",
    "- Apply a chat template to convert the context messages into a formatted prompt string\n",
    "- Include the golden answer (edited_response) for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60e2fbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test subset size: 50\n"
     ]
    }
   ],
   "source": [
    "# Let's use 200 samples for validation for speed\n",
    "VALIDATION_SIZE = 50\n",
    "test_data = hs3_edit['validation'].select(range(VALIDATION_SIZE))\n",
    "print(f\"Test subset size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cb04ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta-llama/Meta-Llama-3.1-8B-Instruct-Reference'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_OSS_MODEL_FOR_FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cb2cffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 50 samples for evaluation\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer for the base model to apply proper chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "def prepare_eval_data(row):\n",
    "    \"\"\"Prepare a single row for evaluation with formatted context and golden answer.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract the conversation context from the row (list of message dicts with 'role' and 'content')\n",
    "    2. Apply the chat template to format the context into a single prompt string\n",
    "       - tokenize=False: Return formatted string instead of token IDs\n",
    "       - add_generation_prompt=False: Don't add the assistant's generation prompt at the end\n",
    "    3. Extract the golden answer (edited_response) from the row\n",
    "    4. Return a dictionary with:\n",
    "       - 'context_formatted': The formatted conversation prompt as a string\n",
    "       - 'golden_answer': The reference response to compare against\n",
    "    \"\"\"\n",
    "    context_formatted = tokenizer.apply_chat_template(\n",
    "        row['context'], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\n",
    "        'context_formatted': context_formatted,\n",
    "        'golden_answer': row['edited_response']\n",
    "    }\n",
    "\n",
    "# Transform test data for evaluation\n",
    "eval_data = [prepare_eval_data(row) for row in test_data]\n",
    "print(f\"Prepared {len(eval_data)} samples for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c7e5b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Eval data format validated successfully\n"
     ]
    }
   ],
   "source": [
    "# Verify the eval_data format\n",
    "assert len(eval_data) > 0, \"eval_data should not be empty\"\n",
    "assert all('context_formatted' in item for item in eval_data), \"All items must have 'context_formatted' key\"\n",
    "assert all('golden_answer' in item for item in eval_data), \"All items must have 'golden_answer' key\"\n",
    "assert all(isinstance(item['context_formatted'], str) for item in eval_data), \"context_formatted must be strings\"\n",
    "assert all(isinstance(item['golden_answer'], str) for item in eval_data), \"golden_answer must be strings\"\n",
    "print(\"‚úì Eval data format validated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84ad5aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file helpsteer3_eval.jsonl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400k/400k [00:01<00:00, 232kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded eval file ID: file-3d3546f2-c399-4bfc-8827-6c4607b192d9\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Save evaluation data to JSONL and upload\n",
    "# Note: For eval files, we need to wrap the data in the expected format with 'text' field\n",
    "EVAL_FILE = \"helpsteer3_eval.jsonl\"\n",
    "with open(EVAL_FILE, 'w') as f:\n",
    "    for eval_item in eval_data:\n",
    "        json.dump(eval_item, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "uploaded_eval_file = together_client.files.upload(file=EVAL_FILE, purpose='eval', check=False)\n",
    "print(f\"Uploaded eval file ID: {uploaded_eval_file.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e5f4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and judge configuration\n",
    "\n",
    "# Input template should add a reference to the context_formatted to allow the model to use it for generation\n",
    "# Judge template should also reference the \"context_formatted\" to allow the judge \n",
    "#  to see what was the cotext for the generation\n",
    "\n",
    "# To refer a field from a dataset in python, please use 4 brackets: {{{{field_name}}}}\n",
    "\n",
    "JUDGE_TEMPLATE = f\"\"\"You are an expert judge evaluating the quality of AI assistant responses.\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "1. **Helpfulness**: Does the response fully address the user's request?\n",
    "2. **Accuracy**: Is the information correct and free from errors?\n",
    "3. **Clarity**: Is the response well-organized and easy to understand?\n",
    "4. **Completeness**: Does it cover all relevant aspects of the query?\n",
    "5. **Safety**: Is the response appropriate and safe?\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Read the conversation context carefully\n",
    "- Compare Response A and Response B\n",
    "- Determine which response better satisfies the user's needs\n",
    "- Provide a brief justification (2-3 sentences)\n",
    "\n",
    "The context for the reply was:\n",
    "{{{{context_formatted}}}}\n",
    "\"\"\"\n",
    "\n",
    "# Model config for generation from context\n",
    "generation_system_template = \"You are a helpful AI assistant.\"\n",
    "input_template = f\"{{{{context_formatted}}}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "891c728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert judge evaluating the quality of AI assistant responses.\n",
      "\n",
      "EVALUATION CRITERIA:\n",
      "1. **Helpfulness**: Does the response fully address the user's request?\n",
      "2. **Accuracy**: Is the information correct and free from errors?\n",
      "3. **Clarity**: Is the response well-organized and easy to understand?\n",
      "4. **Completeness**: Does it cover all relevant aspects of the query?\n",
      "5. **Safety**: Is the response appropriate and safe?\n",
      "\n",
      "INSTRUCTIONS:\n",
      "- Read the conversation context carefully\n",
      "- Compare Response A and Response B\n",
      "- Determine which response better satisfies the user's needs\n",
      "- Provide a brief justification (2-3 sentences)\n",
      "\n",
      "The context for the reply was:\n",
      "<<This is a test context>>\n"
     ]
    }
   ],
   "source": [
    "# Test that jinja actually works\n",
    "from jinja2 import Template\n",
    "test_context = \"<<This is a test context>>\"\n",
    "test_template = Template(JUDGE_TEMPLATE)\n",
    "rendered = test_template.render(context_formatted=test_context)\n",
    "assert test_context in rendered, \"Jinja template rendering failed\"\n",
    "print(rendered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4baa246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 8096\n",
    "TEMPERATURE = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a2de5",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Evaluation 1: Proprietary Model vs Golden Answer\n",
    "\n",
    "Compare the proprietary model's output against the golden answer (edited_response) from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a11554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6947a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 1 (Proprietary vs Golden) ID: eval-5afc-1766166945\n",
      "Status: pending\n"
     ]
    }
   ],
   "source": [
    "# Evaluation 1: Proprietary model (generates) vs Golden answer (from data)\n",
    "proprietary_model_config = {\n",
    "    \"model\": PROPRIETARY_BASE_MODEL,\n",
    "    \"model_source\": \"external\",\n",
    "    \"system_template\": generation_system_template,\n",
    "    \"input_template\": input_template,\n",
    "    \"max_tokens\": MAX_TOKENS,\n",
    "    \"temperature\": TEMPERATURE,\n",
    "    \"external_api_token\": OPENAI_API_KEY\n",
    "}\n",
    "\n",
    "eval_proprietary_vs_golden = together_client.evaluation.create(\n",
    "    type=\"compare\",\n",
    "    input_data_file_path=uploaded_eval_file.id,\n",
    "    judge_model=JUDGE_MODEL,\n",
    "    judge_model_source=\"serverless\",\n",
    "    judge_system_template=JUDGE_TEMPLATE,\n",
    "    model_a=proprietary_model_config,\n",
    "    model_b=\"golden_answer\"\n",
    ")\n",
    "\n",
    "print(f\"Eval 1 (Proprietary vs Golden) ID: {eval_proprietary_vs_golden.workflow_id}\")\n",
    "print(f\"Status: {eval_proprietary_vs_golden.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ee809",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Evaluation 2: Base Model vs Golden Answer\n",
    "\n",
    "Compare the base OSS model's output against the golden answer (edited_response) to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27f5e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 2 (Base vs Golden) ID: eval-deac-1766168048\n",
      "Status: pending\n"
     ]
    }
   ],
   "source": [
    "# Evaluation 2: Base OSS model (generates) vs Golden answer (from data)\n",
    "base_model_config = {\n",
    "    \"model\": BASE_OSS_MODEL_FOR_INFERENCE,\n",
    "    \"model_source\": \"serverless\",\n",
    "    \"system_template\": generation_system_template,\n",
    "    \"input_template\": input_template,\n",
    "    \"max_tokens\": MAX_TOKENS,\n",
    "    \"temperature\": TEMPERATURE\n",
    "}\n",
    "\n",
    "eval_base_vs_golden = together_client.evaluation.create(\n",
    "    type=\"compare\",\n",
    "    input_data_file_path=uploaded_eval_file.id,\n",
    "    judge_model=JUDGE_MODEL,\n",
    "    judge_model_source=\"serverless\",\n",
    "    judge_system_template=JUDGE_TEMPLATE,\n",
    "    model_a=base_model_config,\n",
    "    model_b=\"golden_answer\"\n",
    ")\n",
    "\n",
    "print(f\"Eval 2 (Base vs Golden) ID: {eval_base_vs_golden.workflow_id}\")\n",
    "print(f\"Status: {eval_base_vs_golden.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e0b9f",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Evaluation 3: Fine-tuned Model vs Golden Answer\n",
    "\n",
    "Compare the fine-tuned model's output against the golden answer (edited_response) to measure fine-tuning effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7749dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can customize what model do you want to use\n",
    "# This is an example of a fine-tuned model that is available on serverless\n",
    "finetuned_model = \"ivprov/Meta-Llama-3.1-8B-Instruct-Reference-helpsteer3-sft-d5865876\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09212276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval 3 (Fine-tuned vs Golden) ID: eval-0535-1766168059\n",
      "Status: pending\n"
     ]
    }
   ],
   "source": [
    "# Evaluation 3: Fine-tuned model (generates) vs Golden answer (from data)\n",
    "finetuned_model_config = {\n",
    "    \"model\": finetuned_model,  # From fine-tuning job\n",
    "    \"model_source\": \"serverless\",\n",
    "    \"system_template\": generation_system_template,\n",
    "    \"input_template\": input_template,\n",
    "    \"max_tokens\": MAX_TOKENS,\n",
    "    \"temperature\": TEMPERATURE\n",
    "}\n",
    "\n",
    "eval_finetuned_vs_golden = together_client.evaluation.create(\n",
    "    type=\"compare\",\n",
    "    input_data_file_path=uploaded_eval_file.id,\n",
    "    judge_model=JUDGE_MODEL,\n",
    "    judge_model_source=\"serverless\",\n",
    "    judge_system_template=JUDGE_TEMPLATE,\n",
    "    model_a=finetuned_model_config,\n",
    "    model_b=\"golden_answer\"\n",
    ")\n",
    "\n",
    "print(f\"Eval 3 (Fine-tuned vs Golden) ID: {eval_finetuned_vs_golden.workflow_id}\")\n",
    "print(f\"Status: {eval_finetuned_vs_golden.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dee2b8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All evaluations completed!\n",
      "status=<EvaluationStatus.COMPLETED: 'completed'> results={'A_wins': 45, 'B_wins': 0, 'Ties': 5, 'generation_fail_count': 0, 'judge_fail_count': 0, 'result_file_id': 'file-edec6f11-f25d-4892-9fa6-8d7f3f655406'}\n",
      "status=<EvaluationStatus.COMPLETED: 'completed'> results={'A_wins': 2, 'B_wins': 31, 'Ties': 17, 'generation_fail_count': 0, 'judge_fail_count': 0, 'result_file_id': 'file-30ce9ba7-460e-4ca3-9703-b0b640a33e10'}\n",
      "status=<EvaluationStatus.COMPLETED: 'completed'> results={'A_wins': 1, 'B_wins': 37, 'Ties': 12, 'generation_fail_count': 0, 'judge_fail_count': 0, 'result_file_id': 'file-5f377571-d96b-40b9-9d49-2d9518c76002'}\n"
     ]
    }
   ],
   "source": [
    "# Get status for all evaluations and wait until they have results\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    status_proprietary = together_client.evaluation.status(eval_proprietary_vs_golden.workflow_id)\n",
    "    status_base = together_client.evaluation.status(eval_base_vs_golden.workflow_id)\n",
    "    status_finetuned = together_client.evaluation.status(eval_finetuned_vs_golden.workflow_id)\n",
    "    \n",
    "    if (status_proprietary.results and status_base.results and status_finetuned.results):\n",
    "        break\n",
    "    \n",
    "    print(\"Waiting for evaluations to complete...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "print(\"All evaluations completed!\")\n",
    "print(status_proprietary)\n",
    "print(status_base)\n",
    "print(status_finetuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51be512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb8172ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Eval 1: Proprietary vs Golden\n",
      "============================================================\n",
      "Total samples: 50\n",
      "Proprietary Model wins: 45 (90.0%)\n",
      "Golden Answer wins: 0 (0.0%)\n",
      "Ties: 5 (10.0%)\n",
      "‚úÖ Winner: Proprietary Model\n",
      "\n",
      "============================================================\n",
      "Eval 2: Base vs Golden\n",
      "============================================================\n",
      "Total samples: 50\n",
      "Base Model wins: 37 (74.0%)\n",
      "Golden Answer wins: 2 (4.0%)\n",
      "Ties: 11 (22.0%)\n",
      "‚úÖ Winner: Base Model\n",
      "\n",
      "============================================================\n",
      "Eval 3: Fine-tuned vs Golden\n",
      "============================================================\n",
      "Total samples: 50\n",
      "Fine-tuned Model wins: 3 (6.0%)\n",
      "Golden Answer wins: 36 (72.0%)\n",
      "Ties: 11 (22.0%)\n",
      "‚úÖ Winner: Golden Answer\n"
     ]
    }
   ],
   "source": [
    "def print_comparison_summary(status, eval_name, model_a_name, model_b_name):\n",
    "    \"\"\"Print a summary of comparison results.\"\"\"\n",
    "    if not status.results:\n",
    "        print(f\"{eval_name}: Results not available yet\")\n",
    "        return\n",
    "    \n",
    "    results = status.results\n",
    "    total = results.get('A_wins', 0) + results.get('B_wins', 0) + results.get('Ties', 0)\n",
    "    if total == 0:\n",
    "        print(f\"{eval_name}: No results yet\")\n",
    "        return\n",
    "    \n",
    "    a_wins = results.get('A_wins', 0)\n",
    "    b_wins = results.get('B_wins', 0)\n",
    "    ties = results.get('Ties', 0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{eval_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total samples: {total}\")\n",
    "    print(f\"{model_a_name} wins: {a_wins} ({a_wins/total*100:.1f}%)\")\n",
    "    print(f\"{model_b_name} wins: {b_wins} ({b_wins/total*100:.1f}%)\")\n",
    "    print(f\"Ties: {ties} ({ties/total*100:.1f}%)\")\n",
    "    \n",
    "    if a_wins > b_wins:\n",
    "        print(f\"‚úÖ Winner: {model_a_name}\")\n",
    "    elif b_wins > a_wins:\n",
    "        print(f\"‚úÖ Winner: {model_b_name}\")\n",
    "    else:\n",
    "        print(\"ü§ù It's a tie!\")\n",
    "\n",
    "# Display results summary using status from previous cell\n",
    "print_comparison_summary(status_proprietary, \"Eval 1: Proprietary vs Golden\", \"Proprietary Model\", \"Golden Answer\")\n",
    "print_comparison_summary(status_base, \"Eval 2: Base vs Golden\", \"Base Model\", \"Golden Answer\")\n",
    "print_comparison_summary(status_finetuned, \"Eval 3: Fine-tuned vs Golden\", \"Fine-tuned Model\", \"Golden Answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90874cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "967fc8eb",
   "metadata": {},
   "source": [
    "### We can also download file with the results and feedback from LLM Judge about each decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "abf06d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-5f377571-d96b-40b9-9d49-2d9518c76002'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_finetuned.results['result_file_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3f2a3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file helpsteer3_finetuned_vs_golden_results.jsonl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 594k/594k [00:00<00:00, 3.23MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download results from the comparison of the fine-tuned model against golden answer\n",
    "OUTPUT_FILE_NAME = \"./helpsteer3_finetuned_vs_golden_results.jsonl\"\n",
    "results_finetuned = together_client.files.retrieve_content(\n",
    "    'file-5f377571-d96b-40b9-9d49-2d9518c76002',\n",
    "    output=OUTPUT_FILE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ff657f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the results file:\n",
      "  - context_formatted\n",
      "  - golden_answer\n",
      "  - MODEL_TO_EVALUATE_OUTPUT_A\n",
      "  - evaluation_successful\n",
      "  - choice_original\n",
      "  - judge_feedback_original_order\n",
      "  - choice_flipped\n",
      "  - judge_feedback_flipped_order\n",
      "  - final_decision\n",
      "  - is_invalid_judge_output\n"
     ]
    }
   ],
   "source": [
    "# Read the results file and print columns for the first line\n",
    "import json\n",
    "\n",
    "with open(OUTPUT_FILE_NAME, 'r') as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "    print(\"Columns in the results file:\")\n",
    "    for key in first_line.keys():\n",
    "        print(f\"  - {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a401e7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FIRST ROW DETAILS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!! Context Formatted:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you tell me what is wrong with this Go code:\n",
      "\n",
      "package main\n",
      "\n",
      "import (\n",
      "\t\"fmt\"\n",
      "\t\"os\"\n",
      ")\n",
      "\n",
      "type Page struct {\n",
      "\tTitle string\n",
      "\tBody  []byte\n",
      "}\n",
      "\n",
      "func (p *Page) save() error {\n",
      "\tfilename := p.Title + \".txt\"\n",
      "\treturn os.WriteFile(filename, p.Body, 0600)\n",
      "}\n",
      "\n",
      "func loadPage(title string) *Page {\n",
      "\tfilename := title + \".txt\"\n",
      "\tbody, err := os.ReadFile(filename)\n",
      "\tif err != nil {\n",
      "\t\treturn nil, err\n",
      "\t}\n",
      "\treturn &Page{Title: title, Body: body}, nil\n",
      "}\n",
      "\n",
      "func main() {\n",
      "\tp1 := &Page{Title: \"Test Page\", Body: []byte(\"This is a sample page.\")}\n",
      "\tp1.save()\n",
      "\tp2, _ := loadPage(\"TestPage\")\n",
      "\tfmt.Println(string(p2.Body))\n",
      "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "There are a couple of issues with the provided Go code:\n",
      "\n",
      "1. The `loadPage` function is declared to return a single `*Page` value, but it attempts to return two values (a `*Page` and an `error`). To fix this, you should change the function signature to return both values:\n",
      "```go\n",
      "func loadPage(title string) (*Page, error) {\n",
      "```\n",
      "2. In the `main` function, you are ignoring the error returned by the `loadPage` function. This is generally not a good practice, as it can lead to unexpected behavior or bugs. You should handle the error appropriately, for example by printing it or returning from the function:\n",
      "```go\n",
      "p2, err := loadPage(\"TestPage\")\n",
      "if err != nil {\n",
      "    fmt.Println(\"Error loading page:\", err)\n",
      "    return\n",
      "}\n",
      "fmt.Println(string(p2.Body))\n",
      "```\n",
      "Here's the corrected code:\n",
      "```go\n",
      "package main\n",
      "\n",
      "import (\n",
      "\t\"fmt\"\n",
      "\t\"os\"\n",
      ")\n",
      "\n",
      "type Page struct {\n",
      "\tTitle string\n",
      "\tBody  []byte\n",
      "}\n",
      "\n",
      "func (p *Page) save() error {\n",
      "\tfilename := p.Title + \".txt\"\n",
      "\treturn os.WriteFile(filename, p.Body, 0600)\n",
      "}\n",
      "\n",
      "func loadPage(title string) (*Page, error) {\n",
      "\tfilename := title + \".txt\"\n",
      "\tbody, err := os.ReadFile(filename)\n",
      "\tif err != nil {\n",
      "\t\treturn nil, err\n",
      "\t}\n",
      "\treturn &Page{Title: title, Body: body}, nil\n",
      "}\n",
      "\n",
      "func main() {\n",
      "\tp1 := &Page{Title: \"Test Page\", Body: []byte(\"This is a sample page.\")}\n",
      "\terr := p1.save()\n",
      "\tif err != nil {\n",
      "\t\tfmt.Println(\"Error saving page:\", err)\n",
      "\t\treturn\n",
      "\t}\n",
      "\tp2, err := loadPage(\"TestPage\")\n",
      "\tif err != nil {\n",
      "\t\tfmt.Println(\"Error loading page:\", err)\n",
      "\t\treturn\n",
      "\t}\n",
      "\tfmt.Println(string(p2.Body))\n",
      "}\n",
      "```<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "OK, I fixed that, how about now:\n",
      "\n",
      "package main\n",
      "\n",
      "import (\n",
      "\t\"fmt\"\n",
      "\t\"os\"\n",
      ")\n",
      "\n",
      "type Page struct {\n",
      "\tTitle string\n",
      "\tBody  []byte\n",
      "}\n",
      "\n",
      "func (p *Page) save() error {\n",
      "\tfilename := p.Title + \".txt\"\n",
      "\treturn os.WriteFile(filename, p.Body, 0600)\n",
      "}\n",
      "\n",
      "func loadPage(title string) (*Page, error) {\n",
      "\tfilename := title + \".txt\"\n",
      "\tbody, err := os.ReadFile(filename)\n",
      "\tif err != nil {\n",
      "\t\treturn nil, err\n",
      "\t}\n",
      "\treturn &Page{Title: title, Body: body}, nil\n",
      "}\n",
      "\n",
      "func main() {\n",
      "\tp1 := &Page{Title: \"Test Page\", Body: []byte(\"This is a sample page.\")}\n",
      "\tp1.save()\n",
      "\tp2, _ := loadPage(\"TestPage\")\n",
      "\tfmt.Println(string(p2.Body))\n",
      "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The code still has an issue:\n",
      "\n",
      "1. In the `main` function, you are ignoring the error returned by the `loadPage` function. This is generally not a good practice, as it can lead to unexpected behavior or bugs. You should handle the error appropriately, for example by printing it or returning from the function:\n",
      "```go\n",
      "p2, err := loadPage(\"TestPage\")\n",
      "if err != nil {\n",
      "    fmt.Println(\"Error loading page:\", err)\n",
      "    return\n",
      "}\n",
      "fmt.Println(string(p2.Body))\n",
      "```\n",
      "Here's the corrected code:\n",
      "```go\n",
      "package main\n",
      "\n",
      "import (\n",
      "\t\"fmt\"\n",
      "\t\"os\"\n",
      ")\n",
      "\n",
      "type Page struct {\n",
      "\tTitle string\n",
      "\tBody  []byte\n",
      "}\n",
      "\n",
      "func (p *Page) save() error {\n",
      "\tfilename := p.Title + \".txt\"\n",
      "\treturn os.WriteFile(filename, p.Body, 0600)\n",
      "}\n",
      "\n",
      "func loadPage(title string) (*Page, error) {\n",
      "\tfilename := title + \".txt\"\n",
      "\tbody, err := os.ReadFile(filename)\n",
      "\tif err != nil {\n",
      "\t\treturn nil, err\n",
      "\t}\n",
      "\treturn &Page{Title: title, Body: body}, nil\n",
      "}\n",
      "\n",
      "func main() {\n",
      "\tp1 := &Page{Title: \"Test Page\", Body: []byte(\"This is a sample page.\")}\n",
      "\terr := p1.save()\n",
      "\tif err != nil {\n",
      "\t\tfmt.Println(\"Error saving page:\", err)\n",
      "\t\treturn\n",
      "\t}\n",
      "\tp2, err := loadPage(\"TestPage\")\n",
      "\tif err != nil {\n",
      "\t\tfmt.Println(\"Error loading page:\", err)\n",
      "\t\treturn\n",
      "\t}\n",
      "\tfmt.Println(string(p2.Body))\n",
      "}\n",
      "```<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What does this code do?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "This code defines a `Page` struct with two fields: `Title` and `Body`. It also defines two methods for the `Page` struct: `save` and `loadPage`.\n",
      "\n",
      "The `save` method writes the contents of the `Body` field to a file with the name `Title.txt` and the permissions `0600`.\n",
      "\n",
      "The `loadPage` function takes a `title` string as an argument and returns a pointer to a `Page` struct and an error. It reads the contents of a file with the name `title.txt` and returns a new `Page` struct with the `Title` field set to `title` and the `Body` field set to the contents of the file. If the file does not exist or cannot be read, it returns `nil` and an error.\n",
      "\n",
      "The `main` function creates a new `Page` struct with the `Title` field set to \"Test Page\" and the `Body` field set to the bytes of the string \"This is a sample page.\". It then calls the `save` method to write the contents of the `Body` field to a file named \"Test Page.txt\". It then calls the `loadPage` function to read the contents of the file \"TestPage.txt\" and print the contents of the `Body` field to the console. If the file does not exist or cannot be read, it will print an error message to the console.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Are there any additional errors:\n",
      "\n",
      "package main\n",
      "\n",
      "import (\n",
      "\t\"fmt\"\n",
      "\t\"os\"\n",
      ")\n",
      "\n",
      "type Page struct {\n",
      "\tTitle string\n",
      "\tBody  []byte\n",
      "}\n",
      "\n",
      "func (p *Page) save() error {\n",
      "\tfilename := p.Title + \".txt\"\n",
      "\treturn os.WriteFile(filename, p.Body, 0600)\n",
      "}\n",
      "\n",
      "func loadPage(title string) (*Page, error) {\n",
      "\tfilename := title + \".txt\"\n",
      "\tbody, err := os.ReadFile(filename)\n",
      "\tif err != nil {\n",
      "\t\treturn nil, err\n",
      "\t}\n",
      "\treturn &Page{Title: title, Body: body}, nil\n",
      "}\n",
      "\n",
      "func main() {\n",
      "\tp1 := &Page{Title: \"TestPage\", Body: []byte(\"This is a sample page.\")}\n",
      "\tp1.save()\n",
      "\tp2, _ := loadPage(\"TestPage\")\n",
      "\tfmt.Println(string(p2.Body))\n",
      "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "There are no additional errors in the provided code. It should work as expected, assuming that the file \"TestPage.txt\" is writable and readable by the program.\n",
      "\n",
      "However, as I mentioned earlier, it is generally not a good practice to ignore the error returned by the `loadPage` function. You should handle the error appropriately, for example by printing it or returning from the function:\n",
      "```go\n",
      "p2, err := loadPage(\"TestPage\")\n",
      "if err != nil {\n",
      "    fmt.Println(\"Error loading page:\", err)\n",
      "    return\n",
      "}\n",
      "fmt.Println(string(p2.Body))\n",
      "```\n",
      "This will ensure that the program does not continue executing if there is an error loading the page, and will print an error message to the console instead.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What does the first line do in this code:\n",
      "\n",
      "//go:build ignore\n",
      "\n",
      "package main\n",
      "\n",
      "import (\n",
      "    \"fmt\"\n",
      "    \"log\"\n",
      "    \"net/http\"\n",
      ")\n",
      "\n",
      "func handler(w http.ResponseWriter, r *http.Request) {\n",
      "    fmt.Fprintf(w, \"Hi there, I love %s!\", r.URL.Path[1:])\n",
      "}\n",
      "\n",
      "func main() {\n",
      "    http.HandleFunc(\"/\", handler)\n",
      "    log.Fatal(http.ListenAndServe(\":8080\", nil))\n",
      "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The first line in the provided code is a build constraint comment:\n",
      "```go\n",
      "//go:build ignore\n",
      "```\n",
      "This comment tells the Go compiler to ignore this file when building the program. This can be useful for excluding certain files from the build process, for example if they contain code that is not needed for the final executable.\n",
      "\n",
      "In this case, the build constraint comment is not necessary, as the file is included in the `main` package and will be compiled and linked into the final executable by default. However, it is still a valid comment and will not cause any errors or warnings.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "If I want to test this program in a different directory, do I need to remove that line to build it?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "No, you do not need to remove the build constraint comment to build the program in a different directory. The build constraint comment only affects the build process, and does not prevent the program from being compiled and linked into an executable.\n",
      "\n",
      "To build the program in a different directory, you can simply run the `go build` command in that directory, and the Go compiler will automatically include the necessary files in the build process. For example:\n",
      "```bash\n",
      "$ cd /path/to/different/directory\n",
      "$ go build -o myprogram main.go\n",
      "```\n",
      "This will build the program and create an executable file named `myprogram` in the current directory. The build constraint comment in `main.go` will be ignored, and the file will be included in the build process as usual.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Why is there an asterisk in front of http.Request but not in front of http.ResponseWriter?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "In Go, the `http.Request` and `http.ResponseWriter` types are both interfaces. An interface is a set of methods that a type must implement in order to satisfy the interface.\n",
      "\n",
      "The `http.Request` interface has a single method, `FormValue`, which returns the value of a form field with the given name. The `http.ResponseWriter` interface has several methods, including `Write`, `WriteHeader`, and `Header`, which allow you to write the response body, set the response status code, and set response headers, respectively.\n",
      "\n",
      "In Go, when a function takes an interface as an argument, it is common to use a pointer to the interface type, rather than the interface type itself. This is because interfaces are implemented by concrete types, and it is often more efficient to pass a pointer to a concrete type than to pass the concrete type itself.\n",
      "\n",
      "In the case of the `handler` function in the provided code, the `http.Request` argument is a pointer to the `http.Request` interface, and the `http.ResponseWriter` argument is the `http.ResponseWriter` interface itself. This is because the `handler` function does not need to modify the `http.Request` object, but it does need to write to the `http.ResponseWriter` object.\n",
      "\n",
      "Using a pointer to the `http.Request` interface allows the `handler` function to access the methods of the `http.Request` interface, while using the `http.ResponseWriter` interface itself allows the function to write to the response body and set response headers.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you add handlers for edit and save to this:\n",
      "\n",
      "func main() {\n",
      "\thttp.HandleFunc(\"/view/\", viewHandler)\n",
      "\tlog.Fatal(http.ListenAndServe(\":8080\", nil))\n",
      "}<|eot_id|>\n",
      "\n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!! MODEL_TO_EVALUATE_OUTPUT_A:\n",
      "Here's an updated version of the `main` function with handlers for `edit` and `save`:\n",
      "```go\n",
      "func main() {\n",
      "\thttp.HandleFunc(\"/view/\", viewHandler)\n",
      "\thttp.HandleFunc(\"/edit/\", editHandler)\n",
      "\thttp.HandleFunc(\"/save/\", saveHandler)\n",
      "\tlog.Fatal(http.ListenAndServe(\":8080\", nil))\n",
      "}\n",
      "```\n",
      "This will map the `/edit/` path to the `editHandler` function and the `/save/` path to the `saveHandler` function.\n",
      "\n",
      "Don't forget to define the `editHandler` and `saveHandler` functions:\n",
      "```go\n",
      "func editHandler(w http.ResponseWriter, r *http.Request) {\n",
      "    // edit logic here\n",
      "    fmt.Fprintf(w, \"Editing...\")\n",
      "}\n",
      "\n",
      "func saveHandler(w http.ResponseWriter, r *http.Request) {\n",
      "    // save logic here\n",
      "    fmt.Fprintf(w, \"Saving...\")\n",
      "}\n",
      "```\n",
      "You'll also need to update the `viewHandler` function to handle the edit and save requests, or you can define separate functions for them if you want.\n",
      "\n",
      "Here is a more complete example:\n",
      "\n",
      "```go\n",
      "func viewHandler(w http.ResponseWriter, r *http.Request) {\n",
      "    path := r.URL.Path\n",
      "    if path == \"/view/edit\" {\n",
      "        // edit logic here\n",
      "        fmt.Fprintln(w, \"Editing...\")\n",
      "    } else if path == \"/view/save\" {\n",
      "        // save logic here\n",
      "        fmt.Fprintln(w, \"Saving...\")\n",
      "    } else {\n",
      "        fmt.Fprintf(w, \"Viewing...\")\n",
      "    }\n",
      "}\n",
      "```\n",
      "But a better approach is to use the `http.HandleFunc` function to map each handler to a specific path, like we did earlier. This way, you can keep the logic for each handler in its own function.\n",
      "\n",
      "Remember to replace the `edit` and `save` logic with your actual code.\n",
      "\n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!! Golden Answer:\n",
      "Sure, here's the updated code with handlers for edit and save:\n",
      "```go\n",
      "package main\n",
      "\n",
      "import (\n",
      "\t\"fmt\"\n",
      "\t\"log\"\n",
      "\t\"net/http\"\n",
      "\t\"os\"\n",
      ")\n",
      "\n",
      "type Page struct {\n",
      "\tTitle string\n",
      "\tBody  []byte\n",
      "}\n",
      "\n",
      "func (p *Page) save() error {\n",
      "\tfilename := p.Title + \".txt\"\n",
      "\treturn os.WriteFile(filename, p.Body, 0600)\n",
      "}\n",
      "\n",
      "func (p *Page) edit(w http.ResponseWriter, r *http.Request) {\n",
      "\tif r.Method != http.MethodPost {\n",
      "\t\thttp.Error(w, \"Method not allowed\", http.StatusMethodNotAllowed)\n",
      "\t\treturn\n",
      "\t}\n",
      "\n",
      "\t// Read 'body' parameter from the POST request\n",
      "\tbody := r.FormValue(\"body\")\n",
      "\tif body == \"\" {\n",
      "\t\thttp.Error(w, \"Body is empty\", http.StatusBadRequest)\n",
      "\t\treturn\n",
      "\t}\n",
      "\n",
      "\tp.Body = []byte(body)\n",
      "\n",
      "\t// Save the page after updating the body\n",
      "\terr := p.save()\n",
      "\tif err != nil {\n",
      "\t\thttp.Error(w, \"Error saving page\", http.StatusInternalServerError)\n",
      "\t\treturn\n",
      "\t}\n",
      "\n",
      "\tfmt.Fprintf(w, \"Page updated successfully\")\n",
      "}\n",
      "\n",
      "func (p *Page) view(w http.ResponseWriter, r *http.Request) {\n",
      "\t// Show the title and body in the response\n",
      "\tfmt.Fprintf(w, \"Title: %s\\nBody: %s\", p.Title, string(p.Body))\n",
      "}\n",
      "\n",
      "func main() {\n",
      "\t// Initialize a Page instance\n",
      "\tpage := &Page{Title: \"Test Page\", Body: []byte(\"This is a sample page.\")}\n",
      "\n",
      "\t// Register the handlers for view and edit\n",
      "\thttp.HandleFunc(\"/view/\", page.view)\n",
      "\thttp.HandleFunc(\"/edit/\", page.edit)\n",
      "\n",
      "\tlog.Fatal(http.ListenAndServe(\":8080\", nil))\n",
      "}\n",
      "\n",
      "```\n",
      "**Explanation:**\n",
      "- In this updated code, a `Page` struct is defined with a `Title` and a `Body` field. A `save` method is implemented for the `Page` struct that writes the contents of the `Body` field to a file named after the `Title`. \n",
      "\n",
      "- The `edit` method is implemented on the `Page` struct to handle `POST` requests. It reads the contents of the `body` form field, updates the `Body` field of the `Page` struct, and then calls the `save` method to persist the changes. If there is an error during this process (such as an issue reading the form or saving the page), an appropriate error message is returned. \n",
      "\n",
      "- The `view` method is implemented on the `Page` struct and is used to display the `Title` and `Body` fields in the response when a request is made. \n",
      "\n",
      "- In the `main` function, a `Page` instance is created with the title \"Test Page\" and an initial body content of \"This is a sample page.\" The HTTP handlers are registered for the `/view/` and `/edit/` endpoints using `http.HandleFunc`. The `/view/` handler calls the `view` method of the `Page` instance, and the `/edit/` handler calls the `edit` method, which in turn calls `save` to update the page content. \n",
      "\n",
      "**Note:** The current code does not restrict the `/view/` handler to `GET` requests and would accept any HTTP method. It is likely intended to handle `GET` requests, and an explicit check for `GET` could be added for clarity.\n",
      "\n",
      "\n",
      "\n",
      "!!!!!!!!!!!!!! Judge Feedback (Original Order):\n",
      "Response A correctly adds the handler registrations but only provides minimal placeholder functions without any actual implementation logic. Response B provides a complete, working implementation with proper HTTP method checking, form handling, file I/O via the save method, and meaningful error responses. Response B is far more helpful as it gives the user a functional starting point rather than just stubs.\n"
     ]
    }
   ],
   "source": [
    "# Print specific fields from the first row\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIRST ROW DETAILS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\\n\\n!!!!!!!!!!!!!! Context Formatted:\")\n",
    "print(first_line['context_formatted'])\n",
    "print(\"\\n\\n\\n!!!!!!!!!!!!!! MODEL_TO_EVALUATE_OUTPUT_A:\")\n",
    "print(first_line['MODEL_TO_EVALUATE_OUTPUT_A'])\n",
    "print(\"\\n\\n\\n!!!!!!!!!!!!!! Golden Answer:\")\n",
    "print(first_line['golden_answer'])\n",
    "print(\"\\n\\n\\n!!!!!!!!!!!!!! Judge Feedback (Original Order):\")\n",
    "print(first_line['judge_feedback_original_order'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d361161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
