{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa2dc6b",
   "metadata": {},
   "source": [
    "# Model Comparison on Summarization Tasks\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Evals/Compare_Evals.ipynb)\n",
    "\n",
    "<img src=\"../images/compare_eval.png\" width=\"750\">\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This **exercise notebook** guides you through comparing the quality of a base model, fine-tuned model, and proprietary model using Together AI's Evaluations service on the HelpSteer3 conversational dataset.\n",
    "\n",
    "**You will learn to:**\n",
    "- Fine-tune a base model on the HelpSteer3 dataset\n",
    "- Use external models (OpenAI) alongside Together AI models\n",
    "- Create an LLM-as-a-Judge evaluation pipeline\n",
    "- Compare model outputs head-to-head\n",
    "\n",
    "The full list of supported models can be found [here](https://docs.together.ai/docs/evaluations-supported-models).\n",
    "\n",
    "\n",
    "**Concepts Covered:**\n",
    "- **LLM-as-a-Judge**: Using a strong model to evaluate and compare outputs from other models\n",
    "- **Compare Evaluation**: Head-to-head comparison between multiple models\n",
    "- **Fine-tuning**: Training a base model on domain-specific data\n",
    "- **External Model Integration**: Using models from different providers alongside Together AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f43be5",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "To set up the environment:\n",
    "1. Navigate to the same folder as this notebook\n",
    "2. Run the installation script: `bash install.sh`\n",
    "   - This will create a virtual environment called `env_cookbook_evals`\n",
    "   - It will install all dependencies from `requirements.txt`\n",
    "3. Activate the environment: `source env_cookbook_evals/bin/activate`\n",
    "4. Put your API TOKENS into .env if you want to use load_dotenv\n",
    "4. You're ready to run this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from jinja2 import Template\n",
    "from together import Together\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277e39e",
   "metadata": {},
   "source": [
    "### Model Configurations\n",
    "\n",
    "We will use:\n",
    "- **Judge Model**: `moonshotai/Kimi-K2-Instruct-0905`\n",
    "- **Proprietary Model**: `openai/gpt-5-mini`\n",
    "\n",
    "For the OSS model, we have 2 options:\n",
    "\n",
    "1. **Quick Demo** (`meta-llama/Meta-Llama-3.1-8B-Instruct`):\n",
    "   - Fine-tunes in ~15 minutes\n",
    "   - Can be used with LoRA serverless\n",
    "   - Good for showcasing functionality\n",
    "\n",
    "2. **More Realistic for Quality** (`Qwen/Qwen3-Next-80B-A3B-Instruct`):\n",
    "   - Fine-tunes in ~2 hours\n",
    "   - More realistic setup for quality evaluation\n",
    "   - Requires a dedicated endpoint to run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "JUDGE_MODEL = \"moonshotai/Kimi-K2-Instruct-0905\"\n",
    "PROPRIETARY_BASE_MODEL = \"openai/gpt-5-mini\"\n",
    "\n",
    "BASE_OSS_MODEL_FOR_INFERENCE = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "BASE_OSS_MODEL_FOR_FT = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Reference\"\n",
    "\n",
    "# !!! Uncomment this for more realistic, but slow (several hours) setup.\n",
    "# BASE_OSS_MODEL_FOR_INFERENCE = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n",
    "# BASE_OSS_MODEL_FOR_FT = \"Qwen/Qwen3-Next-80B-A3B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d307fe1",
   "metadata": {},
   "source": [
    "## üìä Understanding the HelpSteer3 Edit Dataset\n",
    "\n",
    "The HelpSteer3 Edit dataset contains conversational contexts paired with multiple response options that can be compared and evaluated.\n",
    "\n",
    "**Key Column for Our Setup:**\n",
    "- We use the `edited_response` column, which represents the ideal \"golden response\"\n",
    "- This can be human-provided or generated by a strong proprietary model (e.g., GPT-4)\n",
    "\n",
    "**Evaluation Approach:**\n",
    "- Compare how different models respond to the same prompts\n",
    "- Assess which model produces higher-quality outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e13d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs3_edit = load_dataset(\"nvidia/HelpSteer3\", \"edit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ae746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all unique domain values\n",
    "print(\"Unique domains in the dataset:\")\n",
    "print(hs3_edit['train'].unique('domain'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84442044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample to understand the structure\n",
    "sample = hs3_edit['train'][0]\n",
    "print(\"Sample context:\")\n",
    "print(sample['context'])\n",
    "print(\"\\nSample edited_response:\")\n",
    "print(sample['edited_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64afcb1e",
   "metadata": {},
   "source": [
    "## üîß SFT Fine-tuning with HelpSteer3 Edit Dataset\n",
    "\n",
    "We'll use the `context` and `edited_response` columns to create training data for Supervised Fine-Tuning (SFT). The context already contains conversation messages, and we'll append the edited_response as the final assistant message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fdf5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "together_client = Together(api_key=TOGETHER_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece54808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_sft_format(row):\n",
    "    \"\"\"Convert HelpSteer3 row to SFT chat format by appending edited_response to context.\n",
    "    \n",
    "    Args:\n",
    "        row: Dict with 'context' (list of messages) and 'edited_response' (string)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'messages' key containing full conversation including the golden response\n",
    "    \"\"\"\n",
    "    # TODO: Exercise 1 - Implement this function\n",
    "    # Hint 1: Copy the context messages to a new list (don't modify original)\n",
    "    # Hint 2: Append a new assistant message with the edited_response content\n",
    "    # Hint 3: Return a dict with key 'messages' containing the complete conversation\n",
    "    \n",
    "    raise NotImplementedError(\"Implement map_to_sft_format\")\n",
    "\n",
    "# Apply transformation to the dataset\n",
    "train_sft = hs3_edit['train'].map(map_to_sft_format, remove_columns=hs3_edit['train'].column_names)\n",
    "print(f\"Transformed dataset size: {len(train_sft)}\")\n",
    "print(f\"Sample messages count: {len(train_sft[0]['messages'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the dataset format\n",
    "assert 'messages' in train_sft.column_names, \"Dataset must contain 'messages' column\"\n",
    "assert len(train_sft) > 0, \"Dataset must not be empty\"\n",
    "assert isinstance(train_sft[0]['messages'], list), \"Messages must be a list\"\n",
    "assert all('role' in msg and 'content' in msg for msg in train_sft[0]['messages']), \"Each message must have 'role' and 'content'\"\n",
    "print(\"‚úì Dataset format validation passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sft[0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06401658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSONL file\n",
    "SFT_TRAIN_FILE = \"helpsteer3_sft_train.jsonl\"\n",
    "train_sft.to_json(SFT_TRAIN_FILE)\n",
    "print(f\"Saved training data to {SFT_TRAIN_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cb418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file to Together AI\n",
    "train_file_resp = together_client.files.upload(SFT_TRAIN_FILE, purpose='fine-tune', check=True)\n",
    "print(f\"Uploaded file ID: {train_file_resp.id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a675019",
   "metadata": {},
   "source": [
    "### Launch Fine-tuning Job\n",
    "\n",
    "Configure and start the SFT fine-tuning job using the uploaded HelpSteer3 data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c1093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3eace1ff",
   "metadata": {},
   "source": [
    "### Training Takes about 15 mins, so we can start it, and then proceed to Evaluations part without waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 2 - Create a fine-tuning job\n",
    "# Fill in the missing parameters below\n",
    "# Docs: https://docs.together.ai/docs/fine-tuning-quickstart\n",
    "\n",
    "ft_resp = together_client.fine_tuning.create(\n",
    "    training_file=train_file_resp.id,\n",
    "    model=BASE_OSS_MODEL_FOR_FT,\n",
    "    # TODO: Set train_on_inputs to False (we only want to train on the assistant response)\n",
    "    train_on_inputs=None,  # <-- Replace None\n",
    "    # TODO: Set number of epochs to 1\n",
    "    n_epochs=None,  # <-- Replace None\n",
    "    # TODO: Set number of checkpoints to 1\n",
    "    n_checkpoints=None,  # <-- Replace None\n",
    "    wandb_api_key=WANDB_API_KEY if WANDB_API_KEY else None,\n",
    "    # TODO: Enable LoRA fine-tuning (set to True)\n",
    "    lora=None,  # <-- Replace None\n",
    "    suffix=\"helpsteer3-sft\",\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning job ID: {ft_resp.id}\")\n",
    "print(f\"Status: {ft_resp.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbeea0",
   "metadata": {},
   "source": [
    "### Monitor Fine-tuning Progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6dfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check job status\n",
    "job_status = together_client.fine_tuning.retrieve(ft_resp.id)\n",
    "print(f\"Status: {job_status.status}\")\n",
    "\n",
    "# List events/logs\n",
    "for event in together_client.fine_tuning.list_events(id=ft_resp.id).data:\n",
    "    print(event.message)\n",
    "\n",
    "print(ft_resp.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9b011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14383ed4",
   "metadata": {},
   "source": [
    "Now we can move to the evaluations, because fine-tuning will take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c33fd0",
   "metadata": {},
   "source": [
    "## üîÑ Preparing Data for Evaluation\n",
    "\n",
    "We'll sample 50 random examples from the original test set and prepare them for evaluation. The evaluation will compare:\n",
    "1. Proprietary model vs Base OSS model - to see how a proprietary model compares to a base open-source model\n",
    "2. Proprietary model vs Fine-tuned OSS model - to measure if fine-tuning closes the gap with proprietary models\n",
    "\n",
    "The judge will use the golden answer (edited_response) from the dataset as a reference to determine which model's response is better aligned with the ideal answer.\n",
    "\n",
    "We need to:\n",
    "- Apply a chat template to convert the context messages into a formatted prompt string\n",
    "- Include the golden answer (edited_response) for the judge to use as reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use 50 samples for validation for speed\n",
    "VALIDATION_SIZE = 50\n",
    "test_data = hs3_edit['validation'].shuffle(seed=42).select(range(VALIDATION_SIZE))\n",
    "print(f\"Test subset size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dec5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all unique values of 'domain' column in test_data\n",
    "# There are 4 domains in the original dataset.\n",
    "unique_domains = set(test_data['domain'])\n",
    "print(f\"Unique domains: {unique_domains}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer for the base model to apply proper chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "def prepare_eval_data(row):\n",
    "    \"\"\"Prepare a single row for evaluation with formatted context and golden answer.\n",
    "    \n",
    "    Args:\n",
    "        row: Dict with 'context' (list of messages) and 'edited_response' (string)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'context_formatted' and 'golden_answer' keys\n",
    "    \"\"\"\n",
    "    # TODO: Exercise 3 - Implement this function\n",
    "    # Step 1: Use tokenizer.apply_chat_template() to format the context\n",
    "    #         - Pass row['context'] as the messages\n",
    "    #         - Set tokenize=False to get string output\n",
    "    #         - Set add_generation_prompt=False\n",
    "    # Step 2: Return a dict with:\n",
    "    #         - 'context_formatted': the formatted context string\n",
    "    #         - 'golden_answer': the edited_response from the row\n",
    "    \n",
    "    raise NotImplementedError(\"Implement prepare_eval_data\")\n",
    "\n",
    "# Transform test data for evaluation\n",
    "eval_data = [prepare_eval_data(row) for row in test_data]\n",
    "print(f\"Prepared {len(eval_data)} samples for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the eval_data format\n",
    "assert len(eval_data) > 0, \"eval_data should not be empty\"\n",
    "assert all('context_formatted' in item for item in eval_data), \"All items must have 'context_formatted' key\"\n",
    "assert all('golden_answer' in item for item in eval_data), \"All items must have 'golden_answer' key\"\n",
    "assert all(isinstance(item['context_formatted'], str) for item in eval_data), \"context_formatted must be strings\"\n",
    "assert all(isinstance(item['golden_answer'], str) for item in eval_data), \"golden_answer must be strings\"\n",
    "print(\"‚úì Eval data format validated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data to JSONL and upload\n",
    "EVAL_FILE = \"helpsteer3_eval.jsonl\"\n",
    "with open(EVAL_FILE, 'w') as f:\n",
    "    for eval_item in eval_data:\n",
    "        json.dump(eval_item, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "uploaded_eval_file = together_client.files.upload(file=EVAL_FILE, purpose='eval', check=False)\n",
    "print(f\"Uploaded eval file ID: {uploaded_eval_file.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and judge configuration\n",
    "\n",
    "# To refer a field from a dataset in templates, use 4 brackets: {{{{field_name}}}}\n",
    "# This becomes {{field_name}} in the final Jinja template\n",
    "\n",
    "# TODO: Exercise 4 - Write the judge template\n",
    "# The judge should evaluate responses based on:\n",
    "# - Helpfulness, Accuracy, Clarity, Completeness, Safety\n",
    "# - Alignment with the golden answer (reference response)\n",
    "# \n",
    "# Include the golden_answer field in your template using: {{{{golden_answer}}}}\n",
    "\n",
    "JUDGE_TEMPLATE = f\"\"\"\n",
    "# TODO: Write your judge template here\n",
    "# Include evaluation criteria and instructions for the judge\n",
    "# Make sure to reference the golden answer using {{{{golden_answer}}}}\n",
    "\"\"\"\n",
    "\n",
    "# Model config for generation from context\n",
    "generation_system_template = \"You are a helpful AI assistant.\"\n",
    "# This template tells the model what input to use for generation\n",
    "input_template = f\"{{{{context_formatted}}}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that jinja template works\n",
    "test_context = \"<<This is a test context>>\"\n",
    "test_template = Template(input_template)\n",
    "rendered = test_template.render(context_formatted=test_context)\n",
    "assert test_context in rendered, \"Jinja template rendering failed\"\n",
    "print(rendered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 8096\n",
    "TEMPERATURE = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a2de5",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Evaluation 1: Proprietary Model vs Base Model\n",
    "\n",
    "Compare the proprietary model's output against the base OSS model, with the judge considering alignment with the golden answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6947a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 5 - Configure models and create evaluation\n",
    "# Docs: https://docs.together.ai/docs/ai-evaluations\n",
    "\n",
    "# Model A: Proprietary model (external - OpenAI)\n",
    "proprietary_model_config = {\n",
    "    \"model\": PROPRIETARY_BASE_MODEL,\n",
    "    \"model_source\": \"external\",  # \"external\" for non-Together models\n",
    "    \"system_template\": generation_system_template,\n",
    "    \"input_template\": input_template,\n",
    "    \"max_tokens\": MAX_TOKENS,\n",
    "    \"temperature\": TEMPERATURE,\n",
    "    # TODO: Add the external_api_token for OpenAI\n",
    "    # \"external_api_token\": ???\n",
    "}\n",
    "\n",
    "# Model B: Base OSS model (serverless - Together AI)\n",
    "base_model_config = {\n",
    "    \"model\": BASE_OSS_MODEL_FOR_INFERENCE,\n",
    "    # TODO: Set model_source to \"serverless\" for Together AI serverless models\n",
    "    \"model_source\": None,  # <-- Replace None\n",
    "    \"system_template\": generation_system_template,\n",
    "    \"input_template\": input_template,\n",
    "    \"max_tokens\": MAX_TOKENS,\n",
    "    \"temperature\": TEMPERATURE\n",
    "}\n",
    "\n",
    "# TODO: Create the evaluation using together_client.evaluation.create()\n",
    "# Required parameters:\n",
    "#   - type: \"compare\" (for head-to-head comparison)\n",
    "#   - input_data_file_path: uploaded_eval_file.id\n",
    "#   - judge_model: JUDGE_MODEL\n",
    "#   - judge_model_source: \"serverless\"\n",
    "#   - judge_system_template: JUDGE_TEMPLATE\n",
    "#   - model_a: proprietary_model_config\n",
    "#   - model_b: base_model_config\n",
    "\n",
    "eval_proprietary_vs_base = None  # <-- Replace with evaluation.create() call\n",
    "\n",
    "print(f\"Eval 1 (Proprietary vs Base) ID: {eval_proprietary_vs_base.workflow_id}\")\n",
    "print(f\"Status: {eval_proprietary_vs_base.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ee809",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Evaluation 2: Proprietary Model vs Fine-tuned Model\n",
    "\n",
    "Compare the proprietary model's output against the fine-tuned model, with the judge considering alignment with the golden answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = \"ivprov/Meta-Llama-3.1-8B-Instruct-Reference-helpsteer3-sft-d5865876-e2abadd3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 6 - Create second evaluation (Proprietary vs Fine-tuned)\n",
    "# This is similar to Exercise 5, but comparing against the fine-tuned model\n",
    "\n",
    "finetuned_model_config = {\n",
    "    \"model\": finetuned_model,\n",
    "    # TODO: Set model_source - use \"dedicated\" for fine-tuned models on dedicated endpoints\n",
    "    #       or \"serverless\" if using LoRA serverless\n",
    "    \"model_source\": None,  # <-- Replace None\n",
    "    \"system_template\": generation_system_template,\n",
    "    \"input_template\": input_template,\n",
    "    \"max_tokens\": MAX_TOKENS,\n",
    "    \"temperature\": TEMPERATURE\n",
    "}\n",
    "\n",
    "# TODO: Create the evaluation comparing proprietary_model_config vs finetuned_model_config\n",
    "# Use the same pattern as Exercise 5\n",
    "\n",
    "eval_proprietary_vs_finetuned = None  # <-- Replace with evaluation.create() call\n",
    "\n",
    "print(f\"Eval 2 (Proprietary vs Fine-tuned) ID: {eval_proprietary_vs_finetuned.workflow_id}\")\n",
    "print(f\"Status: {eval_proprietary_vs_finetuned.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e0b9f",
   "metadata": {},
   "source": [
    "## ‚è≥ Wait for Evaluations to Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7749dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get status for all evaluations and wait until they have results\n",
    "while True:\n",
    "    status_proprietary_vs_base = together_client.evaluation.status(eval_proprietary_vs_base.workflow_id)\n",
    "    status_proprietary_vs_finetuned = together_client.evaluation.status(eval_proprietary_vs_finetuned.workflow_id)\n",
    "    \n",
    "    if status_proprietary_vs_base.results and status_proprietary_vs_finetuned.results:\n",
    "        break\n",
    "    \n",
    "    print(\"Waiting for evaluations to complete...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "print(\"All evaluations completed!\")\n",
    "print(f\"Proprietary vs Base: {status_proprietary_vs_base}\")\n",
    "print(f\"Proprietary vs Fine-tuned: {status_proprietary_vs_finetuned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51be512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a02d4ed",
   "metadata": {},
   "source": [
    "Now we can take a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8172ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison_summary(status, eval_name, model_a_name, model_b_name):\n",
    "    \"\"\"Print a summary of comparison results.\"\"\"\n",
    "    if not status.results:\n",
    "        print(f\"{eval_name}: Results not available yet\")\n",
    "        return\n",
    "    \n",
    "    results = status.results\n",
    "    total = results.get('A_wins', 0) + results.get('B_wins', 0) + results.get('Ties', 0)\n",
    "    if total == 0:\n",
    "        print(f\"{eval_name}: No results yet\")\n",
    "        return\n",
    "    \n",
    "    a_wins = results.get('A_wins', 0)\n",
    "    b_wins = results.get('B_wins', 0)\n",
    "    ties = results.get('Ties', 0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{eval_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total samples: {total}\")\n",
    "    print(f\"{model_a_name} wins: {a_wins} ({a_wins/total*100:.1f}%)\")\n",
    "    print(f\"{model_b_name} wins: {b_wins} ({b_wins/total*100:.1f}%)\")\n",
    "    print(f\"Ties: {ties} ({ties/total*100:.1f}%)\")\n",
    "    \n",
    "    if a_wins > b_wins:\n",
    "        print(f\"‚úÖ Winner: {model_a_name}\")\n",
    "    elif b_wins > a_wins:\n",
    "        print(f\"‚úÖ Winner: {model_b_name}\")\n",
    "    else:\n",
    "        print(\"ü§ù It's a tie!\")\n",
    "\n",
    "# Display results summary using status from previous cell\n",
    "print_comparison_summary(status_proprietary_vs_base, \"Eval 1: Proprietary vs Base\", \"Proprietary Model\", \"Base Model\")\n",
    "print_comparison_summary(status_proprietary_vs_finetuned, \"Eval 2: Proprietary vs Fine-tuned\", \"Proprietary Model\", \"Fine-tuned Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e51ac7",
   "metadata": {},
   "source": [
    "## Scores from Evaluation can be used in addition to performance numbers to better decide which model to choose for a particular settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967fc8eb",
   "metadata": {},
   "source": [
    "### Download Results File\n",
    "We can download the results file with feedback from the LLM Judge about each decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf06d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_proprietary_vs_finetuned.results['result_file_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_proprietary_vs_finetuned.workflow_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results from the comparison of proprietary vs fine-tuned model\n",
    "OUTPUT_FILE_NAME = \"./helpsteer3_proprietary_vs_finetuned_results.jsonl\"\n",
    "results_file_id = status_proprietary_vs_finetuned.results['result_file_id']\n",
    "results_finetuned = together_client.files.retrieve_content(\n",
    "    results_file_id,\n",
    "    output=OUTPUT_FILE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff657f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the results file and print columns for the first line\n",
    "with open(OUTPUT_FILE_NAME, 'r') as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "    print(\"Columns in the results file:\")\n",
    "    for key in first_line.keys():\n",
    "        print(f\"  - {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print specific fields from the first row\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIRST ROW DETAILS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\\n\\n!!!!!!!!!!!!!! Context Formatted:\")\n",
    "print(first_line['context_formatted'])\n",
    "print(\"\\n\\n\\n!!!!!!!!!!!!!! MODEL_TO_EVALUATE_OUTPUT_A:\")\n",
    "print(first_line['MODEL_TO_EVALUATE_OUTPUT_A'])\n",
    "print(\"\\n\\n\\n!!!!!!!!!!!!!! Golden Answer:\")\n",
    "print(first_line['golden_answer'])\n",
    "print(\"\\n\\n\\n!!!!!!!!!!!!!! Judge Feedback (Original Order):\")\n",
    "print(first_line['judge_feedback_original_order'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d361161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
