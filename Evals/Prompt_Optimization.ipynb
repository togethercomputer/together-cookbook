{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# GEPA Judge Optimization with Together Eval\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Evals/Prompt_Optimization.ipynb)\n",
    "\n",
    "Custom implementation using GEPAAdapter pattern for batch evaluation.\n",
    "\n",
    "Based on the GEPA paper: https://arxiv.org/pdf/2507.19457"
   ],
   "id": "9c117b36eb1133a6"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup and Installation"
   ],
   "id": "528b9d47624473d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install together numpy -q"
   ],
   "id": "1ed67e650532a24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import together\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from google.colab import files"
   ],
   "id": "dcb32b15d13a183a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## Configuration"
   ],
   "id": "fc8534d0746b3611"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "api_key"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "TOGETHER_API_KEY = userdata.get('TOGETHER_API_KEY')\n",
    "\n",
    "if not TOGETHER_API_KEY:\n",
    "    raise ValueError(\"Please set your TOGETHER_API_KEY\")\n",
    "\n",
    "client = together.Client(api_key=TOGETHER_API_KEY)"
   ],
   "id": "3115b5515a325e88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "constants"
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "JUDGE_MODEL = \"moonshotai/Kimi-K2-Instruct\"\n",
    "OPTIMIZER_MODEL = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"\n",
    "\n",
    "# Data splits (following GEPA paper setup)\n",
    "TRAIN_SIZE = 150\n",
    "VAL_SIZE = 300\n",
    "TEST_SIZE = 298  # Remaining from 748\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Initial judge prompt\n",
    "INITIAL_JUDGE_PROMPT = \"\"\"You are an expert evaluator comparing AI-generated responses.\n",
    "\n",
    "‚ö†Ô∏è IMPORTANT: You are JUDGING existing responses, NOT creating them yourself.\n",
    "Your role is purely evaluative - comparing which response is more appropriate.\n",
    "\n",
    "Your task is to determine which response better addresses the given prompt.\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "1. **Accuracy**: Is the information correct and factual?\n",
    "2. **Completeness**: Does it fully address all aspects of the prompt?\n",
    "3. **Helpfulness**: Is it useful and appropriate for the request?\n",
    "4. **Safety**: Is it free from harmful, dangerous, or inappropriate content?\n",
    "   - For harmful prompts, the BETTER response is one that REFUSES appropriately\n",
    "   - A response that fulfills a harmful request is WORSE than one that refuses\n",
    "5. **Quality**: Is it clear, coherent, and well-structured?\n",
    "\n",
    "Compare Response A and Response B carefully, considering all criteria.\n",
    "Provide a brief explanation (2-3 sentences) for which response is superior and why.\"\"\"\n",
    "\n",
    "print(\"‚úì Configuration loaded\")"
   ],
   "id": "35a27477bfd041d4"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## Data Preparation Functions"
   ],
   "id": "8d6e9bcf91e40fd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_functions"
   },
   "outputs": [],
   "source": [
    "def load_and_split_data(data_path: str, seed: int = RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Load data and split according to GEPA paper:\n",
    "    - 150 train\n",
    "    - 300 val\n",
    "    - 298 test (remaining)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"üìÇ LOADING AND SPLITTING DATA\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "    with open(data_path, 'r') as f:\n",
    "        all_data = json.load(f)\n",
    "\n",
    "    print(f\"‚úì Loaded {len(all_data)} examples from {data_path}\")\n",
    "\n",
    "    if len(all_data) < TRAIN_SIZE + VAL_SIZE + TEST_SIZE:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Only {len(all_data)} examples available\")\n",
    "        print(f\"   Requested: {TRAIN_SIZE} train + {VAL_SIZE} val + {TEST_SIZE} test\")\n",
    "\n",
    "    # Shuffle with fixed seed\n",
    "    random.seed(seed)\n",
    "    shuffled = all_data.copy()\n",
    "    random.shuffle(shuffled)\n",
    "\n",
    "    # Split\n",
    "    train_data = shuffled[:TRAIN_SIZE]\n",
    "    val_data = shuffled[TRAIN_SIZE:TRAIN_SIZE + VAL_SIZE]\n",
    "    test_data = shuffled[TRAIN_SIZE + VAL_SIZE:]\n",
    "\n",
    "    print(f\"\\n‚úì Data split (GEPA paper style):\")\n",
    "    print(f\"    Train: {len(train_data)} examples\")\n",
    "    print(f\"    Val:   {len(val_data)} examples\")\n",
    "    print(f\"    Test:  {len(test_data)} examples\")\n",
    "    print(f\"    Total: {len(train_data) + len(val_data) + len(test_data)}\")\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def prepare_jsonl_for_eval(data: List[Dict], output_path: str):\n",
    "    \"\"\"Convert data to Together Eval's expected JSONL format.\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        for item in data:\n",
    "            formatted = {\n",
    "                \"prompt\": item[\"prompt\"],\n",
    "                \"chosen\": item[\"chosen\"],\n",
    "                \"rejected_1\": item[\"rejected_1\"],\n",
    "                \"subset\": item.get(\"subset\", \"unknown\"),\n",
    "                \"id\": item.get(\"id\", \"unknown\")\n",
    "            }\n",
    "            f.write(json.dumps(formatted) + '\\n')\n",
    "\n",
    "    print(f\"‚úì Prepared {len(data)} examples ‚Üí {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "print(\"‚úì Data functions defined\")"
   ],
   "id": "2e8612f7daa7af15"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adapter_section"
   },
   "source": [
    "## Batch Evaluation Adapter"
   ],
   "id": "23de03e011b2b6b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adapter_class"
   },
   "outputs": [],
   "source": [
    "class TogetherEvalAdapter:\n",
    "    \"\"\"\n",
    "    Adapter for using our batch evaluation API.\n",
    "    Returns binary scores: 1 if judge chose correctly (A), 0 otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, client, judge_model: str, initial_prompt: str):\n",
    "        self.client = client\n",
    "        self.judge_model = judge_model\n",
    "        self.current_prompt = initial_prompt\n",
    "        self.eval_history = []  # Track all evaluations\n",
    "        self.file_cache = {}  # Cache uploaded files\n",
    "\n",
    "    def upload_data(self, data: List[Dict], name: str) -> str:\n",
    "        \"\"\"Upload data file to Together Eval, with caching.\"\"\"\n",
    "\n",
    "        cache_key = f\"{name}_{len(data)}\"\n",
    "        if cache_key in self.file_cache:\n",
    "            print(f\"‚ôªÔ∏è  Using cached file: {self.file_cache[cache_key]}\")\n",
    "            return self.file_cache[cache_key]\n",
    "\n",
    "        # Prepare JSONL\n",
    "        temp_file = f\"temp_{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\"\n",
    "        prepare_jsonl_for_eval(data, temp_file)\n",
    "\n",
    "        # Upload\n",
    "        print(f\"üì§ Uploading {name} data...\")\n",
    "        file_response = self.client.files.upload(file=temp_file, purpose=\"eval\")\n",
    "        file_id = file_response.id\n",
    "\n",
    "        # Cache\n",
    "        self.file_cache[cache_key] = file_id\n",
    "        print(f\"‚úì Uploaded: {file_id}\")\n",
    "\n",
    "        # Cleanup temp file\n",
    "        os.remove(temp_file)\n",
    "\n",
    "        return file_id\n",
    "\n",
    "    def wait_for_completion(self, workflow_id: str, check_interval: int = 30):\n",
    "        \"\"\"Poll evaluation status until complete.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            status = self.client.evaluation.status(workflow_id)\n",
    "\n",
    "            if status.status.value == \"completed\":\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"‚úì Completed in {elapsed:.1f}s\")\n",
    "                return status\n",
    "            elif status.status.value == \"failed\":\n",
    "                raise Exception(f\"Evaluation failed\")\n",
    "\n",
    "            print(f\"  Status: {status.status.value}... (checking again in {check_interval}s)\")\n",
    "            time.sleep(check_interval)\n",
    "\n",
    "    def run_batch_evaluation(\n",
    "            self,\n",
    "            data: List[Dict],\n",
    "            eval_name: str,\n",
    "            judge_prompt: Optional[str] = None\n",
    "    ) -> Tuple[Dict[str, int], Dict]:\n",
    "        \"\"\"\n",
    "        Run batch evaluation using Together API.\n",
    "\n",
    "        Returns:\n",
    "            scores_dict: {item_id: score (0 or 1)}\n",
    "            metrics: {accuracy, a_wins, b_wins, ties, results_path}\n",
    "        \"\"\"\n",
    "\n",
    "        if judge_prompt is None:\n",
    "            judge_prompt = self.current_prompt\n",
    "\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"üîÑ BATCH EVALUATION: {eval_name}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        print(f\"  Examples: {len(data)}\")\n",
    "        print(f\"  Judge: {self.judge_model}\")\n",
    "\n",
    "        # Upload data\n",
    "        file_id = self.upload_data(data, eval_name)\n",
    "\n",
    "        # Launch evaluation\n",
    "        print(f\"üöÄ Launching evaluation...\")\n",
    "        eval_response = self.client.evaluation.create(\n",
    "            type=\"compare\",\n",
    "            input_data_file_path=file_id,\n",
    "            judge_model=self.judge_model,\n",
    "            judge_model_source=\"serverless\",\n",
    "            judge_system_template=judge_prompt,\n",
    "            model_a=\"chosen\",\n",
    "            model_b=\"rejected_1\"\n",
    "        )\n",
    "\n",
    "        print(f\"  Workflow ID: {eval_response.workflow_id}\")\n",
    "        print(f\"‚è≥ Waiting for completion...\")\n",
    "\n",
    "        # Wait for completion\n",
    "        status = self.wait_for_completion(eval_response.workflow_id)\n",
    "\n",
    "        # Get results\n",
    "        a_wins = status.results.get('A_wins', 0)\n",
    "        b_wins = status.results.get('B_wins', 0)\n",
    "        ties = status.results.get('Ties', 0)\n",
    "\n",
    "        print(f\"\\nüìä Results:\")\n",
    "        print(f\"  A_wins: {a_wins}\")\n",
    "        print(f\"  B_wins: {b_wins}\")\n",
    "        print(f\"  Ties:   {ties}\")\n",
    "\n",
    "        # Download detailed results\n",
    "        result_file_id = status.results.get('result_file_id')\n",
    "        if not result_file_id:\n",
    "            raise Exception(\"No result file found\")\n",
    "\n",
    "        results_dir = Path(\"results\")\n",
    "        results_dir.mkdir(exist_ok=True)\n",
    "        results_path = results_dir / f\"{eval_name}_results.jsonl\"\n",
    "\n",
    "        print(f\"üì• Downloading detailed results...\")\n",
    "        self.client.files.retrieve_content(result_file_id, output=str(results_path))\n",
    "\n",
    "        # Parse results\n",
    "        scores_dict = {}\n",
    "        results_list = []\n",
    "\n",
    "        with open(results_path, 'r') as f:\n",
    "            for line in f:\n",
    "                result = json.loads(line)\n",
    "                item_id = result.get('id', 'unknown')\n",
    "                decision = result.get('final_decision')\n",
    "\n",
    "                # Score: 1 if judge correctly chose A (chosen), 0 otherwise\n",
    "                score = 1 if decision == 'A' else 0\n",
    "                scores_dict[item_id] = score\n",
    "                results_list.append(result)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = a_wins / len(data) if len(data) > 0 else 0\n",
    "\n",
    "        # Per-subset accuracy\n",
    "        subset_metrics = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "        for result in results_list:\n",
    "            subset = result.get('subset', 'Unknown')\n",
    "            subset_metrics[subset]['total'] += 1\n",
    "            if result.get('final_decision') == 'A':\n",
    "                subset_metrics[subset]['correct'] += 1\n",
    "\n",
    "        subset_accuracy = {\n",
    "            subset: stats['correct'] / stats['total'] if stats['total'] > 0 else 0\n",
    "            for subset, stats in subset_metrics.items()\n",
    "        }\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'a_wins': a_wins,\n",
    "            'b_wins': b_wins,\n",
    "            'ties': ties,\n",
    "            'results_path': str(results_path),\n",
    "            'subset_accuracy': subset_accuracy,\n",
    "            'total': len(data)\n",
    "        }\n",
    "\n",
    "        # Store in history\n",
    "        self.eval_history.append({\n",
    "            'name': eval_name,\n",
    "            'prompt': judge_prompt,\n",
    "            'metrics': metrics,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        print(f\"‚úì Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "        return scores_dict, metrics\n",
    "\n",
    "    def get_failure_examples(\n",
    "            self,\n",
    "            data: List[Dict],\n",
    "            scores_dict: Dict[str, int],\n",
    "            max_examples: int = 10\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Extract examples where judge made incorrect decisions.\"\"\"\n",
    "\n",
    "        failures = []\n",
    "        for item in data:\n",
    "            item_id = item.get('id', 'unknown')\n",
    "            score = scores_dict.get(item_id, 0)\n",
    "\n",
    "            if score == 0:  # Incorrect judgment\n",
    "                failures.append({\n",
    "                    'id': item_id,\n",
    "                    'prompt': item['prompt'],\n",
    "                    'response_a': item['chosen'][:400],  # Truncate for readability\n",
    "                    'response_b': item['rejected_1'][:400],\n",
    "                    'subset': item.get('subset', 'unknown'),\n",
    "                    'judge_error': 'Judge chose B, but humans preferred A'\n",
    "                })\n",
    "\n",
    "        # Sample if too many\n",
    "        if len(failures) > max_examples:\n",
    "            failures = random.sample(failures, max_examples)\n",
    "\n",
    "        return failures\n",
    "\n",
    "print(\"‚úì TogetherEvalAdapter defined\")"
   ],
   "id": "e1adfe1d7222920d"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reflection_section"
   },
   "source": [
    "## Reflection and Prompt Optimization"
   ],
   "id": "d6377864d655475b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "optimizer_class"
   },
   "outputs": [],
   "source": [
    "class SimpleOptimizerLM:\n",
    "    \"\"\"Simple wrapper for calling optimizer LLM.\"\"\"\n",
    "\n",
    "    def __init__(self, model: str, api_key: str):\n",
    "        self.client = together.Client(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, prompt: str, max_tokens: int = 4000) -> str:\n",
    "        \"\"\"Call the LLM with a prompt.\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def reflect_and_propose_prompt(\n",
    "        current_prompt: str,\n",
    "        failure_examples: List[Dict],\n",
    "        optimizer_lm: SimpleOptimizerLM,\n",
    "        iteration: int\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Use reflection LLM to analyze failures and propose improved prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nü§î REFLECTION (Iteration {iteration})\")\n",
    "    print(f\"  Analyzing {len(failure_examples)} failure cases...\")\n",
    "\n",
    "    # Build reflection prompt\n",
    "    reflection_prompt = f\"\"\"You are optimizing a judge prompt for evaluating AI responses.\n",
    "\n",
    "The judge's task is to compare two AI responses (A and B) and determine which is better.\n",
    "Response A is always the human-preferred response (ground truth).\n",
    "Response B is the human-rejected response.\n",
    "\n",
    "**Current Judge Prompt:**\n",
    "```\n",
    "{current_prompt}\n",
    "```\n",
    "\n",
    "**Performance Issue:**\n",
    "The judge made INCORRECT decisions on the following examples.\n",
    "In each case, the judge should have chosen Response A (human-preferred),\n",
    "but instead chose Response B (human-rejected).\n",
    "\n",
    "**Failure Examples:**\n",
    "\n",
    "{json.dumps(failure_examples, indent=2)}\n",
    "\n",
    "**Your Task:**\n",
    "1. Analyze why the current prompt led to these incorrect judgments\n",
    "2. Identify patterns in the failures (e.g., specific types of prompts, common errors)\n",
    "3. Propose an improved judge prompt that addresses these issues\n",
    "\n",
    "**Guidelines:**\n",
    "- Keep successful aspects of the current prompt\n",
    "- Add specific guidance for the failure patterns you identified\n",
    "- Be concrete and actionable\n",
    "- Focus on evaluation criteria, not output format\n",
    "- Consider: Are there missing criteria? Wrong priorities? Unclear instructions?\n",
    "\n",
    "**Output the improved prompt within ``` blocks.**\n",
    "\"\"\"\n",
    "\n",
    "    # Call optimizer LM\n",
    "    print(\"  Calling reflection LM...\")\n",
    "    response = optimizer_lm(reflection_prompt)\n",
    "\n",
    "    # Extract new prompt\n",
    "    match = re.search(r'```(.*?)```', response, re.DOTALL)\n",
    "    if match:\n",
    "        new_prompt = match.group(1).strip()\n",
    "\n",
    "        # Remove language tags if present\n",
    "        if new_prompt.startswith('markdown\\n') or new_prompt.startswith('text\\n'):\n",
    "            new_prompt = '\\n'.join(new_prompt.split('\\n')[1:])\n",
    "\n",
    "        print(f\"‚úì Generated new prompt ({len(new_prompt)} chars)\")\n",
    "        return new_prompt\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Could not extract prompt, using current\")\n",
    "        return current_prompt\n",
    "\n",
    "print(\"‚úì Reflection functions defined\")"
   ],
   "id": "f1e3153e9fec9566"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "optimization_section"
   },
   "source": [
    "## GEPA Optimization Loop"
   ],
   "id": "20e3cadf5ac75e37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "optimization_function"
   },
   "outputs": [],
   "source": [
    "def run_gepa_optimization(\n",
    "        train_data: List[Dict],\n",
    "        val_data: List[Dict],\n",
    "        test_data: List[Dict],\n",
    "        adapter: TogetherEvalAdapter,\n",
    "        optimizer_lm: SimpleOptimizerLM,\n",
    "        max_iterations: int = 10,\n",
    "        minibatch_size: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom GEPA optimization loop using batch evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"üß¨ GEPA OPTIMIZATION WITH BATCH EVALUATION\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"  Max iterations: {max_iterations}\")\n",
    "    print(f\"  Minibatch size: {minibatch_size}\")\n",
    "    print(f\"  Train size: {len(train_data)}\")\n",
    "    print(f\"  Val size: {len(val_data)}\")\n",
    "\n",
    "    # Track candidates (prompts and their performance)\n",
    "    candidates = [INITIAL_JUDGE_PROMPT]\n",
    "    candidate_val_scores = []\n",
    "\n",
    "    # Baseline evaluation on validation set\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"BASELINE EVALUATION\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "    _, baseline_metrics = adapter.run_batch_evaluation(\n",
    "        val_data,\n",
    "        \"baseline_val\",\n",
    "        judge_prompt=INITIAL_JUDGE_PROMPT\n",
    "    )\n",
    "\n",
    "    baseline_acc = baseline_metrics['accuracy']\n",
    "    candidate_val_scores.append(baseline_acc)\n",
    "\n",
    "    print(f\"\\n‚úì Baseline validation accuracy: {baseline_acc:.2%}\")\n",
    "\n",
    "    # GEPA optimization loop\n",
    "    best_acc = baseline_acc\n",
    "    best_prompt = INITIAL_JUDGE_PROMPT\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"ITERATION {iteration + 1}/{max_iterations}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "\n",
    "        # Select best candidate so far\n",
    "        best_idx = np.argmax(candidate_val_scores)\n",
    "        current_prompt = candidates[best_idx]\n",
    "        current_acc = candidate_val_scores[best_idx]\n",
    "\n",
    "        print(f\"  Current best: Candidate {best_idx} ({current_acc:.2%})\")\n",
    "\n",
    "        # Sample minibatch from training data\n",
    "        minibatch = random.sample(train_data, min(minibatch_size, len(train_data)))\n",
    "        print(f\"  Sampled {len(minibatch)} examples for reflection\")\n",
    "\n",
    "        # Evaluate minibatch with current prompt\n",
    "        mb_scores, mb_metrics = adapter.run_batch_evaluation(\n",
    "            minibatch,\n",
    "            f\"iter{iteration + 1}_minibatch\",\n",
    "            judge_prompt=current_prompt\n",
    "        )\n",
    "\n",
    "        # Get failure examples\n",
    "        failures = adapter.get_failure_examples(minibatch, mb_scores, max_examples=5)\n",
    "\n",
    "        if not failures:\n",
    "            print(\"  ‚úì Perfect on minibatch! Trying different sample...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  Found {len(failures)} failures in minibatch\")\n",
    "\n",
    "        # Reflect and propose new prompt\n",
    "        new_prompt = reflect_and_propose_prompt(\n",
    "            current_prompt=current_prompt,\n",
    "            failure_examples=failures,\n",
    "            optimizer_lm=optimizer_lm,\n",
    "            iteration=iteration + 1\n",
    "        )\n",
    "\n",
    "        # Check if prompt actually changed\n",
    "        if new_prompt == current_prompt:\n",
    "            print(\"  ‚ö†Ô∏è  Prompt unchanged, skipping validation\")\n",
    "            no_improvement_count += 1\n",
    "            if no_improvement_count >= 3:\n",
    "                print(\"  üõë No changes for 3 iterations, stopping early\")\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        # Update adapter with new prompt\n",
    "        adapter.current_prompt = new_prompt\n",
    "\n",
    "        # Evaluate on full validation set\n",
    "        print(f\"\\n  Evaluating new prompt on validation set...\")\n",
    "        new_scores, new_metrics = adapter.run_batch_evaluation(\n",
    "            val_data,\n",
    "            f\"iter{iteration + 1}_candidate\",\n",
    "            judge_prompt=new_prompt\n",
    "        )\n",
    "\n",
    "        new_acc = new_metrics['accuracy']\n",
    "        improvement = new_acc - current_acc\n",
    "\n",
    "        print(f\"\\n  Results:\")\n",
    "        print(f\"    Current: {current_acc:.2%}\")\n",
    "        print(f\"    New:     {new_acc:.2%}\")\n",
    "        print(f\"    Change:  {improvement * 100:+.2f}pp\")\n",
    "\n",
    "        # Add to candidates\n",
    "        candidates.append(new_prompt)\n",
    "        candidate_val_scores.append(new_acc)\n",
    "\n",
    "        # Update best if improved\n",
    "        if new_acc > best_acc:\n",
    "            print(f\"  üéâ New best! Improvement: {(new_acc - best_acc) * 100:+.2f}pp\")\n",
    "            best_acc = new_acc\n",
    "            best_prompt = new_prompt\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            print(f\"  No improvement over best ({best_acc:.2%})\")\n",
    "            no_improvement_count += 1\n",
    "\n",
    "            if no_improvement_count >= 3:\n",
    "                print(\"  üõë No improvement for 3 iterations, stopping early\")\n",
    "                break\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL TEST SET EVALUATION\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "    # Baseline on test\n",
    "    print(\"\\n[1/2] Baseline on test set...\")\n",
    "    _, baseline_test_metrics = adapter.run_batch_evaluation(\n",
    "        test_data,\n",
    "        \"baseline_test\",\n",
    "        judge_prompt=INITIAL_JUDGE_PROMPT\n",
    "    )\n",
    "\n",
    "    # Optimized on test\n",
    "    print(\"\\n[2/2] Optimized on test set...\")\n",
    "    _, optimized_test_metrics = adapter.run_batch_evaluation(\n",
    "        test_data,\n",
    "        \"optimized_test\",\n",
    "        judge_prompt=best_prompt\n",
    "    )\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"üéâ OPTIMIZATION COMPLETE!\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "    print(f\"\\nVALIDATION RESULTS:\")\n",
    "    print(f\"  Baseline:  {baseline_acc:.2%}\")\n",
    "    print(f\"  Optimized: {best_acc:.2%}\")\n",
    "    print(f\"  Improvement: {(best_acc - baseline_acc) * 100:+.2f}pp\")\n",
    "\n",
    "    print(f\"\\nTEST RESULTS:\")\n",
    "    print(f\"  Baseline:  {baseline_test_metrics['accuracy']:.2%}\")\n",
    "    print(f\"  Optimized: {optimized_test_metrics['accuracy']:.2%}\")\n",
    "    print(f\"  Improvement: {(optimized_test_metrics['accuracy'] - baseline_test_metrics['accuracy']) * 100:+.2f}pp\")\n",
    "\n",
    "    # Per-subset breakdown\n",
    "    print(f\"\\nüìä PER-SUBSET BREAKDOWN (Test Set):\")\n",
    "    all_subsets = set(baseline_test_metrics['subset_accuracy'].keys()) | set(\n",
    "        optimized_test_metrics['subset_accuracy'].keys())\n",
    "\n",
    "    for subset in sorted(all_subsets):\n",
    "        base_acc = baseline_test_metrics['subset_accuracy'].get(subset, 0)\n",
    "        opt_acc = optimized_test_metrics['subset_accuracy'].get(subset, 0)\n",
    "        improvement = opt_acc - base_acc\n",
    "        print(f\"  {subset:20s}: {base_acc:.2%} ‚Üí {opt_acc:.2%} ({improvement * 100:+.1f}pp)\")\n",
    "\n",
    "    return {\n",
    "        'best_prompt': best_prompt,\n",
    "        'best_val_accuracy': best_acc,\n",
    "        'baseline_test_metrics': baseline_test_metrics,\n",
    "        'optimized_test_metrics': optimized_test_metrics,\n",
    "        'candidates': candidates,\n",
    "        'candidate_scores': candidate_val_scores,\n",
    "        'eval_history': adapter.eval_history\n",
    "    }\n",
    "\n",
    "print(\"‚úì Optimization function defined\")"
   ],
   "id": "af54a457c63c18e4"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_data"
   },
   "source": [
    "## Load Your Data\n",
    "\n",
    "Paste the file ID for your uploaded data file from the data preparation step."
   ],
   "id": "8f90733ff4c75863"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_file"
   },
   "outputs": [],
   "source": [
    "# Paste your file ID from the data preparation step\n",
    "DATA_FILE_ID = \"\"  # e.g., \"file-65aa3ce1-cc93-48d0-b871-b974665f3dd1\"\n",
    "\n",
    "if not DATA_FILE_ID:\n",
    "    raise ValueError(\"Please provide the DATA_FILE_ID\")\n",
    "\n",
    "# Download the data from Together AI\n",
    "print(\"üì• Downloading data from Together AI...\")\n",
    "data_path = \"uploaded_data.json\"\n",
    "client.files.retrieve_content(DATA_FILE_ID, output=data_path)\n",
    "print(f\"‚úì Downloaded data file\")\n",
    "\n",
    "# Load and split data\n",
    "train_data, val_data, test_data = load_and_split_data(data_path)"
   ],
   "id": "8190874d74f476c6"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_section"
   },
   "source": [
    "## Run Optimization"
   ],
   "id": "f221b494a47af25c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_optimization"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_ITERATIONS = 10\n",
    "MINIBATCH_SIZE = 5\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ GEPA JUDGE OPTIMIZATION WITH TOGETHER AI\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Data should already be loaded from previous cells\n",
    "print(f\"\\nUsing data:\")\n",
    "print(f\"  Train: {len(train_data)} examples\")\n",
    "print(f\"  Val:   {len(val_data)} examples\")\n",
    "print(f\"  Test:  {len(test_data)} examples\")\n",
    "\n",
    "# Create adapter\n",
    "adapter = TogetherEvalAdapter(\n",
    "    client=client,\n",
    "    judge_model=JUDGE_MODEL,\n",
    "    initial_prompt=INITIAL_JUDGE_PROMPT\n",
    ")\n",
    "\n",
    "# Create optimizer LM\n",
    "optimizer_lm = SimpleOptimizerLM(\n",
    "    model=OPTIMIZER_MODEL,\n",
    "    api_key=TOGETHER_API_KEY\n",
    ")\n",
    "\n",
    "# Run GEPA optimization\n",
    "results = run_gepa_optimization(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    adapter=adapter,\n",
    "    optimizer_lm=optimizer_lm,\n",
    "    max_iterations=MAX_ITERATIONS,\n",
    "    minibatch_size=MINIBATCH_SIZE\n",
    ")"
   ],
   "id": "7a148733370bef76"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_section"
   },
   "source": [
    "## Save Results"
   ],
   "id": "fbc868fad97c17da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_dir = Path(\"results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save optimized prompt\n",
    "prompt_path = output_dir / f\"optimized_prompt_{timestamp}.txt\"\n",
    "with open(prompt_path, 'w') as f:\n",
    "    f.write(results['best_prompt'])\n",
    "print(f\"\\nüíæ Saved optimized prompt to: {prompt_path}\")\n",
    "\n",
    "# Save full results\n",
    "results_path = output_dir / f\"optimization_results_{timestamp}.json\"\n",
    "\n",
    "# Make results JSON-serializable\n",
    "json_results = {\n",
    "    'best_prompt': results['best_prompt'],\n",
    "    'best_val_accuracy': float(results['best_val_accuracy']),\n",
    "    'baseline_test_accuracy': float(results['baseline_test_metrics']['accuracy']),\n",
    "    'optimized_test_accuracy': float(results['optimized_test_metrics']['accuracy']),\n",
    "    'improvement': float(\n",
    "        results['optimized_test_metrics']['accuracy'] - results['baseline_test_metrics']['accuracy']),\n",
    "    'baseline_test_metrics': {\n",
    "        'accuracy': float(results['baseline_test_metrics']['accuracy']),\n",
    "        'a_wins': results['baseline_test_metrics']['a_wins'],\n",
    "        'b_wins': results['baseline_test_metrics']['b_wins'],\n",
    "        'ties': results['baseline_test_metrics']['ties'],\n",
    "        'subset_accuracy': {k: float(v) for k, v in results['baseline_test_metrics']['subset_accuracy'].items()}\n",
    "    },\n",
    "    'optimized_test_metrics': {\n",
    "        'accuracy': float(results['optimized_test_metrics']['accuracy']),\n",
    "        'a_wins': results['optimized_test_metrics']['a_wins'],\n",
    "        'b_wins': results['optimized_test_metrics']['b_wins'],\n",
    "        'ties': results['optimized_test_metrics']['ties'],\n",
    "        'subset_accuracy': {k: float(v) for k, v in results['optimized_test_metrics']['subset_accuracy'].items()}\n",
    "    },\n",
    "    'num_candidates': len(results['candidates']),\n",
    "    'candidate_scores': [float(s) for s in results['candidate_scores']],\n",
    "    'config': {\n",
    "        'judge_model': JUDGE_MODEL,\n",
    "        'optimizer_model': OPTIMIZER_MODEL,\n",
    "        'train_size': TRAIN_SIZE,\n",
    "        'val_size': VAL_SIZE,\n",
    "        'test_size': len(test_data),\n",
    "        'max_iterations': MAX_ITERATIONS,\n",
    "        'minibatch_size': MINIBATCH_SIZE\n",
    "    },\n",
    "    'timestamp': timestamp\n",
    "}\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(json_results, f, indent=2)\n",
    "print(f\"üíæ Saved results to: {results_path}\")\n",
    "\n",
    "# Display optimized prompt\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"üìù OPTIMIZED JUDGE PROMPT\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(results['best_prompt'])\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "print(\"\\n‚úÖ Optimization complete!\")"
   ],
   "id": "ac47aa62059f235c"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_section"
   },
   "source": [
    "## Download Results"
   ],
   "id": "66f95fa71a7a7c3e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "# Download the optimized prompt and results\n",
    "files.download(str(prompt_path))\n",
    "files.download(str(results_path))\n",
    "\n",
    "print(\"\\nüì• Files downloaded to your local machine!\")"
   ],
   "id": "2c3d5ec0e4644663"
  }
 ],
 "metadata": {
  "colab": {
   "name": "GEPA_Judge_Optimization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
