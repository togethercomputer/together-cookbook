{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating LLMs on SimpleQA Using Batch Inference API\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Evals/Batch_Inference_Evals.ipynb)\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to use Together AI's Batch Inference API to evaluate large language models on OpenAI's SimpleQA benchmark. We'll evaluate DeepSeek-V3-0324's performance on factual question answering and use an LLM-as-a-Judge grading system.\n",
    "\n",
    "By the end of this tutorial, you'll understand how to:\n",
    "- Format requests for the Batch Inference API\n",
    "- Evaluate LLMs on factual QA benchmarks\n",
    "- Implement automated grading workflows\n",
    "- Calculate evaluation metrics for model performance\n",
    "\n",
    "### What is SimpleQA?\n",
    "SimpleQA is a benchmark consisting of factual questions with short answers. It tests a model's ability to provide accurate, concise responses to straightforward factual queries. The benchmark includes questions across various domains like science, geography, sports, and arts.\n",
    "\n",
    "### Methodology\n",
    "Our evaluation follows a two-step process:\n",
    "1. **Answer Generation**: Use DeepSeek-V3-0324 to generate answers to SimpleQA questions\n",
    "2. **Automated Grading**: Use Llama 3.3 70B as a judge to grade the answers as CORRECT, INCORRECT, or NOT_ATTEMPTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU together pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Understanding the Dataset\n",
    "Let's examine the SimpleQA dataset structure to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'topic': 'Science and technology', 'answer_ty...</td>\n",
       "      <td>Who received the IEEE Frank Rosenblatt Award i...</td>\n",
       "      <td>Michio Sugeno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'topic': 'Science and technology', 'answer_ty...</td>\n",
       "      <td>Who was awarded the Oceanography Society's Jer...</td>\n",
       "      <td>Annick Bricaud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'topic': 'Geography', 'answer_type': 'Place',...</td>\n",
       "      <td>What's the name of the women's liberal arts co...</td>\n",
       "      <td>Radcliffe College</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'topic': 'Sports', 'answer_type': 'Person', '...</td>\n",
       "      <td>In whose honor was the Leipzig 1877 tournament...</td>\n",
       "      <td>Adolf Anderssen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'topic': 'Art', 'answer_type': 'Person', 'url...</td>\n",
       "      <td>According to Karl K√ºchler, what did Empress El...</td>\n",
       "      <td>Poet Henrich Heine.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            metadata  \\\n",
       "0  {'topic': 'Science and technology', 'answer_ty...   \n",
       "1  {'topic': 'Science and technology', 'answer_ty...   \n",
       "2  {'topic': 'Geography', 'answer_type': 'Place',...   \n",
       "3  {'topic': 'Sports', 'answer_type': 'Person', '...   \n",
       "4  {'topic': 'Art', 'answer_type': 'Person', 'url...   \n",
       "\n",
       "                                             problem               answer  \n",
       "0  Who received the IEEE Frank Rosenblatt Award i...        Michio Sugeno  \n",
       "1  Who was awarded the Oceanography Society's Jer...       Annick Bricaud  \n",
       "2  What's the name of the women's liberal arts co...    Radcliffe College  \n",
       "3  In whose honor was the Leipzig 1877 tournament...      Adolf Anderssen  \n",
       "4  According to Karl K√ºchler, what did Empress El...  Poet Henrich Heine.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# Load the SimpleQA dataset\n",
    "df = pd.read_csv(\"https://openaipublic.blob.core.windows.net/simple-evals/simple_qa_test_set.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that every row has a factual question and a corresponding answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 1: Preparing Student Model Requests\n",
    "Now we'll create batch requests for DeepSeek V3 to answer all the SimpleQA questions.\n",
    "\n",
    "### Batch API Request Format\n",
    "The Batch API expects JSONL format where each line contains a request with:\n",
    "- `custom_id`: Unique identifier for tracking responses\n",
    "- `body`: The actual API request payload\n",
    "\n",
    "```json\n",
    "{\"custom_id\": \"request-1\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello, world!\"}]}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template that is provided to the student model for simpleqa\n",
    "\n",
    "STUDENT_TEMPLATE = \"\"\"\n",
    "You are a helpful language model answering factual questions correctly. Give your best answer \n",
    "to the following factual question, focusing on concision and correctness. Do not output anything else \n",
    "besides your answer to the question. \n",
    "Question: {question}\n",
    "Predicted Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created simpleqa_batch_student.jsonl with 4326 batch requests\n",
      "Each request uses model: deepseek-ai/DeepSeek-V3\n"
     ]
    }
   ],
   "source": [
    "# Create batch API format file\n",
    "output_filename = \"simpleqa_batch_student.jsonl\"\n",
    "\n",
    "with open(output_filename, 'w') as f:\n",
    "    for idx, row in df.iterrows():\n",
    "        # Generate unique UUID for each request\n",
    "        custom_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Create the batch API request format\n",
    "        batch_request = {\n",
    "            \"custom_id\": custom_id,\n",
    "            \"body\": {\n",
    "                \"model\": \"deepseek-ai/DeepSeek-V3\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": STUDENT_TEMPLATE.format(question=row[\"problem\"])\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Write as JSONL (one JSON object per line)\n",
    "        f.write(json.dumps(batch_request) + '\\n')\n",
    "\n",
    "print(f\"Created {output_filename} with {len(df)} batch requests\")\n",
    "print(f\"Each request uses model: deepseek-ai/DeepSeek-V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample first few requests:\n",
      "Custom ID: 042fffda-ce74-4c38-8f42-45429b8ee3dc\n",
      "Question: \n",
      "You are a helpful language model answering factual questions correctly. Give your best answer \n",
      "to the following factual question, focusing on concision and correctness. Do not output anything else \n",
      "besides your answer to the question. \n",
      "Question: Who received the IEEE Frank Rosenblatt Award in 2010?\n",
      "Predicted Answer: \n",
      "\n",
      "---\n",
      "Custom ID: f1305d3b-0688-439b-8fb1-5fe5725575a7\n",
      "Question: \n",
      "You are a helpful language model answering factual questions correctly. Give your best answer \n",
      "to the following factual question, focusing on concision and correctness. Do not output anything else \n",
      "besides your answer to the question. \n",
      "Question: Who was awarded the Oceanography Society's Jerlov Award in 2018?\n",
      "Predicted Answer: \n",
      "\n",
      "---\n",
      "Custom ID: 917ccf80-be0e-4043-8836-481275015830\n",
      "Question: \n",
      "You are a helpful language model answering factual questions correctly. Give your best answer \n",
      "to the following factual question, focusing on concision and correctness. Do not output anything else \n",
      "besides your answer to the question. \n",
      "Question: What's the name of the women's liberal arts college in Cambridge, Massachusetts?\n",
      "Predicted Answer: \n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample first few requests:\")\n",
    "\n",
    "# Show first few requests as verification\n",
    "with open(output_filename, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 3:  # Show first 3 requests\n",
    "            request = json.loads(line)\n",
    "            print(f\"Custom ID: {request['custom_id']}\")\n",
    "            print(f\"Question: {request['body']['messages'][0]['content']}\")\n",
    "            print(\"---\")\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custom_id</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>042fffda-ce74-4c38-8f42-45429b8ee3dc</td>\n",
       "      <td>{'model': 'deepseek-ai/DeepSeek-V3', 'messages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f1305d3b-0688-439b-8fb1-5fe5725575a7</td>\n",
       "      <td>{'model': 'deepseek-ai/DeepSeek-V3', 'messages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>917ccf80-be0e-4043-8836-481275015830</td>\n",
       "      <td>{'model': 'deepseek-ai/DeepSeek-V3', 'messages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>451ba7ff-9385-43f9-80bf-03bafccf7942</td>\n",
       "      <td>{'model': 'deepseek-ai/DeepSeek-V3', 'messages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20ec64a2-2a87-4d28-be0a-0fe145e57ea2</td>\n",
       "      <td>{'model': 'deepseek-ai/DeepSeek-V3', 'messages...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              custom_id  \\\n",
       "0  042fffda-ce74-4c38-8f42-45429b8ee3dc   \n",
       "1  f1305d3b-0688-439b-8fb1-5fe5725575a7   \n",
       "2  917ccf80-be0e-4043-8836-481275015830   \n",
       "3  451ba7ff-9385-43f9-80bf-03bafccf7942   \n",
       "4  20ec64a2-2a87-4d28-be0a-0fe145e57ea2   \n",
       "\n",
       "                                                body  \n",
       "0  {'model': 'deepseek-ai/DeepSeek-V3', 'messages...  \n",
       "1  {'model': 'deepseek-ai/DeepSeek-V3', 'messages...  \n",
       "2  {'model': 'deepseek-ai/DeepSeek-V3', 'messages...  \n",
       "3  {'model': 'deepseek-ai/DeepSeek-V3', 'messages...  \n",
       "4  {'model': 'deepseek-ai/DeepSeek-V3', 'messages...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSONL file into a DataFrame\n",
    "batch_df = pd.read_json('simpleqa_batch_student.jsonl', lines=True)\n",
    "\n",
    "# Add custom_id column to df so we can match the output to the input\n",
    "df['custom_id'] = batch_df['custom_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Step 2: Running the Student Model Batch Job\n",
    "Time to submit our batch job and wait for DeepSeek V3-0324 to generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file simpleqa_batch_student.jsonl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.24M/2.24M [00:00<00:00, 4.24MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch file ID: file-73585c5e-0a77-49f6-ac0c-604ebd49727b\n"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together()\n",
    "\n",
    "# Upload the batch file\n",
    "batch_file = client.files.upload(file=\"simpleqa_batch_student.jsonl\", purpose=\"batch-api\")\n",
    "\n",
    "print(f\"Batch file ID: {batch_file.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch created with ID: 1ea94710-592e-4f11-a92f-1a91a4ba3454\n"
     ]
    }
   ],
   "source": [
    "# Create the batch job\n",
    "batch = client.batches.create_batch(file_id=batch_file.id, endpoint=\"/v1/chat/completions\")\n",
    "\n",
    "print(f\"Batch created with ID: {batch.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchJobStatus.IN_PROGRESS\n"
     ]
    }
   ],
   "source": [
    "# monitor the batch status\n",
    "batch_stat = client.batches.get_batch(batch.id)\n",
    "\n",
    "print(batch_stat.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BatchJob(id='a2e03a80-a083-4c73-8790-43cac5d40715', user_id='66f0bd504fb9511df3489b9a', input_file_id='file-c6131920-f4b3-44ff-8daa-f8e3f5cc4e70', file_size_bytes=1056749, status=<BatchJobStatus.COMPLETED: 'COMPLETED'>, job_deadline=datetime.datetime(2025, 6, 10, 21, 14, 25, 429867, tzinfo=TzInfo(UTC)), created_at=datetime.datetime(2025, 6, 9, 21, 14, 25, 429872, tzinfo=TzInfo(UTC)), endpoint='/v1/chat/completions', progress=100.0, model_id='deepseek-ai/DeepSeek-R1', output_file_id='file-94924b8e-ac08-4166-932d-74055bbda804', error_file_id='file-ac6939fc-b542-438d-9487-cb034afad215', error=None, completed_at=datetime.datetime(2025, 6, 9, 23, 8, 52, 900997, tzinfo=TzInfo(UTC))),\n",
       " BatchJob(id='a14c0f60-63c3-4ce4-9181-d6a69d48655e', user_id='66f0bd504fb9511df3489b9a', input_file_id='file-c6131920-f4b3-44ff-8daa-f8e3f5cc4e70', file_size_bytes=1056749, status=<BatchJobStatus.COMPLETED: 'COMPLETED'>, job_deadline=datetime.datetime(2025, 6, 10, 21, 13, 54, 381593, tzinfo=TzInfo(UTC)), created_at=datetime.datetime(2025, 6, 9, 21, 13, 54, 381596, tzinfo=TzInfo(UTC)), endpoint='/v1/chat/completions', progress=100.0, model_id='deepseek-ai/DeepSeek-R1', output_file_id='file-cd65f265-538c-47d2-8373-0fb5eef41f0b', error_file_id='file-7927bcf4-ba29-4ce4-8cc9-fb96af6c8db4', error=None, completed_at=datetime.datetime(2025, 6, 9, 23, 8, 20, 271809, tzinfo=TzInfo(UTC))),\n",
       " BatchJob(id='1ea94710-592e-4f11-a92f-1a91a4ba3454', user_id='66f0bd504fb9511df3489b9a', input_file_id='file-73585c5e-0a77-49f6-ac0c-604ebd49727b', file_size_bytes=2237747, status=<BatchJobStatus.IN_PROGRESS: 'IN_PROGRESS'>, job_deadline=datetime.datetime(2025, 6, 12, 6, 11, 22, 752345, tzinfo=TzInfo(UTC)), created_at=datetime.datetime(2025, 6, 11, 6, 11, 22, 752350, tzinfo=TzInfo(UTC)), endpoint='/v1/chat/completions', progress=71.75, model_id='deepseek-ai/DeepSeek-V3', output_file_id=None, error_file_id=None, error=None, completed_at=None),\n",
       " BatchJob(id='1e82cb6d-e79e-4e7a-993a-ed59b24a21e4', user_id='66f0bd504fb9511df3489b9a', input_file_id='file-c6131920-f4b3-44ff-8daa-f8e3f5cc4e70', file_size_bytes=1056749, status=<BatchJobStatus.COMPLETED: 'COMPLETED'>, job_deadline=datetime.datetime(2025, 6, 10, 22, 18, 17, 586343, tzinfo=TzInfo(UTC)), created_at=datetime.datetime(2025, 6, 9, 22, 18, 17, 586348, tzinfo=TzInfo(UTC)), endpoint='/v1/chat/completions', progress=100.0, model_id='deepseek-ai/DeepSeek-R1', output_file_id='file-1f20c3d2-1d53-429f-9b16-acc0b986931d', error_file_id='file-c432ae3a-0ff0-4968-b73b-826b20c7c9c0', error=None, completed_at=datetime.datetime(2025, 6, 9, 23, 9, 17, 546888, tzinfo=TzInfo(UTC)))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all batches - contains other batches as well\n",
    "client.batches.list_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file simpleqa_v3_output.jsonl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.39M/2.39M [00:00<00:00, 6.00MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the file content if job completed\n",
    "if batch_stat.status == 'COMPLETED':\n",
    "    output_response = client.files.retrieve_content(id=batch_stat.output_file_id,\n",
    "                                                    output=\"simpleqa_v3_output.jsonl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 3: Analyzing Model Responses\n",
    "Let's examine some of the generated responses before we grade them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the combined output:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>custom_id</th>\n",
       "      <th>response</th>\n",
       "      <th>metadata</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>br_bJdmq2ZaOJVtnquDCyMUaZBXCdhoRejs5DEaetspnR0</td>\n",
       "      <td>042fffda-ce74-4c38-8f42-45429b8ee3dc</td>\n",
       "      <td>{'status_code': 200, 'body': {'choices': [{'fi...</td>\n",
       "      <td>{'topic': 'Science and technology', 'answer_ty...</td>\n",
       "      <td>Who received the IEEE Frank Rosenblatt Award i...</td>\n",
       "      <td>Michio Sugeno</td>\n",
       "      <td>{'model': 'deepseek-ai/DeepSeek-V3', 'messages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>br_GTayN4efdayplpuNnIvXAC6W1V552BX4f6UNTafNIso</td>\n",
       "      <td>f1305d3b-0688-439b-8fb1-5fe5725575a7</td>\n",
       "      <td>{'status_code': 200, 'body': {'choices': [{'fi...</td>\n",
       "      <td>{'topic': 'Science and technology', 'answer_ty...</td>\n",
       "      <td>Who was awarded the Oceanography Society's Jer...</td>\n",
       "      <td>Annick Bricaud</td>\n",
       "      <td>{'model': 'deepseek-ai/DeepSeek-V3', 'messages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>br_Ex2m6NyCx9XKARt8HIvIaaxy8A1YQymlC5IQAcGXyQM</td>\n",
       "      <td>917ccf80-be0e-4043-8836-481275015830</td>\n",
       "      <td>{'status_code': 200, 'body': {'choices': [{'fi...</td>\n",
       "      <td>{'topic': 'Geography', 'answer_type': 'Place',...</td>\n",
       "      <td>What's the name of the women's liberal arts co...</td>\n",
       "      <td>Radcliffe College</td>\n",
       "      <td>{'model': 'deepseek-ai/DeepSeek-V3', 'messages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>br_rlKv8Cvs2euv2fsuyQkGtK16SH4Zfbp6kfBjf7estG0</td>\n",
       "      <td>451ba7ff-9385-43f9-80bf-03bafccf7942</td>\n",
       "      <td>{'status_code': 200, 'body': {'choices': [{'fi...</td>\n",
       "      <td>{'topic': 'Sports', 'answer_type': 'Person', '...</td>\n",
       "      <td>In whose honor was the Leipzig 1877 tournament...</td>\n",
       "      <td>Adolf Anderssen</td>\n",
       "      <td>{'model': 'deepseek-ai/DeepSeek-V3', 'messages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>br_hvXdufXYdjAZz3uQwWQrzAcdxrLW1P-mFQEJ7Wsb6pg</td>\n",
       "      <td>20ec64a2-2a87-4d28-be0a-0fe145e57ea2</td>\n",
       "      <td>{'status_code': 200, 'body': {'choices': [{'fi...</td>\n",
       "      <td>{'topic': 'Art', 'answer_type': 'Person', 'url...</td>\n",
       "      <td>According to Karl K√ºchler, what did Empress El...</td>\n",
       "      <td>Poet Henrich Heine.</td>\n",
       "      <td>{'model': 'deepseek-ai/DeepSeek-V3', 'messages...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id  \\\n",
       "0  br_bJdmq2ZaOJVtnquDCyMUaZBXCdhoRejs5DEaetspnR0   \n",
       "1  br_GTayN4efdayplpuNnIvXAC6W1V552BX4f6UNTafNIso   \n",
       "2  br_Ex2m6NyCx9XKARt8HIvIaaxy8A1YQymlC5IQAcGXyQM   \n",
       "3  br_rlKv8Cvs2euv2fsuyQkGtK16SH4Zfbp6kfBjf7estG0   \n",
       "4  br_hvXdufXYdjAZz3uQwWQrzAcdxrLW1P-mFQEJ7Wsb6pg   \n",
       "\n",
       "                              custom_id  \\\n",
       "0  042fffda-ce74-4c38-8f42-45429b8ee3dc   \n",
       "1  f1305d3b-0688-439b-8fb1-5fe5725575a7   \n",
       "2  917ccf80-be0e-4043-8836-481275015830   \n",
       "3  451ba7ff-9385-43f9-80bf-03bafccf7942   \n",
       "4  20ec64a2-2a87-4d28-be0a-0fe145e57ea2   \n",
       "\n",
       "                                            response  \\\n",
       "0  {'status_code': 200, 'body': {'choices': [{'fi...   \n",
       "1  {'status_code': 200, 'body': {'choices': [{'fi...   \n",
       "2  {'status_code': 200, 'body': {'choices': [{'fi...   \n",
       "3  {'status_code': 200, 'body': {'choices': [{'fi...   \n",
       "4  {'status_code': 200, 'body': {'choices': [{'fi...   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {'topic': 'Science and technology', 'answer_ty...   \n",
       "1  {'topic': 'Science and technology', 'answer_ty...   \n",
       "2  {'topic': 'Geography', 'answer_type': 'Place',...   \n",
       "3  {'topic': 'Sports', 'answer_type': 'Person', '...   \n",
       "4  {'topic': 'Art', 'answer_type': 'Person', 'url...   \n",
       "\n",
       "                                             problem               answer  \\\n",
       "0  Who received the IEEE Frank Rosenblatt Award i...        Michio Sugeno   \n",
       "1  Who was awarded the Oceanography Society's Jer...       Annick Bricaud   \n",
       "2  What's the name of the women's liberal arts co...    Radcliffe College   \n",
       "3  In whose honor was the Leipzig 1877 tournament...      Adolf Anderssen   \n",
       "4  According to Karl K√ºchler, what did Empress El...  Poet Henrich Heine.   \n",
       "\n",
       "                                                body  \n",
       "0  {'model': 'deepseek-ai/DeepSeek-V3', 'messages...  \n",
       "1  {'model': 'deepseek-ai/DeepSeek-V3', 'messages...  \n",
       "2  {'model': 'deepseek-ai/DeepSeek-V3', 'messages...  \n",
       "3  {'model': 'deepseek-ai/DeepSeek-V3', 'messages...  \n",
       "4  {'model': 'deepseek-ai/DeepSeek-V3', 'messages...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the JSONL file into a dataframe\n",
    "df_output = pd.read_json('simpleqa_v3_output.jsonl', lines=True)\n",
    "\n",
    "# Merge with original dataframe and batch dataframe using custom_id\n",
    "df_combined = pd.merge(df_output, df, on='custom_id', how='inner')\n",
    "df_combined = pd.merge(df_combined, batch_df, on='custom_id', how='inner')\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"First few rows of the combined output:\")\n",
    "display(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Prompt: \n",
      "You are a helpful language model answering factual questions correctly. Give your best answer \n",
      "to the following factual question, focusing on concision and correctness. Do not output anything else \n",
      "besides your answer to the question. \n",
      "Question: Which architects designed the Abasto?\n",
      "Predicted Answer: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Prompt:\", df_combined['body'].iloc[100]['messages'][0]['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: Juan Antonio Buschiazzo and Virgilio Colombo.\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Answer:\", df_combined['response'].iloc[100]['body']['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Answer: Jos√© Luis Delpini, Viktor Sulƒçiƒç and Ra√∫l Bes\n"
     ]
    }
   ],
   "source": [
    "print(\"Gold Answer:\", df_combined['answer'].iloc[100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for this particular example the model got wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 4: Preparing Grader Requests\n",
    "Now we'll create batch requests for our judge model to evaluate the answers.\n",
    "\n",
    "### Grading Criteria\n",
    "The grader uses strict criteria from the SimpleQA paper:\n",
    "- **CORRECT**: Answer contains all required information without contradictions\n",
    "- **INCORRECT**: Answer contains factual errors or contradictions\n",
    "- **NOT_ATTEMPTED**: Answer doesn't provide the required information but doesn't contradict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADER_TEMPLATE = \"\"\"\n",
    "Your job is to look at a question, a gold target, and a predicted answer, and then assign a\n",
    "grade of either [\"CORRECT\", \"INCORRECT\", \"NOT_ATTEMPTED\"].\n",
    "First, I will give examples of each grade, and then you will grade a new example.\n",
    "The following are examples of CORRECT predicted answers.\n",
    "‚Äò‚Äò‚Äò\n",
    "Question: What are the names of Barack Obama‚Äôs children?\n",
    "Gold target: Malia Obama and Sasha Obama\n",
    "Predicted answer 1: sasha and malia obama\n",
    "Predicted answer 2: most people would say Malia and Sasha, but I‚Äôm not sure and would have to\n",
    "double check\n",
    "Predicted answer 3: Barack Obama has two daughters. Their names are Malia Ann and Natasha\n",
    "Marian, but they are commonly referred to as Malia Obama and Sasha Obama. Malia was born\n",
    "on July 4, 1998, and Sasha was born on June 10, 2001.\n",
    "‚Äò‚Äò‚Äò\n",
    "These predicted answers are all CORRECT because:\n",
    "- They fully contain the important information in the gold target.\n",
    "- They do not contain any information that contradicts the gold target.\n",
    "- Only semantic meaning matters; capitalization, punctuation, grammar, and order don‚Äôt\n",
    "matter.\n",
    "- Hedging and guessing are permissible, provided that the gold target is fully included\n",
    "and the response contains no incorrect information or contradictions.\n",
    "The following are examples of INCORRECT predicted answers.\n",
    "‚Äò‚Äò‚Äò\n",
    "Question: What are the names of Barack Obama‚Äôs children?\n",
    "Gold target: Malia and Sasha\n",
    "Predicted answer 1: Malia.\n",
    "Predicted answer 2: Malia, Sasha, and Susan.\n",
    "Predicted answer 3: Barack Obama does not have any children.\n",
    "Predicted answer 4: I think it‚Äôs either Malia and Sasha. Or it could be Malia and Jackie. Or\n",
    "it could be Joey and Malia.\n",
    "Predicted answer 4: While I don‚Äôt know their exact names, I can tell you that Barack Obama\n",
    "has three children.\n",
    "Predicted answer 5: It‚Äôs possible you may mean Betsy and Olivia. However, you should clarify\n",
    "further details with updated references if necessary. Is that the correct answer?\n",
    "Predicted answer 6: It may be the case that Obama‚Äôs child is named James. However, it‚Äôs\n",
    "recommended to confirm the most accurate and updated information since this could change\n",
    "over time. This model may not always reflect the most current information.\n",
    "‚Äò‚Äò‚Äò\n",
    "These predicted answers are all INCORRECT because:\n",
    "- A factual statement in the answer contradicts the gold target. Incorrect statements\n",
    "that have some hedging (e.g., \"it is possible that\", \"although i‚Äôm not sure, i think\n",
    "\") are also considered incorrect.\n",
    "The following are examples of NOT_ATTEMPTED predicted answers.\n",
    "‚Äò‚Äò‚Äò\n",
    "Question: What are the names of Barack Obama‚Äôs children?\n",
    "Gold target: Malia and Sasha\n",
    "Predicted answer 1: I don‚Äôt know.\n",
    "Predicted answer 2: I need more context about which Obama you are talking about.\n",
    "Predicted answer 3: Without researching the web, I cannot answer this question. However, I\n",
    "can tell you that Barack Obama has two children.\n",
    "11\n",
    "Predicted answer 4: Barack Obama has two children. I know that one of them is Malia, but I‚Äôm\n",
    "not sure about the other one.\n",
    "‚Äò‚Äò‚Äò\n",
    "These predicted answers are all NOT_ATTEMPTED because:\n",
    "- The important information in the gold target is not included in the answer.\n",
    "- No statements in the answer contradict the gold target.\n",
    "Also note the following things:\n",
    "- For grading questions where the gold target is a number, the predicted answer needs to be\n",
    "correct to the last significant figure in the gold answer. For example, consider a\n",
    "question \"How many citations does the Transformer Paper have?\" with gold target \"120k\".\n",
    "- Predicted answers \"120k\", \"124k\", and 115k\" are all CORRECT.\n",
    "- Predicted answers \"100k\" and \"113k\" are INCORRECT.\n",
    "- Predicted answers \"around 100k\" and \"more than 50k\" are considered NOT_ATTEMPTED\n",
    "because they neither confirm nor contradict the gold target.\n",
    "- The gold target may contain more information than the question. In such cases, the\n",
    "predicted answer only needs to contain the information that is in the question.\n",
    "- For example, consider the question \"What episode did Derek and Meredith get legally\n",
    "married in Grey‚Äôs Anatomy?\" with gold target \"Season 7, Episode 20: White Wedding\".\n",
    "Either \"Season 7, Episode 20\" or \"White Wedding\" would be considered a CORRECT answer\n",
    ".\n",
    "- Do not punish predicted answers if they omit information that would be clearly inferred\n",
    "from the question.\n",
    "- For example, consider the question \"What city is OpenAI headquartered in?\" and the gold\n",
    "target \"San Francisco, California\". The predicted answer \"San Francisco\" would be\n",
    "considered CORRECT, even though it does not include \"California\".\n",
    "- Consider the question \"What award did A pretrainer‚Äôs guide to training data: Measuring\n",
    "the effects of data age, domain coverage, quality, & toxicity win at NAACL ‚Äô24?\", the\n",
    "gold target is \"Outstanding Paper Award\". The predicted answer \"Outstanding Paper\"\n",
    "would be considered CORRECT, because \"award\" is presumed in the question.\n",
    "- For the question \"What is the height of Jason Wei in meters?\", the gold target is \"1.73\n",
    "m\". The predicted answer \"1.75\" would be considered CORRECT, because meters is\n",
    "specified in the question.\n",
    "- For the question \"What is the name of Barack Obama‚Äôs wife?\", the gold target is \"\n",
    "Michelle Obama\". The predicted answer \"Michelle\" would be considered CORRECT, because\n",
    "the last name can be presumed.\n",
    "- Do not punish for typos in people‚Äôs name if it‚Äôs clearly the same name.\n",
    "- For example, if the gold target is \"Hyung Won Chung\", you can consider the following\n",
    "predicted answers as correct: \"Hyoong Won Choong\", \"Hyungwon Chung\", or \"Hyun Won\n",
    "Chung\".\n",
    "Here is a new example. Simply reply with either CORRECT, INCORRECT, NOT ATTEMPTED. Don‚Äôt\n",
    "apologize or correct yourself if there was a mistake; we are just trying to grade the\n",
    "answer.\n",
    "‚Äò‚Äò‚Äò\n",
    "Question: {question}\n",
    "Gold target: {target}\n",
    "Predicted answer: {predicted_answer}\n",
    "‚Äò‚Äò‚Äò\n",
    "Grade the predicted answer of this new question as one of:\n",
    "A: CORRECT\n",
    "B: INCORRECT\n",
    "C: NOT_ATTEMPTED\n",
    "Just return the one of the letters \"A\", \"B\", or \"C\", with no text around it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created simpleqa_batch_grader.jsonl with 4326 batch requests\n",
      "Each request uses model: meta-llama/Llama-3.3-70B-Instruct-Turbo\n"
     ]
    }
   ],
   "source": [
    "# Create batch API format file\n",
    "output_filename = \"simpleqa_batch_grader.jsonl\"\n",
    "\n",
    "with open(output_filename, 'w') as f:\n",
    "    for idx, row in df_combined.iterrows():\n",
    "        # Use existing custom_id from combined dataframe\n",
    "        custom_id = row['custom_id']\n",
    "        \n",
    "        # Get model response and gold answer\n",
    "        model_response = row['response']['body']['choices'][0]['message']['content']\n",
    "        gold_answer = row['answer']\n",
    "        question = row['problem']\n",
    "        \n",
    "        # Create the batch API request format for grading\n",
    "        batch_request = {\n",
    "            \"custom_id\": custom_id,\n",
    "            \"body\": {\n",
    "                \"model\": \"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "                \"messages\": [\n",
    "                    { # the judge model uses a system prompt to guide its behavior\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an AI judge evaluating the correctness of answers to factual questions.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": GRADER_TEMPLATE.format(\n",
    "                            question=question,\n",
    "                            target=gold_answer,\n",
    "                            predicted_answer=model_response\n",
    "                        )\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Write as JSONL (one JSON object per line)\n",
    "        f.write(json.dumps(batch_request) + '\\n')\n",
    "\n",
    "print(f\"Created {output_filename} with {len(df_combined)} batch requests\")\n",
    "print(f\"Each request uses model: meta-llama/Llama-3.3-70B-Instruct-Turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Step 5: Running the Grader Batch Job\n",
    "Submit the grading requests and wait for evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading simpleqa_batch_grader.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file simpleqa_batch_grader.jsonl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9M/28.9M [00:05<00:00, 5.64MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grader batch file ID: file-45de36cc-41a7-4b34-9b00-2f730903d0d3\n",
      "Creating batch job...\n",
      "Grader batch created with ID: f5b32360-48b0-40c2-8c27-92d0d0f7e029\n",
      "Initial status: BatchJobStatus.IN_PROGRESS\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Upload the grader batch file\n",
    "print(\"Uploading simpleqa_batch_grader.jsonl...\")\n",
    "grader_batch_file = client.files.upload(file=\"simpleqa_batch_grader.jsonl\", purpose=\"batch-api\")\n",
    "print(f\"Grader batch file ID: {grader_batch_file.id}\")\n",
    "\n",
    "# Step 2: Create the batch job\n",
    "print(\"Creating batch job...\")\n",
    "grader_batch = client.batches.create_batch(file_id=grader_batch_file.id, endpoint=\"/v1/chat/completions\")\n",
    "print(f\"Grader batch created with ID: {grader_batch.id}\")\n",
    "\n",
    "# Step 3: Check initial status\n",
    "grader_batch_stat = client.batches.get_batch(grader_batch.id)\n",
    "print(f\"Initial status: {grader_batch_stat.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch is complete! Downloading results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file simpleqa_grader_output.jsonl: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.34M/2.34M [00:00<00:00, 4.39MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Poll for completion and download results when ready\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    grader_batch_stat = client.batches.get_batch(grader_batch.id)\n",
    "    print(f\"Current status: {grader_batch_stat.status}\")\n",
    "    \n",
    "    if grader_batch_stat.status == 'COMPLETED':\n",
    "        print(\"Batch is complete! Downloading results...\")\n",
    "        \n",
    "        # Download the output file\n",
    "        grader_output_response = client.files.retrieve_content(\n",
    "            id=grader_batch_stat.output_file_id,\n",
    "            output=\"simpleqa_grader_output.jsonl\"\n",
    "        )\n",
    "        break\n",
    "    \n",
    "    print(\"Waiting 1 minute before next check...\")\n",
    "    time.sleep(60)  # Wait 1 minute before checking again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 6: Final Results Analysis\n",
    "Let's analyze DeepSeek V3's performance on SimpleQA.\n",
    "\n",
    "### Key Metrics\n",
    "- **Accuracy**: Overall accuracy across all questions\n",
    "- **Accuracy for attempted**: Accuracy when the model provided an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n",
      "Correct: 919\n",
      "Incorrect: 3333\n",
      "Not Attempted: 74\n",
      "Total: 4326\n",
      "Accuracy = 21.2%\n",
      "Accuracy (attempted) = 21.6%\n"
     ]
    }
   ],
   "source": [
    "# Read and process the grader output file\n",
    "nright, nidk, nwrong = 0, 0, 0  # correspond to [A, C, B] in grading schema\n",
    "\n",
    "with open('simpleqa_grader_output.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        response = json.loads(line)\n",
    "        # Extract the grader's response from the output\n",
    "        grader_response = response['response']['body']['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        if grader_response == \"A\":  # correct\n",
    "            nright += 1\n",
    "        elif grader_response == \"B\":  # incorrect\n",
    "            nwrong += 1\n",
    "        elif grader_response == \"C\":  # not attempted\n",
    "            nidk += 1\n",
    "\n",
    "# Final results\n",
    "print(f'\\nFinal Results:')\n",
    "print(f'Correct: {nright}')\n",
    "print(f'Incorrect: {nwrong}')\n",
    "print(f'Not Attempted: {nidk}')\n",
    "print(f'Total: {nright + nwrong + nidk}')\n",
    "\n",
    "# Calculate probabilities for summary\n",
    "total = nright + nwrong + nidk\n",
    "attempted = nright + nwrong\n",
    "\n",
    "p_right = nright / total if total > 0 else 0\n",
    "p_right_attempted = nright / attempted if attempted > 0 else 0\n",
    "\n",
    "print(f'Accuracy = {p_right*100:.1f}%')\n",
    "print(f'Accuracy (attempted) = {p_right_attempted*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "We successfully evaluated **DeepSeek V3-0324** on 4,326 SimpleQA questions using Together AI's Batch Inference API, achieving and **Overall Accuracy**: 21.2% (919/4,326 questions)\n",
    "\n",
    "## üîë Key Takeaways\n",
    "\n",
    "- **Batch Processing Efficiency**: Batch inference dramatically reduces costs and eliminates rate limiting for large-scale evaluations\n",
    "- **Automated Evaluation**: LLM-as-a-judge enables scalable assessment without manual annotation\n",
    "- **Systematic Methodology**: Proper request formatting, progress monitoring, and result alignment are crucial for reliable evaluations\n",
    "- **Evaluation Complexity**: Even straightforward factual questions pose significant challenges for state-of-the-art models\n",
    "\n",
    "## üìö Learn More\n",
    "\n",
    "For detailed documentation on batch inference capabilities and implementation:\n",
    "üëâ **[Together AI Batch Inference Documentation](https://docs.together.ai/docs/batch-inference)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batchAPI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
