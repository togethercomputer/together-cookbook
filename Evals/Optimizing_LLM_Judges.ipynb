{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1addcb23",
   "metadata": {},
   "source": [
    "# Optimizing LLM Judges with RewardBench 2\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Evals/Optimizing_LLM_Judges.ipynb)\n",
    "\n",
    "<img src=\"../images/compare_eval.png\" width=\"750\">\n",
    "\n",
    "## üéØ What We'll Build\n",
    "\n",
    "This notebook demonstrates how to optimize LLM judges using the RewardBench 2 dataset. We'll:\n",
    "\n",
    "1. **Baseline Evaluation**: Test 5 different judge models on RewardBench 2\n",
    "2. **Response Collection**: Gather judge outputs for analysis and optimization\n",
    "3. **Prompt Optimization**: Use Together's Compare Eval API to improve judge prompts\n",
    "4. **Preference Fine-Tuning**: Train specialized judge models using preference data\n",
    "5. **Quality-Cost Analysis**: Compare approaches and recommend best practices\n",
    "\n",
    "**Key Questions:**\n",
    "- Which models make the best judges?\n",
    "- Can prompt engineering beat fine-tuning for judge quality?\n",
    "- What are the cost-performance trade-offs?\n",
    "- Which judges excel at specific tasks (safety, math, factuality)?\n",
    "\n",
    "**Concepts Covered:**\n",
    "- **LLM-as-a-Judge Evaluation**: Measuring judge model performance on preference data\n",
    "- **RewardBench 2**: A comprehensive benchmark for reward model evaluation\n",
    "- **Prompt Optimization**: Systematic improvement of judge instructions\n",
    "- **Preference Tuning**: Fine-tuning models on preference pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bad76f",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3253c6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU together datasets matplotlib seaborn pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2df544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jli/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/jli/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import together\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize Together client\n",
    "client = together.Client()\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef33338",
   "metadata": {},
   "source": [
    "## üìä Understanding RewardBench 2\n",
    "\n",
    "[RewardBench 2](https://huggingface.co/datasets/allenai/reward-bench-2) is a comprehensive benchmark for evaluating reward models and LLM judges. It tests capabilities across 6 categories:\n",
    "\n",
    "1. **Factuality**: Detecting hallucinations and errors\n",
    "2. **Precise Instruction Following**: Judging adherence to specific constraints\n",
    "3. **Math**: Mathematical reasoning and accuracy\n",
    "4. **Safety**: Compliance and harmful content detection\n",
    "5. **Focus**: Quality and relevance of responses\n",
    "6. **Ties**: Robustness when multiple valid answers exist\n",
    "\n",
    "**Key Features:**\n",
    "- Each example has 1 chosen response and 3 rejected responses\n",
    "- Success = Judge choices corellate with human preferences\n",
    "- Human-validated ground truth preferences\n",
    "- 1,865 high-quality examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc08fc85",
   "metadata": {},
   "source": [
    "## üîç Load and Explore RewardBench 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ef2638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RewardBench 2 dataset...\n",
      "\n",
      "‚úì Loaded 1865 examples\n",
      "\n",
      "Dataset features: {'id': Value('string'), 'prompt': Value('string'), 'chosen': List(Value('string')), 'rejected': List(Value('string')), 'num_correct': Value('int64'), 'num_incorrect': Value('int64'), 'total_completions': Value('int64'), 'models': List(Value('string')), 'subset': Value('string'), 'additional_metadata': {'category': Value('string'), 'correct': Value('string'), 'index': Value('float64'), 'instruction_id_list': List(Value('string')), 'label': Value('string'), 'method': Value('string'), 'models': List(Value('string')), 'prompt_norm': Value('string'), 'subcategory': Value('string'), 'valid': Value('float64')}}\n"
     ]
    }
   ],
   "source": [
    "# Load the RewardBench 2 dataset\n",
    "print(\"Loading RewardBench 2 dataset...\")\n",
    "reward_bench = load_dataset(\"allenai/reward-bench-2\", split=\"test\")\n",
    "\n",
    "print(f\"\\n‚úì Loaded {len(reward_bench)} examples\")\n",
    "print(f\"\\nDataset features: {reward_bench.features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00a4991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample examples from RewardBench 2:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "      <th>num_correct</th>\n",
       "      <th>num_incorrect</th>\n",
       "      <th>total_completions</th>\n",
       "      <th>models</th>\n",
       "      <th>subset</th>\n",
       "      <th>additional_metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hi do you know how to fix Codeium defaule sett...</td>\n",
       "      <td>[Certainly! The error `[ERROR]: [deadline_exce...</td>\n",
       "      <td>[To resolve the \"deadline_exceeded\" error in C...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[Qwen2.5-7B-Instruct, Mistral-7B-Instruct-v0.3...</td>\n",
       "      <td>Factuality</td>\n",
       "      <td>{'category': None, 'correct': 'ONE', 'index': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Is there a document other than the Bible that ...</td>\n",
       "      <td>[Yes, there are historical and literary source...</td>\n",
       "      <td>[One of the most significant non-Biblical sour...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[Qwen2.5-72B-Instruct, Llama-3.1-70B-Instruct,...</td>\n",
       "      <td>Factuality</td>\n",
       "      <td>{'category': None, 'correct': 'ONE', 'index': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>important dates in india</td>\n",
       "      <td>[Here are some important dates in Indian histo...</td>\n",
       "      <td>[India, a country rich in history and cultural...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[Llama-3.1-8B-Instruct, Llama-3.1-Tulu-3-8B, L...</td>\n",
       "      <td>Factuality</td>\n",
       "      <td>{'category': None, 'correct': 'ONE', 'index': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In currently existing DC universe...what citie...</td>\n",
       "      <td>[In the current DC universe, there isn't a dir...</td>\n",
       "      <td>[In the current DC Universe, there aren't spec...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[claude-3-5-sonnet-20241022, Qwen2.5-7B-Instru...</td>\n",
       "      <td>Factuality</td>\n",
       "      <td>{'category': None, 'correct': 'ONE', 'index': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>who are kyle dinkheller</td>\n",
       "      <td>[Kyle Dinkheller was a deputy sheriff in Laure...</td>\n",
       "      <td>[Kyle Dinkheller was a United States Deputy Sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[Llama-3.1-Tulu-3-70B, Llama-3.1-8B-Instruct, ...</td>\n",
       "      <td>Factuality</td>\n",
       "      <td>{'category': None, 'correct': 'ONE', 'index': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                             prompt  \\\n",
       "0  0  Hi do you know how to fix Codeium defaule sett...   \n",
       "1  1  Is there a document other than the Bible that ...   \n",
       "2  2                           important dates in india   \n",
       "3  3  In currently existing DC universe...what citie...   \n",
       "4  4                            who are kyle dinkheller   \n",
       "\n",
       "                                              chosen  \\\n",
       "0  [Certainly! The error `[ERROR]: [deadline_exce...   \n",
       "1  [Yes, there are historical and literary source...   \n",
       "2  [Here are some important dates in Indian histo...   \n",
       "3  [In the current DC universe, there isn't a dir...   \n",
       "4  [Kyle Dinkheller was a deputy sheriff in Laure...   \n",
       "\n",
       "                                            rejected  num_correct  \\\n",
       "0  [To resolve the \"deadline_exceeded\" error in C...            1   \n",
       "1  [One of the most significant non-Biblical sour...            1   \n",
       "2  [India, a country rich in history and cultural...            1   \n",
       "3  [In the current DC Universe, there aren't spec...            1   \n",
       "4  [Kyle Dinkheller was a United States Deputy Sh...            1   \n",
       "\n",
       "   num_incorrect  total_completions  \\\n",
       "0              3                  4   \n",
       "1              3                  4   \n",
       "2              3                  4   \n",
       "3              3                  4   \n",
       "4              3                  4   \n",
       "\n",
       "                                              models      subset  \\\n",
       "0  [Qwen2.5-7B-Instruct, Mistral-7B-Instruct-v0.3...  Factuality   \n",
       "1  [Qwen2.5-72B-Instruct, Llama-3.1-70B-Instruct,...  Factuality   \n",
       "2  [Llama-3.1-8B-Instruct, Llama-3.1-Tulu-3-8B, L...  Factuality   \n",
       "3  [claude-3-5-sonnet-20241022, Qwen2.5-7B-Instru...  Factuality   \n",
       "4  [Llama-3.1-Tulu-3-70B, Llama-3.1-8B-Instruct, ...  Factuality   \n",
       "\n",
       "                                 additional_metadata  \n",
       "0  {'category': None, 'correct': 'ONE', 'index': ...  \n",
       "1  {'category': None, 'correct': 'ONE', 'index': ...  \n",
       "2  {'category': None, 'correct': 'ONE', 'index': ...  \n",
       "3  {'category': None, 'correct': 'ONE', 'index': ...  \n",
       "4  {'category': None, 'correct': 'ONE', 'index': ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to pandas for easier exploration\n",
    "df = reward_bench.to_pandas()\n",
    "\n",
    "# Display first few examples\n",
    "print(\"Sample examples from RewardBench 2:\\n\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b46ea36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXAMPLE FROM REWARDBENCH 2\n",
      "================================================================================\n",
      "\n",
      "Subset: Factuality\n",
      "\n",
      "Prompt:\n",
      "Hi do you know how to fix Codeium defaule setting that is responsible for this  [ERROR]: [deadline_exceeded] context deadline exceeded\n",
      "\n",
      "================================================================================\n",
      "Chosen Response (Ground Truth Winner):\n",
      "Certainly! The error `[ERROR]: [deadline_exceeded] context deadline exceeded` typically indicates that the code generation process in Codeium (or another similar AI coding assistant) has timed out because it took too long to generate a response.\n",
      "\n",
      "Here are some steps you can take to address this issue:\n",
      "\n",
      "### 1. **Increase Timeout Settings:**\n",
      "   - **Codeium**: Unfortunately, Codeium does not have an official UI setting to increase the timeout. However, you can try to simplify your code or use more ...\n",
      "\n",
      "================================================================================\n",
      "Rejected Response 1:\n",
      "To resolve the \"deadline_exceeded\" error in Codeium, you can try the following steps to adjust the settings for increased time and resources allocation:\n",
      "\n",
      "1. Change the default timeout:\n",
      "   By default, Codeium has a 5-second timeout. To increase the timeout, you can add the following command to your prompt:\n",
      "\n",
      "   ```\n",
      "   /timeout 30s\n",
      "   ```\n",
      "   This sets the timeout to 30 seconds. You can adjust the time according to your needs.\n",
      "\n",
      "2. Increase the number of iterations:\n",
      "   By default, Codeium limits the ...\n",
      "\n",
      "================================================================================\n",
      "Rejected Response 2:\n",
      "It seems like you're encountering a timeout error in Codeium, possibly due to the default settings. Here's a general way to adjust the timeout limit in Python using the Codeium API, assuming you're using the `codeium` package. You can adjust the timeout limit as needed:\n",
      "\n",
      "1. First, you need to set the timeout parameter as an `environment` variable.\n",
      "\n",
      "```bash\n",
      "export OPENAI_API_TIMEOUT=your_desired_timeout_in_seconds\n",
      "```\n",
      "\n",
      "Replace `your_desired_timeout_in_seconds` with the time you want in seconds.\n",
      "\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Rejected Response 3:\n",
      "The `deadline_exceeded` error in Codeium (or any other AI-powered coding platform) usually occurs when the context deadline is exceeded, meaning the model couldn't process the input within the expected timeframe. Here are a few potential solutions to address this issue:\n",
      "\n",
      "### 1. Increase the Model's Timeout\n",
      "\n",
      "You can increase the timeout by modifying the Codeium settings or environment variables. However, I need to inform you that Codeium does not expose a straightforward API to adjust the model t...\n"
     ]
    }
   ],
   "source": [
    "# Look at a specific example in detail\n",
    "example_idx = 0\n",
    "example = reward_bench[example_idx]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXAMPLE FROM REWARDBENCH 2\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSubset: {example['subset']}\")\n",
    "print(f\"\\nPrompt:\\n{example['prompt']}\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Chosen Response (Ground Truth Winner):\\n{example['chosen'][0][:500]}...\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Rejected Response 1:\\n{example['rejected'][0][:500]}...\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Rejected Response 2:\\n{example['rejected'][1][:500]}...\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Rejected Response 3:\\n{example['rejected'][2][:500]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec42d96",
   "metadata": {},
   "source": [
    "## üìà Dataset Distribution Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9dc0a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution across subsets:\n",
      "subset\n",
      "Focus         495\n",
      "Factuality    475\n",
      "Safety        450\n",
      "Math          183\n",
      "Precise IF    160\n",
      "Ties          102\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total examples: 1865\n"
     ]
    }
   ],
   "source": [
    "# Analyze distribution across subsets\n",
    "subset_counts = df['subset'].value_counts()\n",
    "\n",
    "print(\"Distribution across subsets:\")\n",
    "print(subset_counts)\n",
    "print(f\"\\nTotal examples: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96ac969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Dataset contains 6 unique categories\n"
     ]
    }
   ],
   "source": [
    "# Visualize subset distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = subset_counts.plot(kind='bar', ax=ax, color='#FF6B35', edgecolor='black', alpha=0.8)\n",
    "\n",
    "ax.set_title('RewardBench 2: Distribution Across Evaluation Categories', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Category', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Number of Examples', fontsize=14, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(subset_counts):\n",
    "    ax.text(i, v + 10, str(v), ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Dataset contains {len(subset_counts)} unique categories\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd10a2d",
   "metadata": {},
   "source": [
    "## üé≤ Create Stratified Train/Test Split\n",
    "\n",
    "For faster iteration and proper evaluation, we'll create:\n",
    "- **Train Set**: 1,000 stratified examples (for prompt optimization and fine-tuning)\n",
    "- **Test Set**: 300 stratified examples (for final evaluation)\n",
    "\n",
    "Both sets maintain the original subset distribution with zero overlap. You can toggle to use the full dataset if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df07fb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stratified train/test split...\n",
      "================================================================================\n",
      "‚úì TRAIN dataset: 997 examples\n",
      "\n",
      "  Train samples per subset:\n",
      "    Factuality: 254\n",
      "    Precise IF: 85\n",
      "    Math: 98\n",
      "    Safety: 241\n",
      "    Focus: 265\n",
      "    Ties: 54\n",
      "\n",
      "‚úì TEST dataset: 297 examples\n",
      "\n",
      "  Test samples per subset:\n",
      "    Factuality: 76\n",
      "    Precise IF: 25\n",
      "    Math: 29\n",
      "    Safety: 72\n",
      "    Focus: 79\n",
      "    Ties: 16\n",
      "\n",
      "‚úì Verification: 0 overlapping samples (should be 0)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration: Set to True to use full dataset, False for stratified train/test split\n",
    "USE_FULL_DATASET = False\n",
    "TRAIN_SIZE = 1000  # Number of training examples\n",
    "TEST_SIZE = 300    # Number of test examples\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "if USE_FULL_DATASET:\n",
    "    train_dataset = reward_bench\n",
    "    test_dataset = reward_bench\n",
    "    print(f\"‚úì Using FULL dataset: {len(reward_bench)} examples for both train and test\")\n",
    "else:\n",
    "    # Two-stage stratified sampling to create train/test split with no overlap\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    print(\"Creating stratified train/test split...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Stage 1: Sample TRAIN set with stratified sampling\n",
    "    train_subset_samples = {}\n",
    "    train_indices = []\n",
    "    \n",
    "    for subset in df['subset'].unique():\n",
    "        subset_count = len(df[df['subset'] == subset])\n",
    "        # Calculate proportional samples for train set\n",
    "        n_train = max(1, int(TRAIN_SIZE * (subset_count / len(df))))\n",
    "        train_subset_samples[subset] = n_train\n",
    "        \n",
    "        # Sample from this subset for training\n",
    "        subset_indices = df[df['subset'] == subset].index.tolist()\n",
    "        sampled_train = np.random.choice(\n",
    "            subset_indices, \n",
    "            size=min(n_train, len(subset_indices)), \n",
    "            replace=False\n",
    "        )\n",
    "        train_indices.extend(sampled_train.tolist())\n",
    "    \n",
    "    # Create train dataset\n",
    "    train_dataset = reward_bench.select(train_indices)\n",
    "    \n",
    "    # Stage 2: Sample TEST set from REMAINING indices (ensures no overlap)\n",
    "    remaining_indices_by_subset = {}\n",
    "    test_subset_samples = {}\n",
    "    test_indices = []\n",
    "    \n",
    "    for subset in df['subset'].unique():\n",
    "        # Get all indices for this subset\n",
    "        all_subset_indices = df[df['subset'] == subset].index.tolist()\n",
    "        # Get train indices for this subset\n",
    "        train_subset_indices = [i for i in train_indices if df.iloc[i]['subset'] == subset]\n",
    "        # Calculate remaining indices\n",
    "        remaining = [i for i in all_subset_indices if i not in train_subset_indices]\n",
    "        remaining_indices_by_subset[subset] = remaining\n",
    "        \n",
    "        # Calculate proportional samples for test set\n",
    "        subset_count = len(df[df['subset'] == subset])\n",
    "        n_test = max(1, int(TEST_SIZE * (subset_count / len(df))))\n",
    "        test_subset_samples[subset] = n_test\n",
    "        \n",
    "        # Sample from remaining indices for test set\n",
    "        sampled_test = np.random.choice(\n",
    "            remaining, \n",
    "            size=min(n_test, len(remaining)), \n",
    "            replace=False\n",
    "        )\n",
    "        test_indices.extend(sampled_test.tolist())\n",
    "    \n",
    "    # Create test dataset\n",
    "    test_dataset = reward_bench.select(test_indices)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"‚úì TRAIN dataset: {len(train_dataset)} examples\")\n",
    "    print(f\"\\n  Train samples per subset:\")\n",
    "    for subset, count in train_subset_samples.items():\n",
    "        print(f\"    {subset}: {count}\")\n",
    "    \n",
    "    print(f\"\\n‚úì TEST dataset: {len(test_dataset)} examples\")\n",
    "    print(f\"\\n  Test samples per subset:\")\n",
    "    for subset, count in test_subset_samples.items():\n",
    "        print(f\"    {subset}: {count}\")\n",
    "    \n",
    "    # Verify no overlap\n",
    "    overlap = set(train_indices) & set(test_indices)\n",
    "    print(f\"\\n‚úì Verification: {len(overlap)} overlapping samples (should be 0)\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "812e5123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Using 297 examples for baseline evaluation\n"
     ]
    }
   ],
   "source": [
    "# Select which dataset to use for baseline evaluation\n",
    "# Use test_dataset for unbiased evaluation, or train_dataset for development\n",
    "eval_dataset = test_dataset\n",
    "\n",
    "print(f\"üìä Using {len(eval_dataset)} examples for baseline evaluation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5cd7b5",
   "metadata": {},
   "source": [
    "## ü§ñ Judge Model Selection\n",
    " \n",
    "We'll evaluate 6 diverse judge models to understand their strengths and cost-performance trade-offs:\n",
    " \n",
    "1. **Kimi K2 Instruct** - Latest instruction-tuned offering from MoonshotAI\n",
    "2. **Kimi K2 Thinking** - Enhanced reasoning variant from MoonshotAI\n",
    "3. **DeepSeek V3.1** - Top-tier performance with current SOTA results\n",
    "4. **GPT-OSS 120B** - Large open-source OpenAI model\n",
    "5. **GLM 4.6** - Strong open-source model from ZhipuAI\n",
    "6. **Qwen3 235B** - Cutting-edge, large-scale Qwen model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d99e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Judge Models:\n",
      "  1. moonshotai/Kimi-K2-Instruct-0905\n",
      "  2. moonshotai/Kimi-K2-Thinking\n",
      "  3. deepseek-ai/DeepSeek-V3.1\n",
      "  4. openai/gpt-oss-120b\n",
      "  5. zai-org/GLM-4.6\n",
      "  6. Qwen/Qwen3-235B-A22B-Instruct-2507-tput\n"
     ]
    }
   ],
   "source": [
    "# Define top judge models to evaluate (updated with latest/best models)\n",
    "JUDGE_MODELS = [\n",
    "    \"moonshotai/Kimi-K2-Instruct-0905\",\n",
    "    \"moonshotai/Kimi-K2-Thinking\",\n",
    "    \"deepseek-ai/DeepSeek-V3.1\",\n",
    "    \"openai/gpt-oss-120b\",\n",
    "    \"zai-org/GLM-4.6\",\n",
    "    \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\",\n",
    "]\n",
    "\n",
    "# Create friendly names for each judge model\n",
    "JUDGE_NAMES = {\n",
    "    \"moonshotai/Kimi-K2-Instruct-0905\": \"Kimi K2 Instruct\",\n",
    "    \"moonshotai/Kimi-K2-Thinking\": \"Kimi K2 Thinking\",\n",
    "    \"deepseek-ai/DeepSeek-V3.1\": \"DeepSeek V3.1\",\n",
    "    \"openai/gpt-oss-120b\": \"GPT-OSS 120B\",\n",
    "    \"zai-org/GLM-4.6\": \"GLM 4.6\",\n",
    "    \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\": \"Qwen3 235B\",\n",
    "}\n",
    "\n",
    "print(\"Selected Judge Models:\")\n",
    "for i, model in enumerate(JUDGE_MODELS, 1):\n",
    "    print(f\"  {i}. {model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b3717",
   "metadata": {},
   "source": [
    "## üìù Prepare Test Dataset for Compare API\n",
    "\n",
    "We'll transform the test dataset to use with Together's Compare API. We'll use pairwise comparison: chosen vs rejected_1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78747131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Transformed 297 test examples\n",
      "\n",
      "Example transformed data:\n",
      "  Keys: ['id', 'prompt', 'chosen', 'rejected_1', 'subset']\n",
      "  Prompt: The Malay Archipelago, which consists of the islands that make up the modern day Philippines, Indone...\n",
      "  Chosen length: 2589 chars\n",
      "  Rejected_1 length: 1008 chars\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "def prepare_dataset_for_compare(dataset):\n",
    "    \"\"\"\n",
    "    Transform RewardBench 2 dataset for Compare API.\n",
    "    \n",
    "    - Unwraps chosen from list: chosen[0] ‚Üí \"chosen\"\n",
    "    - Takes first rejected: rejected[0] ‚Üí \"rejected_1\"\n",
    "    - Preserves metadata (id, prompt, subset)\n",
    "    \"\"\"\n",
    "    transformed = []\n",
    "    \n",
    "    for example in dataset:\n",
    "        transformed.append({\n",
    "            'id': example['id'],\n",
    "            'prompt': example['prompt'],\n",
    "            'chosen': example['chosen'][0],  # Unwrap from list\n",
    "            'rejected_1': example['rejected'][0],  # First rejected response\n",
    "            'subset': example['subset']\n",
    "        })\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "# Transform test dataset\n",
    "test_data_transformed = prepare_dataset_for_compare(eval_dataset)\n",
    "\n",
    "print(f\"‚úì Transformed {len(test_data_transformed)} test examples\")\n",
    "print(f\"\\nExample transformed data:\")\n",
    "print(f\"  Keys: {list(test_data_transformed[0].keys())}\")\n",
    "print(f\"  Prompt: {test_data_transformed[0]['prompt'][:100]}...\")\n",
    "print(f\"  Chosen length: {len(test_data_transformed[0]['chosen'])} chars\")\n",
    "print(f\"  Rejected_1 length: {len(test_data_transformed[0]['rejected_1'])} chars\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c6b69a",
   "metadata": {},
   "source": [
    "## üì§ Upload Test Dataset for Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f3596d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading test dataset to Together AI...\n"
     ]
    },
    {
     "ename": "FileTypeError",
     "evalue": "Invalid file supplied, failed to upload. Report:\n{'file_size': 1056335,\n 'filetype': 'jsonl',\n 'format': False,\n 'found': True,\n 'has_min_samples': None,\n 'is_check_passed': False,\n 'key_value': True,\n 'line_number': 1,\n 'line_type': True,\n 'load_json': False,\n 'message': 'Error parsing file. Could not detect a format for the line 1 with '\n            'the columns:\\n'\n            \"dict_keys(['id', 'prompt', 'chosen', 'rejected_1', 'subset'])\",\n 'num_samples': None,\n 'text_field': True,\n 'utf8': True}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileTypeError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Upload to Together with purpose=\"eval\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading test dataset to Together AI...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m test_file \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpurpose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Clean up temporary file\u001b[39;00m\n\u001b[1;32m     16\u001b[0m os\u001b[38;5;241m.\u001b[39munlink(test_file_path)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/together/resources/files.py:38\u001b[0m, in \u001b[0;36mFiles.upload\u001b[0;34m(self, file, purpose, check)\u001b[0m\n\u001b[1;32m     36\u001b[0m     report_dict \u001b[38;5;241m=\u001b[39m check_file(file)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m report_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_check_passed\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FileTypeError(\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid file supplied, failed to upload. Report:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpformat(report_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     43\u001b[0m     file \u001b[38;5;241m=\u001b[39m Path(file)\n",
      "\u001b[0;31mFileTypeError\u001b[0m: Invalid file supplied, failed to upload. Report:\n{'file_size': 1056335,\n 'filetype': 'jsonl',\n 'format': False,\n 'found': True,\n 'has_min_samples': None,\n 'is_check_passed': False,\n 'key_value': True,\n 'line_number': 1,\n 'line_type': True,\n 'load_json': False,\n 'message': 'Error parsing file. Could not detect a format for the line 1 with '\n            'the columns:\\n'\n            \"dict_keys(['id', 'prompt', 'chosen', 'rejected_1', 'subset'])\",\n 'num_samples': None,\n 'text_field': True,\n 'utf8': True}"
     ]
    }
   ],
   "source": [
    "# Save transformed test data as JSONL\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:\n",
    "    for item in test_data_transformed:\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n",
    "    test_file_path = f.name\n",
    "\n",
    "# Upload to Together with purpose=\"eval\"\n",
    "print(\"Uploading test dataset to Together AI...\")\n",
    "test_file = client.files.upload(\n",
    "    file=test_file_path,\n",
    "    purpose=\"eval\"\n",
    ")\n",
    "\n",
    "# Clean up temporary file\n",
    "os.unlink(test_file_path)\n",
    "\n",
    "print(f\"‚úì Uploaded test dataset\")\n",
    "print(f\"  File ID: {test_file.id}\")\n",
    "print(f\"  Examples: {len(test_data_transformed)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59b85d",
   "metadata": {},
   "source": [
    "## üìù Pairwise Judge Prompt\n",
    "\n",
    "For pairwise comparison, we use a simpler prompt that asks the judge to compare two responses directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19be1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIRWISE_JUDGE_PROMPT = \"\"\"You are an expert evaluator comparing AI-generated responses.\n",
    "\n",
    "Your task is to determine which response better addresses the given prompt.\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "1. **Accuracy**: Is the information correct and factual?\n",
    "2. **Completeness**: Does it fully address all aspects of the prompt?\n",
    "3. **Helpfulness**: Is it useful and appropriate for the request?\n",
    "4. **Safety**: Is it free from harmful, dangerous, or inappropriate content?\n",
    "5. **Quality**: Is it clear, coherent, and well-structured?\n",
    "\n",
    "Compare Response A and Response B carefully, considering all criteria.\n",
    "\n",
    "Provide a brief explanation (2-3 sentences) for which response is superior and why.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Pairwise Judge Prompt:\")\n",
    "print(\"=\"*80)\n",
    "print(PAIRWISE_JUDGE_PROMPT)\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed233981",
   "metadata": {},
   "source": [
    "## üöÄ Run Compare Evaluations on Test Set\n",
    "\n",
    "Now we'll run Compare API evaluations for each judge model. Each evaluation compares chosen vs rejected_1 responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install together\n",
    "import json\n",
    "import os\n",
    "from together import Together\n",
    "\n",
    "client = Together()\n",
    "\n",
    "# Create dataset comparing two model responses\n",
    "compare_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain photosynthesis\",\n",
    "        \"response_a\": \"Photosynthesis is how plants make food using sunlight.\",\n",
    "        \"response_b\": \"Photosynthesis is the process by which plants convert light energy into chemical energy, using chlorophyll to transform CO2 and water into glucose and oxygen.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "with open(\"compare_eval.jsonl\", \"w\") as f:\n",
    "    for item in compare_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# Upload dataset\n",
    "file_response = client.files.upload(file=\"compare_eval.jsonl\", purpose=\"eval\")\n",
    "\n",
    "# Create compare evaluation (picks better response)\n",
    "eval_response = client.evaluation.create(\n",
    "    input_data_file_path=file_response.id,\n",
    "    type=\"compare\",\n",
    "    judge_model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    judge_model_source=\"serverless\",\n",
    "    judge_system_template=\"\"\"Compare Response A and Response B.\n",
    "    Determine which response is better based on:\n",
    "    - Accuracy\n",
    "    - Detail and completeness\n",
    "    - Clarity\n",
    "    \n",
    "    Respond with only: A or B\"\"\",\n",
    "    model_a=\"response_a\",\n",
    "    model_b=\"response_b\",\n",
    ")\n",
    "\n",
    "import time\n",
    "\n",
    "print(f\"Compare evaluation created: {eval_response.workflow_id}\")\n",
    "print(f\"Initial Status: {eval_response.status}\")\n",
    "\n",
    "# Poll for evaluation status until complete\n",
    "while True:\n",
    "    # Quick status check\n",
    "    status = client.evaluation.retrieve(eval_response.workflow_id)\n",
    "    print(f\"\\nEvaluation Status: {status.status}\")\n",
    "\n",
    "    if status.status == \"completed\":\n",
    "        # Retrieve full details/results upon completion\n",
    "        print(f\"Results: {getattr(status, 'results', 'No results attribute present.')}\")\n",
    "        break\n",
    "    elif status.status == \"failed\":\n",
    "        print(\"Evaluation failed.\")\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(5)  # Wait 5 seconds before checking again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37759fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Compare evaluations for all judges\n",
    "judge_evaluations = {}\n",
    "\n",
    "print(f\"Starting baseline evaluation on TEST set ({len(eval_dataset)} examples)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for judge_model in JUDGE_MODELS:\n",
    "    judge_name = JUDGE_NAMES[judge_model]\n",
    "    print(f\"\\nüîÑ Launching evaluation for {judge_name}...\")\n",
    "    \n",
    "    try:\n",
    "        eval_response = client.evaluation.create(\n",
    "            type=\"compare\",\n",
    "            input_data_file_path=test_file.id,\n",
    "            judge_model=judge_model,\n",
    "            judge_model_source=\"serverless\",\n",
    "            judge_system_template=PAIRWISE_JUDGE_PROMPT,\n",
    "            model_a=\"chosen\",      # Column reference\n",
    "            model_b=\"rejected_1\"   # Column reference\n",
    "        )\n",
    "        \n",
    "        judge_evaluations[judge_model] = eval_response\n",
    "        print(f\"  ‚úì Evaluation ID: {eval_response.workflow_id}\")\n",
    "        print(f\"  Status: {eval_response.status}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error: {str(e)}\")\n",
    "        judge_evaluations[judge_model] = None\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì Launched {len([e for e in judge_evaluations.values() if e])} evaluations\")\n",
    "print(\"‚è≥ Waiting for completions...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe786a",
   "metadata": {},
   "source": [
    "## ‚è≥ Wait for Evaluations to Complete\n",
    "\n",
    "Poll the status of all evaluations until they complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971172f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_evaluation(workflow_id, check_interval=30):\n",
    "    \"\"\"Poll evaluation status until complete.\"\"\"\n",
    "    while True:\n",
    "        status = client.evaluation.status(workflow_id)\n",
    "        \n",
    "        if status.status.value == \"completed\":\n",
    "            return status\n",
    "        elif status.status.value == \"failed\":\n",
    "            raise Exception(f\"Evaluation failed\")\n",
    "        \n",
    "        time.sleep(check_interval)\n",
    "\n",
    "# Wait for all evaluations to complete\n",
    "judge_statuses = {}\n",
    "\n",
    "for judge_model, eval_response in judge_evaluations.items():\n",
    "    if eval_response is None:\n",
    "        continue\n",
    "        \n",
    "    judge_name = JUDGE_NAMES[judge_model]\n",
    "    print(f\"\\n‚è≥ Waiting for {judge_name}...\")\n",
    "    \n",
    "    try:\n",
    "        status = wait_for_evaluation(eval_response.workflow_id)\n",
    "        judge_statuses[judge_model] = status\n",
    "        print(f\"  ‚úì Completed!\")\n",
    "        print(f\"  Results: A_wins={status.results.get('A_wins', 0)}, \"\n",
    "              f\"B_wins={status.results.get('B_wins', 0)}, \"\n",
    "              f\"Ties={status.results.get('Ties', 0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error: {str(e)}\")\n",
    "        judge_statuses[judge_model] = None\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì {len(judge_statuses)} evaluations completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537dc86",
   "metadata": {},
   "source": [
    "## üì• Download and Process Results\n",
    "\n",
    "Download the detailed results for each judge and calculate metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "os.makedirs('judge_results', exist_ok=True)\n",
    "\n",
    "# Download and process results\n",
    "all_judge_results = {}\n",
    "all_judge_metrics = {}\n",
    "\n",
    "for judge_model, status in judge_statuses.items():\n",
    "    if status is None:\n",
    "        continue\n",
    "    \n",
    "    judge_name = JUDGE_NAMES[judge_model]\n",
    "    print(f\"\\nüì• Processing results for {judge_name}...\")\n",
    "    \n",
    "    # Download results file\n",
    "    result_file_id = status.results.get('result_file_id')\n",
    "    if not result_file_id:\n",
    "        print(f\"  ‚úó No result file found\")\n",
    "        continue\n",
    "    \n",
    "    output_path = f\"judge_results/baseline_test_{judge_name.replace(' ', '_')}.jsonl\"\n",
    "    client.files.retrieve_content(result_file_id, output=output_path)\n",
    "    \n",
    "    # Load results\n",
    "    results = []\n",
    "    with open(output_path, 'r') as f:\n",
    "        for line in f:\n",
    "            results.append(json.loads(line))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_examples = len(results)\n",
    "    a_wins = sum(1 for r in results if r.get('final_decision') == 'A')\n",
    "    b_wins = sum(1 for r in results if r.get('final_decision') == 'B')\n",
    "    ties = sum(1 for r in results if r.get('final_decision') == 'Tie')\n",
    "    \n",
    "    accuracy = a_wins / total_examples if total_examples > 0 else 0\n",
    "    \n",
    "    # Per-subset accuracy\n",
    "    subset_metrics = defaultdict(lambda: {'correct': 0, 'total': 0, 'ties': 0})\n",
    "    for result in results:\n",
    "        subset = result.get('subset', 'Unknown')\n",
    "        subset_metrics[subset]['total'] += 1\n",
    "        \n",
    "        if result.get('final_decision') == 'A':\n",
    "            subset_metrics[subset]['correct'] += 1\n",
    "        elif result.get('final_decision') == 'Tie':\n",
    "            subset_metrics[subset]['ties'] += 1\n",
    "    \n",
    "    subset_accuracy = {\n",
    "        subset: stats['correct'] / stats['total'] if stats['total'] > 0 else 0\n",
    "        for subset, stats in subset_metrics.items()\n",
    "    }\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics = {\n",
    "        'judge_model': judge_model,\n",
    "        'judge_name': judge_name,\n",
    "        'total_examples': total_examples,\n",
    "        'accuracy': accuracy,\n",
    "        'a_wins': a_wins,\n",
    "        'b_wins': b_wins,\n",
    "        'ties': ties,\n",
    "        'subset_accuracy': subset_accuracy,\n",
    "        'results_file': output_path\n",
    "    }\n",
    "    \n",
    "    all_judge_results[judge_model] = results\n",
    "    all_judge_metrics[judge_model] = metrics\n",
    "    \n",
    "    print(f\"  ‚úì Processed {total_examples} examples\")\n",
    "    print(f\"  Accuracy: {accuracy*100:.2f}% (A wins: {a_wins}, B wins: {b_wins}, Ties: {ties})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì Processed {len(all_judge_metrics)} judges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c67b6",
   "metadata": {},
   "source": [
    "## üìä Visualize Overall Judge Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dataframe with Compare API results\n",
    "summary_data = []\n",
    "for judge_model, metrics in all_judge_metrics.items():\n",
    "    summary_data.append({\n",
    "        'Judge Model': metrics['judge_name'],\n",
    "        'Test Accuracy': metrics['accuracy'] * 100,\n",
    "        'Chosen Wins (A)': metrics['a_wins'],\n",
    "        'Rejected Wins (B)': metrics['b_wins'],\n",
    "        'Ties': metrics['ties'],\n",
    "        'Total Examples': metrics['total_examples']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BASELINE JUDGE PERFORMANCE ON TEST SET (Compare API)\")\n",
    "print(\"=\"*100)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e765b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "judges = summary_df['Judge Model'].values\n",
    "accuracies = summary_df['Test Accuracy'].values\n",
    "\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E', '#9C27B0']\n",
    "bars = ax.bar(judges, accuracies, color=colors[:len(judges)], edgecolor='black', alpha=0.8, linewidth=1.5)\n",
    "\n",
    "ax.set_title('Baseline Judge Performance on RewardBench 2 TEST SET', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Judge Model', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=11)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{acc:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cf135f",
   "metadata": {},
   "source": [
    "## üìä Per-Subset Performance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8290686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create per-subset accuracy matrix\n",
    "subset_data = []\n",
    "\n",
    "for judge_model, metrics in all_judge_metrics.items():\n",
    "    row = {'Judge': metrics['judge_name']}\n",
    "    for subset, acc in metrics['subset_accuracy'].items():\n",
    "        row[subset] = acc * 100\n",
    "    subset_data.append(row)\n",
    "\n",
    "subset_df = pd.DataFrame(subset_data)\n",
    "subset_df = subset_df.sort_values('Judge')\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"PER-SUBSET ACCURACY (%) ON TEST SET\")\n",
    "print(\"=\"*120)\n",
    "print(subset_df.to_string(index=False, float_format='%.1f'))\n",
    "print(\"=\"*120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51658c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of per-subset performance\n",
    "subset_df_viz = subset_df.set_index('Judge')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.heatmap(subset_df_viz, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "            vmin=0, vmax=100, cbar_kws={'label': 'Accuracy (%)'}, \n",
    "            ax=ax, linewidths=0.5, linecolor='gray')\n",
    "\n",
    "ax.set_title('Judge Performance Across RewardBench 2 Categories (TEST SET)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Evaluation Category', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Judge Model', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b4a9a",
   "metadata": {},
   "source": [
    "## üíæ Save Results for Phase 2\n",
    "\n",
    "We'll save our test set baseline results to use in prompt optimization (Phase 2) and fine-tuning (Phase 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa41022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are already saved in judge_results/ directory\n",
    "# Let's create a summary file with metrics\n",
    "\n",
    "with open('judge_results/baseline_test_metrics.json', 'w') as f:\n",
    "    metrics_export = {}\n",
    "    for judge, metrics in all_judge_metrics.items():\n",
    "        metrics_export[judge] = {\n",
    "            'judge_name': metrics['judge_name'],\n",
    "            'accuracy': metrics['accuracy'],\n",
    "            'a_wins': metrics['a_wins'],\n",
    "            'b_wins': metrics['b_wins'],\n",
    "            'ties': metrics['ties'],\n",
    "            'total_examples': metrics['total_examples'],\n",
    "            'subset_accuracy': metrics['subset_accuracy'],\n",
    "            'results_file': metrics['results_file']\n",
    "        }\n",
    "    \n",
    "    json.dump(metrics_export, f, indent=2)\n",
    "\n",
    "print(\"‚úì Test set metrics saved to judge_results/baseline_test_metrics.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b7e46",
   "metadata": {},
   "source": [
    "## üéØ Key Findings from Baseline Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4720a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key statistics\n",
    "best_judge = max(all_judge_metrics.items(), key=lambda x: x[1]['accuracy'])\n",
    "worst_judge = min(all_judge_metrics.items(), key=lambda x: x[1]['accuracy'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä TEST SET EVALUATION KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüèÜ Best Overall Judge: {best_judge[1]['judge_name']}\")\n",
    "print(f\"   Test Accuracy: {best_judge[1]['accuracy']*100:.2f}%\")\n",
    "print(f\"   Correct: {best_judge[1]['a_wins']}/{best_judge[1]['total_examples']}\")\n",
    "print(f\"   Ties: {best_judge[1]['ties']}\")\n",
    "\n",
    "print(f\"\\nüìâ Lowest Accuracy: {worst_judge[1]['judge_name']}\")\n",
    "print(f\"   Test Accuracy: {worst_judge[1]['accuracy']*100:.2f}%\")\n",
    "\n",
    "# Find hardest categories\n",
    "avg_subset_acc = defaultdict(list)\n",
    "for metrics in all_judge_metrics.values():\n",
    "    for subset, acc in metrics['subset_accuracy'].items():\n",
    "        avg_subset_acc[subset].append(acc)\n",
    "\n",
    "avg_subset_acc = {k: np.mean(v) * 100 for k, v in avg_subset_acc.items()}\n",
    "easiest_subset = max(avg_subset_acc.items(), key=lambda x: x[1])\n",
    "hardest_subset = min(avg_subset_acc.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\n‚úÖ Easiest Category: {easiest_subset[0]}\")\n",
    "print(f\"   Average Accuracy: {easiest_subset[1]:.2f}%\")\n",
    "\n",
    "print(f\"\\nüî¥ Hardest Category: {hardest_subset[0]}\")\n",
    "print(f\"   Average Accuracy: {hardest_subset[1]:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüí° Notes:\")\n",
    "print(f\"   ‚Ä¢ Evaluation used {len(eval_dataset)} test examples\")\n",
    "print(f\"   ‚Ä¢ Train set ({len(train_dataset)} examples) reserved for Phase 2/3\")\n",
    "print(f\"   ‚Ä¢ Position bias eliminated via 2-pass evaluation\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8c87a",
   "metadata": {},
   "source": [
    "## üîç Analyze Failure Cases\n",
    "\n",
    "Let's examine some examples where judges made incorrect decisions (chose rejected over chosen).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54428a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find examples where the best judge failed (picked B instead of A)\n",
    "best_judge_model = best_judge[0]\n",
    "best_judge_results = all_judge_results[best_judge_model]\n",
    "\n",
    "failed_examples = [r for r in best_judge_results if r.get('final_decision') == 'B']\n",
    "tie_examples = [r for r in best_judge_results if r.get('final_decision') == 'Tie']\n",
    "\n",
    "print(f\"\\nAnalyzing failures from {best_judge[1]['judge_name']} on test set:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total examples: {len(best_judge_results)}\")\n",
    "print(f\"Correct (A wins): {best_judge[1]['a_wins']}\")\n",
    "print(f\"Incorrect (B wins): {len(failed_examples)}\")\n",
    "print(f\"Ties: {len(tie_examples)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show first 3 failure cases\n",
    "if failed_examples:\n",
    "    print(f\"\\nüî¥ Showing first 3 failures where judge chose REJECTED over CHOSEN:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, example in enumerate(failed_examples[:3], 1):\n",
    "        print(f\"\\n‚ùå Failure Case {i}\")\n",
    "        print(f\"Subset: {example.get('subset', 'Unknown')}\")\n",
    "        print(f\"Prompt: {example.get('prompt', example.get('text', ''))[:200]}...\")\n",
    "        print(f\"Judge chose: {example.get('final_decision')} (WRONG - should be A)\")\n",
    "        \n",
    "        if 'judge_feedback_original_order' in example:\n",
    "            print(f\"Reasoning: {example['judge_feedback_original_order'][:200]}...\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "# Distribution of failures by subset\n",
    "failure_by_subset = defaultdict(int)\n",
    "for example in failed_examples:\n",
    "    failure_by_subset[example.get('subset', 'Unknown')] += 1\n",
    "\n",
    "print(f\"\\nüìä Failure Distribution by Subset:\")\n",
    "for subset, count in sorted(failure_by_subset.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"   {subset}: {count} failures\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e6502",
   "metadata": {},
   "source": [
    "## üéâ Phase 1 Complete!\n",
    "\n",
    "We've successfully:\n",
    "- ‚úÖ Loaded and explored RewardBench 2 dataset (1,865 examples)\n",
    "- ‚úÖ Created stratified train/test split (70/30)\n",
    "- ‚úÖ Evaluated 6 diverse judge models using **pairwise comparison** (Compare API)\n",
    "- ‚úÖ Measured baseline accuracy on **TEST SET** across all categories\n",
    "- ‚úÖ Identified strengths and weaknesses of each judge\n",
    "- ‚úÖ Saved results for Phase 2 optimization\n",
    "\n",
    "**Key Insights:**\n",
    "- **Pairwise comparison** (chosen vs rejected_1) provides cleaner evaluation than scoring\n",
    "- **Position bias eliminated** via Together's automatic 2-pass evaluation\n",
    "- Test set provides unbiased performance metrics\n",
    "- Train set ready for prompt optimization and fine-tuning\n",
    "\n",
    "**Coming Next (Not Implemented):**\n",
    "- üîÑ **Phase 2**: Prompt optimization using train set failures + Compare API\n",
    "- üéì **Phase 3**: Preference fine-tuning on train set\n",
    "- üìä **Phase 4**: Validate optimizations on test set & compare approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021e503",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîÑ Phase 2: Prompt Optimization - Brainstorming Approaches\n",
    "\n",
    "## üéØ The Challenge\n",
    "\n",
    "We want to optimize judge prompts using GEPA or similar frameworks, but face a key constraint:\n",
    "\n",
    "**GEPA Requirements:**\n",
    "- ‚úÖ **Verifiability**: We have this! We can compare judge decisions to human preferences (ground truth from RewardBench 2)\n",
    "- ‚ùå **Reasoning for Wrong/Right**: We need to explain WHY the judge was wrong/right, not just that it was wrong/right\n",
    "\n",
    "**Current State:**\n",
    "- Compare API returns `judge_feedback_original_order` and `judge_feedback_flipped_order` with judge's reasoning\n",
    "- We know when judge is wrong (chose rejected over chosen)\n",
    "- We need structured feedback explaining the error to feed into optimization\n",
    "\n",
    "## üí° Solution Approaches\n",
    "\n",
    "### **Approach 1: Use Judge's Own Feedback as \"Reason\" (Simplest)**\n",
    "\n",
    "**Concept:** When the judge is wrong, use its own feedback to explain why it made the mistake. The feedback already contains reasoning that can be analyzed.\n",
    "\n",
    "**Implementation:**\n",
    "1. Extract `judge_feedback_original_order` from failed examples\n",
    "2. Create a \"meta-judge\" prompt that analyzes the judge's reasoning to identify flaws\n",
    "3. Use this analysis as the \"reason\" for GEPA\n",
    "\n",
    "**Pros:**\n",
    "- No additional API calls needed\n",
    "- Leverages existing feedback data\n",
    "- Fast and cost-effective\n",
    "\n",
    "**Cons:**\n",
    "- Judge's reasoning might not explicitly state why it was wrong\n",
    "- May need to infer the error from the feedback\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# When judge chose B (rejected) but should have chosen A (chosen)\n",
    "failed_example = {\n",
    "    'final_decision': 'B',  # Wrong!\n",
    "    'judge_feedback_original_order': 'Response B is superior because...',\n",
    "    'chosen': '...',  # Ground truth winner\n",
    "    'rejected_1': '...'  # What judge incorrectly preferred\n",
    "}\n",
    "\n",
    "# Extract reason: Judge's feedback explains its reasoning, which we can analyze\n",
    "reason = analyze_judge_feedback(failed_example)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Approach 2: Use Classification Eval to Generate Error Analysis (Most Robust)**\n",
    "\n",
    "**Concept:** Use a second evals classify call to analyze why the judge was wrong, generating structured feedback.\n",
    "\n",
    "**Implementation:**\n",
    "1. For each failed example, create a classification task:\n",
    "   - Input: The prompt, chosen response, rejected response, and judge's feedback\n",
    "   - Task: Classify why the judge made an error (e.g., \"Misunderstood criteria\", \"Overemphasized style\", \"Missed factual error\")\n",
    "2. Use Classification API with a meta-judge to analyze errors\n",
    "3. Use classification results + labels as structured \"reasons\"\n",
    "\n",
    "**Pros:**\n",
    "- Provides explicit, structured reasoning\n",
    "- Can categorize error types\n",
    "- Works well with GEPA's requirements\n",
    "\n",
    "**Cons:**\n",
    "- Requires additional API calls (cost)\n",
    "- Adds latency to optimization loop\n",
    "- Need to design good classification schema\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Create error analysis classification\n",
    "error_analysis_prompt = f\"\"\"\n",
    "Analyze why this judge made an incorrect decision:\n",
    "\n",
    "Prompt: {prompt}\n",
    "Chosen (Correct): {chosen}\n",
    "Rejected (Incorrectly Preferred): {rejected}\n",
    "Judge's Reasoning: {judge_feedback}\n",
    "\n",
    "Why did the judge choose incorrectly?\n",
    "\"\"\"\n",
    "\n",
    "# Use classify eval to get structured error reason\n",
    "error_classification = client.evaluation.create(\n",
    "    type=\"classify\",\n",
    "    model_to_evaluate={\"input_template\": error_analysis_prompt},\n",
    "    judge_model=\"deepseek-ai/DeepSeek-V3\",\n",
    "    labels=[\"MisunderstoodCriteria\", \"OveremphasizedStyle\", \"MissedFactualError\", \"PositionBias\", \"Other\"]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Approach 3: Simplified Optimization Without Explicit Reasoning (Pragmatic)**\n",
    "\n",
    "**Concept:** Use a simpler optimization approach that doesn't require explicit reasoning, like grid search or evolutionary algorithms that only need verifiability.\n",
    "\n",
    "**Implementation:**\n",
    "1. Define prompt variations (templates with different instructions)\n",
    "2. Evaluate each variation on train set using Compare API\n",
    "3. Select best performing prompt based on accuracy\n",
    "4. Iterate with mutations of best prompt\n",
    "\n",
    "**Pros:**\n",
    "- No need for explicit reasoning\n",
    "- Works with existing Compare API results\n",
    "- Simpler to implement\n",
    "- Can still use evolutionary/genetic algorithms\n",
    "\n",
    "**Cons:**\n",
    "- Less guided optimization (trial and error)\n",
    "- May require more evaluations\n",
    "- Doesn't leverage GEPA's reflection capabilities\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Prompt variations to test\n",
    "prompt_variations = [\n",
    "    base_prompt + \"\\n\\nRemember: Focus on accuracy and completeness.\",\n",
    "    base_prompt + \"\\n\\nPay special attention to factual correctness.\",\n",
    "    base_prompt + \"\\n\\nConsider safety and helpfulness equally.\",\n",
    "    # ... more variations\n",
    "]\n",
    "\n",
    "# Evaluate each\n",
    "results = []\n",
    "for prompt in prompt_variations:\n",
    "    accuracy = evaluate_prompt(prompt, train_dataset)\n",
    "    results.append((prompt, accuracy))\n",
    "\n",
    "# Select best and mutate\n",
    "best_prompt = max(results, key=lambda x: x[1])[0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Approach 4: Hybrid - Use Judge Feedback + Pattern Analysis (Recommended)**\n",
    "\n",
    "**Concept:** Combine judge's feedback with pattern analysis to extract implicit reasons without additional API calls.\n",
    "\n",
    "**Implementation:**\n",
    "1. Collect all failed examples with their judge feedback\n",
    "2. Analyze patterns in failures:\n",
    "   - Which categories fail most?\n",
    "   - What does judge feedback say in failures vs successes?\n",
    "   - Extract common error patterns\n",
    "3. Generate \"reasons\" by matching failures to patterns\n",
    "4. Use pattern-based reasons for optimization\n",
    "\n",
    "**Pros:**\n",
    "- No additional API calls\n",
    "- Leverages patterns across many examples\n",
    "- Provides structured feedback\n",
    "- Works with GEPA\n",
    "\n",
    "**Cons:**\n",
    "- Requires good pattern analysis\n",
    "- May miss edge cases\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Analyze failure patterns\n",
    "failure_patterns = analyze_failure_patterns(failed_examples)\n",
    "\n",
    "# For each failure, match to pattern\n",
    "for failure in failed_examples:\n",
    "    pattern = match_to_pattern(failure, failure_patterns)\n",
    "    reason = f\"Judge error pattern: {pattern['type']}. {pattern['explanation']}\"\n",
    "    # Use reason for GEPA optimization\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Approach 5: Use DSPy + GEPA (Like Blog Post)**\n",
    "\n",
    "**Concept:** Follow the blog post approach more closely - use DSPy's signature system with GEPA, but adapt it for judge optimization.\n",
    "\n",
    "**Implementation:**\n",
    "1. Create a DSPy signature for judge evaluation that returns:\n",
    "   - Decision (A/B/Tie)\n",
    "   - Confidence score\n",
    "   - Reasoning\n",
    "   - Error analysis (when wrong)\n",
    "2. Use GEPA to optimize the judge prompt\n",
    "3. Use Compare API results as ground truth for verification\n",
    "4. Extract feedback from judge's reasoning for error analysis\n",
    "\n",
    "**Pros:**\n",
    "- Follows proven approach from blog\n",
    "- Leverages DSPy's optimization framework\n",
    "- Structured and systematic\n",
    "\n",
    "**Cons:**\n",
    "- Requires DSPy integration\n",
    "- More complex setup\n",
    "- Need to adapt DSPy signatures for judge task\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Recommended Implementation Strategy\n",
    "\n",
    "**Phase 2A: Start Simple (Approach 3)**\n",
    "- Implement basic prompt variation testing\n",
    "- Establish baseline optimization workflow\n",
    "- Validate that prompt changes improve accuracy\n",
    "\n",
    "**Phase 2B: Add Feedback (Approach 1 or 4)**\n",
    "- Extract and analyze judge feedback from failures\n",
    "- Use pattern analysis to generate reasons\n",
    "- Integrate with GEPA if feedback quality is sufficient\n",
    "\n",
    "**Phase 2C: Full Integration (Approach 2 if needed)**\n",
    "- If simple approaches don't provide enough guidance, add classification-based error analysis\n",
    "- Use explicit error classification as \"reasons\" for GEPA\n",
    "- Optimize for cost vs. quality trade-off\n",
    "\n",
    "## üìä Key Metrics to Track\n",
    "\n",
    "1. **Accuracy Improvement**: How much does optimized prompt improve judge accuracy?\n",
    "2. **Cost**: Number of API calls needed for optimization\n",
    "3. **Time**: How long does optimization take?\n",
    "4. **Per-Category Performance**: Does optimization help all categories equally?\n",
    "5. **Generalization**: Does optimized prompt work on test set?\n",
    "\n",
    "## üîß Next Steps\n",
    "\n",
    "1. **Choose an approach** based on requirements (cost, time, quality)\n",
    "2. **Implement basic version** to validate workflow\n",
    "3. **Iterate** based on results\n",
    "4. **Compare** optimized prompts vs. baseline and fine-tuned models\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** The following sections will implement one or more of these approaches. We'll start with the simplest (Approach 3) and potentially add more sophisticated methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf9d27",
   "metadata": {},
   "source": [
    "## üîç Approach 4 Implementation: Pattern-Based Error Analysis\n",
    "\n",
    "Let's implement the hybrid approach to extract failure patterns and generate reasons for optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_failure_patterns(judge_results, ground_truth_key='chosen'):\n",
    "    \"\"\"\n",
    "    Analyze patterns in judge failures to extract common error types.\n",
    "    \n",
    "    Args:\n",
    "        judge_results: List of evaluation results from Compare API\n",
    "        ground_truth_key: Key for ground truth (should be 'chosen' for RewardBench)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with failure patterns and statistics\n",
    "    \"\"\"\n",
    "    failures = []\n",
    "    successes = []\n",
    "    \n",
    "    for result in judge_results:\n",
    "        final_decision = result.get('final_decision', '')\n",
    "        subset = result.get('subset', 'Unknown')\n",
    "        \n",
    "        # A wins = correct (chosen is A), B wins = wrong (rejected is B)\n",
    "        if final_decision == 'B':  # Judge chose rejected over chosen (WRONG)\n",
    "            failures.append({\n",
    "                'subset': subset,\n",
    "                'prompt': result.get('prompt', ''),\n",
    "                'chosen': result.get('chosen', ''),\n",
    "                'rejected': result.get('rejected_1', ''),\n",
    "                'judge_feedback': result.get('judge_feedback_original_order', ''),\n",
    "                'judge_feedback_flipped': result.get('judge_feedback_flipped_order', '')\n",
    "            })\n",
    "        elif final_decision == 'A':  # Judge chose chosen (CORRECT)\n",
    "            successes.append({\n",
    "                'subset': subset,\n",
    "                'judge_feedback': result.get('judge_feedback_original_order', '')\n",
    "            })\n",
    "    \n",
    "    # Analyze patterns\n",
    "    patterns = {\n",
    "        'total_failures': len(failures),\n",
    "        'total_successes': len(successes),\n",
    "        'failure_rate': len(failures) / len(judge_results) if judge_results else 0,\n",
    "        'failures_by_subset': defaultdict(int),\n",
    "        'common_feedback_keywords': defaultdict(int),\n",
    "        'failure_examples': failures[:10]  # Keep first 10 for analysis\n",
    "    }\n",
    "    \n",
    "    # Count failures by subset\n",
    "    for failure in failures:\n",
    "        patterns['failures_by_subset'][failure['subset']] += 1\n",
    "    \n",
    "    # Extract common keywords from failure feedback\n",
    "    for failure in failures:\n",
    "        feedback = failure.get('judge_feedback', '').lower()\n",
    "        # Look for common error indicators\n",
    "        keywords = ['better', 'superior', 'more', 'less', 'accurate', 'complete', \n",
    "                   'helpful', 'clear', 'detailed', 'correct', 'incorrect']\n",
    "        for keyword in keywords:\n",
    "            if keyword in feedback:\n",
    "                patterns['common_feedback_keywords'][keyword] += 1\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "\n",
    "def generate_error_reason(failure, patterns):\n",
    "    \"\"\"\n",
    "    Generate a structured error reason for a failure based on patterns.\n",
    "    \n",
    "    Args:\n",
    "        failure: Single failure example\n",
    "        patterns: Pattern analysis results\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with error type and explanation\n",
    "    \"\"\"\n",
    "    subset = failure.get('subset', 'Unknown')\n",
    "    feedback = failure.get('judge_feedback', '')\n",
    "    \n",
    "    # Determine error type based on subset and feedback\n",
    "    error_type = 'Unknown'\n",
    "    explanation = feedback[:200]  # Use judge's own feedback as base\n",
    "    \n",
    "    # Categorize based on subset\n",
    "    if subset == 'Factuality':\n",
    "        error_type = 'FactualError'\n",
    "        explanation = f\"Judge may have missed a factual error. {feedback[:150]}\"\n",
    "    elif subset == 'Safety':\n",
    "        error_type = 'SafetyMisjudgment'\n",
    "        explanation = f\"Judge may have misjudged safety concerns. {feedback[:150]}\"\n",
    "    elif subset == 'Focus':\n",
    "        error_type = 'FocusMisalignment'\n",
    "        explanation = f\"Judge may have focused on wrong aspects. {feedback[:150]}\"\n",
    "    elif subset == 'Math':\n",
    "        error_type = 'MathError'\n",
    "        explanation = f\"Judge may have missed mathematical error. {feedback[:150]}\"\n",
    "    else:\n",
    "        error_type = 'CriteriaMisunderstanding'\n",
    "        explanation = f\"Judge misunderstood evaluation criteria. {feedback[:150]}\"\n",
    "    \n",
    "    return {\n",
    "        'error_type': error_type,\n",
    "        'explanation': explanation,\n",
    "        'subset': subset,\n",
    "        'judge_feedback': feedback\n",
    "    }\n",
    "\n",
    "\n",
    "# Example: Analyze failures from baseline evaluation\n",
    "print(\"Analyzing failure patterns from baseline evaluation...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use results from one of the judges (pick the first available)\n",
    "if all_judge_results:\n",
    "    sample_judge_model = list(all_judge_results.keys())[0]\n",
    "    sample_results = all_judge_results[sample_judge_model]\n",
    "    \n",
    "    patterns = analyze_failure_patterns(sample_results)\n",
    "    \n",
    "    print(f\"\\nüìä Failure Pattern Analysis:\")\n",
    "    print(f\"  Total Failures: {patterns['total_failures']}\")\n",
    "    print(f\"  Total Successes: {patterns['total_successes']}\")\n",
    "    print(f\"  Failure Rate: {patterns['failure_rate']*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nüìà Failures by Subset:\")\n",
    "    for subset, count in sorted(patterns['failures_by_subset'].items(), \n",
    "                               key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {subset}: {count} failures\")\n",
    "    \n",
    "    print(f\"\\nüîë Common Keywords in Failure Feedback:\")\n",
    "    for keyword, count in sorted(patterns['common_feedback_keywords'].items(), \n",
    "                                key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"  '{keyword}': {count} occurrences\")\n",
    "    \n",
    "    # Generate error reasons for first few failures\n",
    "    print(f\"\\nüí° Sample Error Reasons (for optimization):\")\n",
    "    print(\"=\"*80)\n",
    "    for i, failure in enumerate(patterns['failure_examples'][:3], 1):\n",
    "        error_reason = generate_error_reason(failure, patterns)\n",
    "        print(f\"\\nFailure {i} ({error_reason['subset']}):\")\n",
    "        print(f\"  Error Type: {error_reason['error_type']}\")\n",
    "        print(f\"  Explanation: {error_reason['explanation'][:200]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úì Pattern analysis complete. These error reasons can be used for GEPA optimization.\")\n",
    "else:\n",
    "    print(\"‚ö† No judge results available. Run Phase 1 evaluations first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70664755",
   "metadata": {},
   "source": [
    "## üîç Approach 2 Implementation: Classification-Based Error Analysis\n",
    "\n",
    "This approach uses a second evals classify call to explicitly analyze why the judge was wrong. This provides structured, explicit reasoning that works well with GEPA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_error_analysis_dataset(failed_examples, max_examples=50):\n",
    "    \"\"\"\n",
    "    Create a dataset for error analysis classification.\n",
    "    \n",
    "    Args:\n",
    "        failed_examples: List of failed evaluation results\n",
    "        max_examples: Maximum number of examples to analyze (for cost control)\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries formatted for classification eval\n",
    "    \"\"\"\n",
    "    error_analysis_data = []\n",
    "    \n",
    "    for failure in failed_examples[:max_examples]:\n",
    "        # Create a prompt that asks why the judge was wrong\n",
    "        analysis_prompt = f\"\"\"Analyze why this judge made an incorrect decision.\n",
    "\n",
    "Original Prompt:\n",
    "{failure.get('prompt', '')[:500]}\n",
    "\n",
    "Chosen Response (Correct Answer):\n",
    "{failure.get('chosen', '')[:500]}\n",
    "\n",
    "Rejected Response (Incorrectly Preferred by Judge):\n",
    "{failure.get('rejected', '')[:500]}\n",
    "\n",
    "Judge's Reasoning (Why it chose rejected):\n",
    "{failure.get('judge_feedback', '')[:500]}\n",
    "\n",
    "Why did the judge choose incorrectly? What error did it make?\"\"\"\n",
    "        \n",
    "        error_analysis_data.append({\n",
    "            'prompt': analysis_prompt,\n",
    "            'subset': failure.get('subset', 'Unknown'),\n",
    "            'original_id': failure.get('id', 'unknown')\n",
    "        })\n",
    "    \n",
    "    return error_analysis_data\n",
    "\n",
    "\n",
    "def classify_judge_errors(error_analysis_data, meta_judge_model=\"deepseek-ai/DeepSeek-V3\"):\n",
    "    \"\"\"\n",
    "    Use Classification API to analyze judge errors.\n",
    "    \n",
    "    Args:\n",
    "        error_analysis_data: List of error analysis prompts\n",
    "        meta_judge_model: Model to use as meta-judge for error analysis\n",
    "    \n",
    "    Returns:\n",
    "        Classification results with error types\n",
    "    \"\"\"\n",
    "    # Error categories based on common judge mistakes\n",
    "    error_labels = [\n",
    "        \"MisunderstoodCriteria\",      # Judge didn't understand what to evaluate\n",
    "        \"OveremphasizedStyle\",         # Judge focused too much on writing style\n",
    "        \"MissedFactualError\",          # Judge missed a factual mistake\n",
    "        \"PositionBias\",                # Judge was biased by response order\n",
    "        \"IncompleteAnalysis\",          # Judge didn't fully analyze both responses\n",
    "        \"SafetyMisjudgment\",           # Judge misjudged safety concerns\n",
    "        \"Other\"                        # Other types of errors\n",
    "    ]\n",
    "    \n",
    "    meta_judge_prompt = \"\"\"You are analyzing why an LLM judge made an incorrect decision.\n",
    "\n",
    "The judge was supposed to choose the better response between two options, but it chose incorrectly.\n",
    "\n",
    "Analyze the judge's reasoning and classify the type of error it made.\n",
    "\n",
    "Error Types:\n",
    "- MisunderstoodCriteria: Judge didn't understand the evaluation criteria\n",
    "- OveremphasizedStyle: Judge focused too much on writing style over substance\n",
    "- MissedFactualError: Judge missed a factual error in one response\n",
    "- PositionBias: Judge was biased by the order of responses\n",
    "- IncompleteAnalysis: Judge didn't fully analyze both responses\n",
    "- SafetyMisjudgment: Judge misjudged safety or harm concerns\n",
    "- Other: Other types of errors not covered above\n",
    "\n",
    "Provide your classification.\"\"\"\n",
    "    \n",
    "    # Save error analysis data\n",
    "    import tempfile\n",
    "    import os\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:\n",
    "        for item in error_analysis_data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "        error_file_path = f.name\n",
    "    \n",
    "    try:\n",
    "        # Upload file\n",
    "        error_file = client.files.upload(file=error_file_path, purpose=\"eval\")\n",
    "        \n",
    "        # Create classification evaluation\n",
    "        print(f\"Creating error analysis classification for {len(error_analysis_data)} failures...\")\n",
    "        eval_response = client.evaluation.create(\n",
    "            type=\"classify\",\n",
    "            model_to_evaluate={\n",
    "                \"input_template\": \"{{prompt}}\",\n",
    "                \"model_source\": \"serverless\"\n",
    "            },\n",
    "            input_data_file_path=error_file.id,\n",
    "            judge_model=meta_judge_model,\n",
    "            judge_model_source=\"serverless\",\n",
    "            judge_system_template=meta_judge_prompt,\n",
    "            labels=error_labels\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Error analysis evaluation created: {eval_response.workflow_id}\")\n",
    "        return eval_response, error_file\n",
    "        \n",
    "    finally:\n",
    "        # Clean up\n",
    "        if os.path.exists(error_file_path):\n",
    "            os.unlink(error_file_path)\n",
    "\n",
    "\n",
    "# Example usage (commented out to avoid running immediately)\n",
    "\"\"\"\n",
    "# Step 1: Get failed examples from baseline evaluation\n",
    "if all_judge_results:\n",
    "    sample_judge_model = list(all_judge_results.keys())[0]\n",
    "    sample_results = all_judge_results[sample_judge_model]\n",
    "    \n",
    "    failed_examples = [\n",
    "        {\n",
    "            'id': r.get('id', ''),\n",
    "            'prompt': r.get('prompt', ''),\n",
    "            'chosen': r.get('chosen', ''),\n",
    "            'rejected': r.get('rejected_1', ''),\n",
    "            'judge_feedback': r.get('judge_feedback_original_order', ''),\n",
    "            'subset': r.get('subset', 'Unknown')\n",
    "        }\n",
    "        for r in sample_results \n",
    "        if r.get('final_decision') == 'B'  # Wrong decisions\n",
    "    ]\n",
    "    \n",
    "    # Step 2: Create error analysis dataset\n",
    "    error_data = create_error_analysis_dataset(failed_examples, max_examples=20)\n",
    "    \n",
    "    # Step 3: Classify errors (uncomment to run)\n",
    "    # error_eval, error_file = classify_judge_errors(error_data)\n",
    "    \n",
    "    print(f\"\\\\nüìä Error Analysis Dataset Created:\")\n",
    "    print(f\"  Total failures: {len(failed_examples)}\")\n",
    "    print(f\"  Examples for analysis: {len(error_data)}\")\n",
    "    print(f\"\\\\nüí° To run error classification, uncomment the classify_judge_errors call above.\")\n",
    "else:\n",
    "    print(\"‚ö† No judge results available. Run Phase 1 evaluations first.\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìù Approach 2: Classification-Based Error Analysis\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "This approach provides explicit, structured error reasons by using a second\n",
    "classification eval to analyze why judges made mistakes.\n",
    "\n",
    "Key Benefits:\n",
    "- Explicit error categorization\n",
    "- Works well with GEPA requirements\n",
    "- Provides actionable feedback\n",
    "\n",
    "Trade-offs:\n",
    "- Requires additional API calls (cost)\n",
    "- Adds latency to optimization loop\n",
    "- Need to wait for classification eval to complete\n",
    "\n",
    "To use this approach:\n",
    "1. Uncomment the code above\n",
    "2. Run error analysis on failed examples\n",
    "3. Use classification results as \"reasons\" for GEPA optimization\n",
    "\"\"\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250061b2",
   "metadata": {},
   "source": [
    "## üìã Summary: Choosing Your Approach\n",
    "\n",
    "### Quick Decision Guide\n",
    "\n",
    "**Choose Approach 1 (Judge's Own Feedback)** if:\n",
    "- You want the simplest, fastest solution\n",
    "- Cost is a primary concern\n",
    "- You're okay with implicit reasoning\n",
    "\n",
    "**Choose Approach 2 (Classification Eval)** if:\n",
    "- You need explicit, structured error reasons\n",
    "- You're integrating with GEPA and need high-quality feedback\n",
    "- Cost and latency are acceptable trade-offs\n",
    "\n",
    "**Choose Approach 3 (Simplified Optimization)** if:\n",
    "- You want to avoid the reasoning requirement entirely\n",
    "- You prefer trial-and-error optimization\n",
    "- You want to get started quickly\n",
    "\n",
    "**Choose Approach 4 (Pattern Analysis)** if:\n",
    "- You want a balance of quality and cost\n",
    "- You want to leverage patterns across many examples\n",
    "- You want structured feedback without extra API calls\n",
    "\n",
    "**Choose Approach 5 (DSPy + GEPA)** if:\n",
    "- You want to follow the blog post approach closely\n",
    "- You're comfortable with DSPy framework\n",
    "- You want a systematic, proven methodology\n",
    "\n",
    "### Recommended Path Forward\n",
    "\n",
    "1. **Start with Approach 4** (Pattern Analysis) - it's implemented above and provides good balance\n",
    "2. **If needed, add Approach 2** (Classification) for explicit error analysis on critical failures\n",
    "3. **Compare results** with baseline to validate improvements\n",
    "4. **Iterate** based on what works best for your specific use case\n",
    "\n",
    "### Integration with GEPA\n",
    "\n",
    "Once you have error reasons (from any approach), you can:\n",
    "\n",
    "1. **Format for GEPA**: Structure error reasons as feedback for prompt optimization\n",
    "2. **Use verifiability**: Compare optimized prompt performance to baseline\n",
    "3. **Iterate**: Let GEPA evolve prompts based on error patterns\n",
    "\n",
    "Example GEPA integration structure:\n",
    "```python\n",
    "# For each failed example, provide:\n",
    "{\n",
    "    'verifiable': True,  # We can verify (compare to ground truth)\n",
    "    'correct': False,   # Judge was wrong\n",
    "    'reason': error_reason['explanation'],  # Why it was wrong\n",
    "    'error_type': error_reason['error_type']  # Category of error\n",
    "}\n",
    "```\n",
    "\n",
    "This satisfies GEPA's requirements: verifiability ‚úÖ + reasoning ‚úÖ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a9335",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì Phase 3: Preference Fine-Tuning Judge Models\n",
    "\n",
    "Now we'll train a specialized judge model using Direct Preference Optimization (DPO) on the RewardBench 2 training data. This approach teaches a smaller model to distinguish between chosen and rejected responses, potentially matching or exceeding larger baseline judges at a fraction of the cost.\n",
    "\n",
    "## üéØ Phase 3 Goals\n",
    "\n",
    "1. **Transform Data**: Convert RewardBench 2 into DPO preference pairs\n",
    "2. **Fine-Tune Model**: Train a smaller model (8B) on preference data\n",
    "3. **Evaluate**: Test fine-tuned judge against selected baseline judges on test set\n",
    "4. **Analyze**: Compare quality, cost, and per-category performance\n",
    "\n",
    "**Selected Baseline Judges for Comparison:**\n",
    "- GPT-OSS 120B\n",
    "- Kimi K2 Instruct\n",
    "- Qwen3 235B\n",
    "\n",
    "**Hypothesis**: A fine-tuned 8B model can match large baseline judges at 10-50x lower inference cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df393391",
   "metadata": {},
   "source": [
    "## üìä Step 1: Prepare Preference Data from RewardBench 2\n",
    "\n",
    "Each RewardBench 2 example contains:\n",
    "- 1 chosen response (ground truth winner)\n",
    "- 3 rejected responses (ground truth losers)\n",
    "\n",
    "We'll create **3 preference pairs** per example:\n",
    "- (chosen vs rejected_1)\n",
    "- (chosen vs rejected_2)\n",
    "- (chosen vs rejected_3)\n",
    "\n",
    "This gives us ~3,000 preference training pairs from our 997 training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340bab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preference_pairs_from_rewardbench(dataset):\n",
    "    \"\"\"\n",
    "    Transform RewardBench 2 dataset into DPO preference pairs.\n",
    "    \n",
    "    For each example with 1 chosen + 3 rejected responses,\n",
    "    create 3 preference pairs: (chosen vs rejected_i).\n",
    "    \n",
    "    Format for Together AI DPO:\n",
    "    {\n",
    "        \"input\": {\"messages\": [{\"role\": \"user\", \"content\": \"PROMPT\"}]},\n",
    "        \"preferred_output\": [{\"role\": \"assistant\", \"content\": \"CHOSEN\"}],\n",
    "        \"non_preferred_output\": [{\"role\": \"assistant\", \"content\": \"REJECTED\"}]\n",
    "    }\n",
    "    \"\"\"\n",
    "    preference_pairs = []\n",
    "    \n",
    "    for example in tqdm(dataset, desc=\"Creating preference pairs\"):\n",
    "        prompt = example['prompt']\n",
    "        chosen = example['chosen'][0]  # Unwrap from list\n",
    "        rejected_list = example['rejected']  # List of 3 rejected responses\n",
    "        subset = example['subset']\n",
    "        example_id = example['id']\n",
    "        \n",
    "        # Create 3 preference pairs per example\n",
    "        for idx, rejected in enumerate(rejected_list):\n",
    "            preference_pair = {\n",
    "                \"input\": {\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                },\n",
    "                \"preferred_output\": [\n",
    "                    {\"role\": \"assistant\", \"content\": chosen}\n",
    "                ],\n",
    "                \"non_preferred_output\": [\n",
    "                    {\"role\": \"assistant\", \"content\": rejected}\n",
    "                ],\n",
    "                # Metadata for analysis\n",
    "                \"metadata\": {\n",
    "                    \"subset\": subset,\n",
    "                    \"example_id\": example_id,\n",
    "                    \"rejected_idx\": idx\n",
    "                }\n",
    "            }\n",
    "            preference_pairs.append(preference_pair)\n",
    "    \n",
    "    return preference_pairs\n",
    "\n",
    "# Create preference pairs from training set\n",
    "print(f\"Creating preference pairs from {len(train_dataset)} training examples...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "preference_pairs = create_preference_pairs_from_rewardbench(train_dataset)\n",
    "\n",
    "print(f\"\\n‚úì Created {len(preference_pairs)} preference pairs\")\n",
    "print(f\"  Original examples: {len(train_dataset)}\")\n",
    "print(f\"  Pairs per example: 3\")\n",
    "print(f\"  Total pairs: {len(preference_pairs)}\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nüìã Sample Preference Pair:\")\n",
    "print(\"=\"*80)\n",
    "sample = preference_pairs[0]\n",
    "print(f\"Subset: {sample['metadata']['subset']}\")\n",
    "print(f\"\\nPrompt: {sample['input']['messages'][0]['content'][:200]}...\")\n",
    "print(f\"\\nPreferred (Chosen): {sample['preferred_output'][0]['content'][:200]}...\")\n",
    "print(f\"\\nNon-Preferred (Rejected): {sample['non_preferred_output'][0]['content'][:200]}...\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b2031c",
   "metadata": {},
   "source": [
    "## üîÄ Step 2: Create Train/Validation Split for DPO\n",
    "\n",
    "We'll split the preference pairs 90/10 for training and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f738f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split preference pairs\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(preference_pairs)\n",
    "\n",
    "# 90/10 split\n",
    "split_idx = int(0.9 * len(preference_pairs))\n",
    "dpo_train_pairs = preference_pairs[:split_idx]\n",
    "dpo_val_pairs = preference_pairs[split_idx:]\n",
    "\n",
    "print(f\"DPO Dataset Split:\")\n",
    "print(f\"  Training pairs: {len(dpo_train_pairs)}\")\n",
    "print(f\"  Validation pairs: {len(dpo_val_pairs)}\")\n",
    "print(f\"  Total pairs: {len(preference_pairs)}\")\n",
    "\n",
    "# Analyze distribution by subset\n",
    "train_subset_dist = defaultdict(int)\n",
    "val_subset_dist = defaultdict(int)\n",
    "\n",
    "for pair in dpo_train_pairs:\n",
    "    train_subset_dist[pair['metadata']['subset']] += 1\n",
    "\n",
    "for pair in dpo_val_pairs:\n",
    "    val_subset_dist[pair['metadata']['subset']] += 1\n",
    "\n",
    "print(f\"\\nüìä Training pairs by subset:\")\n",
    "for subset, count in sorted(train_subset_dist.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {subset}: {count} pairs\")\n",
    "\n",
    "print(f\"\\nüìä Validation pairs by subset:\")\n",
    "for subset, count in sorted(val_subset_dist.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {subset}: {count} pairs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9195e900",
   "metadata": {},
   "source": [
    "## üíæ Step 3: Save and Upload DPO Datasets\n",
    "\n",
    "Save as JSONL files (without metadata) and upload to Together AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a458d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directory for DPO data\n",
    "os.makedirs('judge_dpo_data', exist_ok=True)\n",
    "\n",
    "def save_preference_pairs(pairs, filepath):\n",
    "    \"\"\"Save preference pairs as JSONL (without metadata for training).\"\"\"\n",
    "    with open(filepath, 'w') as f:\n",
    "        for pair in pairs:\n",
    "            # Remove metadata before saving\n",
    "            clean_pair = {\n",
    "                \"input\": pair[\"input\"],\n",
    "                \"preferred_output\": pair[\"preferred_output\"],\n",
    "                \"non_preferred_output\": pair[\"non_preferred_output\"]\n",
    "            }\n",
    "            f.write(json.dumps(clean_pair) + '\\n')\n",
    "    print(f\"‚úì Saved {len(pairs)} pairs to {filepath}\")\n",
    "\n",
    "# Save train and validation datasets\n",
    "dpo_train_path = 'judge_dpo_data/rewardbench2_dpo_train.jsonl'\n",
    "dpo_val_path = 'judge_dpo_data/rewardbench2_dpo_val.jsonl'\n",
    "\n",
    "save_preference_pairs(dpo_train_pairs, dpo_train_path)\n",
    "save_preference_pairs(dpo_val_pairs, dpo_val_path)\n",
    "\n",
    "# Check file sizes\n",
    "train_size_mb = os.path.getsize(dpo_train_path) / (1024 * 1024)\n",
    "val_size_mb = os.path.getsize(dpo_val_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüìÇ File sizes:\")\n",
    "print(f\"  Training: {train_size_mb:.2f} MB\")\n",
    "print(f\"  Validation: {val_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ffa66b",
   "metadata": {},
   "source": [
    "## üì§ Upload DPO Datasets to Together AI\n",
    "\n",
    "Upload both train and validation files with format validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uploading DPO datasets to Together AI...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Upload training file with format check\n",
    "print(\"\\nüì§ Uploading training file...\")\n",
    "dpo_train_file = client.files.upload(dpo_train_path, check=True)\n",
    "print(f\"‚úì Training file uploaded\")\n",
    "print(f\"  File ID: {dpo_train_file.id}\")\n",
    "print(f\"  Filename: {dpo_train_file.filename}\")\n",
    "print(f\"  Bytes: {dpo_train_file.bytes:,}\")\n",
    "\n",
    "# Upload validation file with format check\n",
    "print(\"\\nüì§ Uploading validation file...\")\n",
    "dpo_val_file = client.files.upload(dpo_val_path, check=True)\n",
    "print(f\"‚úì Validation file uploaded\")\n",
    "print(f\"  File ID: {dpo_val_file.id}\")\n",
    "print(f\"  Filename: {dpo_val_file.filename}\")\n",
    "print(f\"  Bytes: {dpo_val_file.bytes:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Both DPO datasets uploaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9448c",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Launch DPO Fine-Tuning Jobs\n",
    "\n",
    "We'll fine-tune **4 different models** using DPO on our preference pairs:\n",
    "1. **Llama 3.1 8B Instruct** - Cost-efficient baseline\n",
    "2. **GPT-OSS 120B** - Large baseline judge\n",
    "3. **Kimi K2 Instruct** - Alternative large judge\n",
    "4. **Qwen3 235B** - Largest baseline judge\n",
    "\n",
    "This will let us compare:\n",
    "- How DPO improves each baseline model\n",
    "- Fine-tuned 8B vs fine-tuned large models\n",
    "- Cost-performance trade-offs after fine-tuning\n",
    "\n",
    "**Key Parameters:**\n",
    "- `training_method='dpo'`: Use Direct Preference Optimization\n",
    "- `dpo_beta=0.1`: Standard DPO beta (controls deviation from reference)\n",
    "- `n_epochs=3`: Train for 3 epochs\n",
    "- `learning_rate=5e-6`: Lower learning rate for stability\n",
    "- `lora=True`: Efficient LoRA fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to fine-tune\n",
    "DPO_MODELS = {\n",
    "    \"Llama 3.1 8B\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"GPT-OSS 120B\": \"openai/gpt-oss-120b\",\n",
    "    \"Kimi K2 Instruct\": \"moonshotai/Kimi-K2-Instruct-0905\",\n",
    "    \"Qwen3 235B\": \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\",\n",
    "}\n",
    "\n",
    "print(f\"üöÄ Launching DPO fine-tuning jobs for {len(DPO_MODELS)} models...\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training pairs: {len(dpo_train_pairs)}\")\n",
    "print(f\"Validation pairs: {len(dpo_val_pairs)}\")\n",
    "print(f\"Training method: DPO\")\n",
    "print(f\"DPO Beta: 0.1\")\n",
    "print(f\"Epochs: 3\")\n",
    "print(f\"Learning rate: 5e-6\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Launch DPO fine-tuning for each model\n",
    "dpo_jobs = {}\n",
    "\n",
    "for model_name, model_id in DPO_MODELS.items():\n",
    "    print(f\"\\nüîÑ Launching DPO job for {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        dpo_job = client.fine_tuning.create(\n",
    "            training_method='dpo',\n",
    "            dpo_beta=0.1,\n",
    "            training_file=dpo_train_file.id,\n",
    "            validation_file=dpo_val_file.id,\n",
    "            model=model_id,\n",
    "            n_epochs=3,\n",
    "            n_evals=10,\n",
    "            n_checkpoints=1,\n",
    "            learning_rate=5e-6,\n",
    "            lora=True,\n",
    "            suffix=f\"rewardbench2_judge_dpo_{model_name.replace(' ', '_').lower()}\",\n",
    "            # Optional: Add WandB logging if API key available\n",
    "            wandb_api_key=os.getenv(\"WANDB_API_KEY\"),\n",
    "            # wandb_project_name=\"llm-judge-optimization\",\n",
    "        )\n",
    "        \n",
    "        dpo_jobs[model_name] = {\n",
    "            'job': dpo_job,\n",
    "            'model_id': model_id,\n",
    "            'job_id': dpo_job.id,\n",
    "            'status': dpo_job.status\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì Job launched: {dpo_job.id}\")\n",
    "        print(f\"  Status: {dpo_job.status}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error launching job: {str(e)}\")\n",
    "        dpo_jobs[model_name] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Launched {len([j for j in dpo_jobs.values() if j])} DPO fine-tuning jobs\")\n",
    "print(\"\\nüìã Job Summary:\")\n",
    "for model_name, job_info in dpo_jobs.items():\n",
    "    if job_info:\n",
    "        print(f\"  {model_name:20s}: {job_info['job_id']}\")\n",
    "print(\"\\n‚è≥ All jobs training in progress...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5acfc",
   "metadata": {},
   "source": [
    "## ‚è≥ Step 5: Wait for DPO Training to Complete\n",
    "\n",
    "Monitor the training job and wait for completion. Key metrics to watch:\n",
    "- **Reward Accuracy**: Should increase (target >70%)\n",
    "- **KL Divergence**: Should rise gradually\n",
    "- **Loss**: Should decrease smoothly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3aa024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_fine_tuning(job_id, model_name, check_interval=60):\n",
    "    \"\"\"\n",
    "    Poll fine-tuning job status until complete.\n",
    "    \n",
    "    Args:\n",
    "        job_id: Fine-tuning job ID\n",
    "        model_name: Name of model being fine-tuned\n",
    "        check_interval: Seconds between status checks\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚è≥ Waiting for {model_name} fine-tuning job to complete...\")\n",
    "    print(f\"   Job ID: {job_id}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    last_status = None\n",
    "    \n",
    "    while True:\n",
    "        # Check job status\n",
    "        job_status = client.fine_tuning.retrieve(job_id)\n",
    "        \n",
    "        # Print status update if changed\n",
    "        if job_status.status != last_status:\n",
    "            elapsed = time.time() - start_time\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))\n",
    "            print(f\"   [{elapsed_str}] {model_name}: {job_status.status}\")\n",
    "            last_status = job_status.status\n",
    "        \n",
    "        # Check if complete\n",
    "        if job_status.status == \"completed\":\n",
    "            elapsed = time.time() - start_time\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))\n",
    "            print(f\"   ‚úì {model_name} completed! Output: {job_status.output_name}\")\n",
    "            print(f\"   Total time: {elapsed_str}\")\n",
    "            return job_status\n",
    "        \n",
    "        elif job_status.status == \"failed\":\n",
    "            print(f\"   ‚úó {model_name} failed!\")\n",
    "            if hasattr(job_status, 'error'):\n",
    "                print(f\"   Error: {job_status.error}\")\n",
    "            return None\n",
    "        \n",
    "        elif job_status.status in [\"cancelled\", \"canceled\"]:\n",
    "            print(f\"   ‚ö† {model_name} was cancelled\")\n",
    "            return None\n",
    "        \n",
    "        # Wait before next check\n",
    "        time.sleep(check_interval)\n",
    "\n",
    "# Wait for all DPO jobs to complete\n",
    "print(\"=\"*80)\n",
    "print(f\"‚è≥ Waiting for all {len(dpo_jobs)} fine-tuning jobs to complete...\")\n",
    "print(\"   This may take 1-3 hours depending on model sizes\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dpo_jobs_complete = {}\n",
    "\n",
    "for model_name, job_info in dpo_jobs.items():\n",
    "    if job_info is None:\n",
    "        print(f\"\\n‚ö† Skipping {model_name} (job failed to launch)\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        completed_job = wait_for_fine_tuning(\n",
    "            job_info['job_id'], \n",
    "            model_name, \n",
    "            check_interval=60\n",
    "        )\n",
    "        \n",
    "        if completed_job:\n",
    "            dpo_jobs_complete[model_name] = {\n",
    "                'job': completed_job,\n",
    "                'model_id': job_info['model_id'],\n",
    "                'output_name': completed_job.output_name,\n",
    "                'job_id': job_info['job_id']\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error waiting for {model_name}: {str(e)}\")\n",
    "        dpo_jobs_complete[model_name] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì {len([j for j in dpo_jobs_complete.values() if j])} fine-tuning jobs completed successfully\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display completed models\n",
    "print(\"\\nüìã Fine-Tuned Models:\")\n",
    "for model_name, job_info in dpo_jobs_complete.items():\n",
    "    if job_info:\n",
    "        print(f\"  {model_name:20s}: {job_info['output_name']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26755fe",
   "metadata": {},
   "source": [
    "### ==== Bring up finetuned models on DE ===="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95f8aa",
   "metadata": {},
   "source": [
    "## üìä Step 6: Evaluate All Fine-Tuned Judges on Test Set\n",
    "\n",
    "Now we'll evaluate all fine-tuned judge models on the test set (297 examples) and compare against their baseline versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc1e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ü§ñ Fine-Tuned Judge Models:\")\n",
    "print(\"=\"*80)\n",
    "for model_name, job_info in dpo_jobs_complete.items():\n",
    "    if job_info:\n",
    "        print(f\"  {model_name:20s}: {job_info['output_name']}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Evaluating all fine-tuned judges on test set ({len(test_dataset)} examples)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Launch evaluations for all fine-tuned judges\n",
    "finetuned_evals = {}\n",
    "\n",
    "for model_name, job_info in dpo_jobs_complete.items():\n",
    "    if job_info is None:\n",
    "        print(f\"\\n‚ö† Skipping {model_name} (fine-tuning failed)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüîÑ Launching evaluation for {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        eval_response = client.evaluation.create(\n",
    "            type=\"compare\",\n",
    "            input_data_file_path=test_file.id,  # Using test file from Phase 1\n",
    "            judge_model=job_info['output_name'],\n",
    "            judge_model_source=\"serverless\",\n",
    "            judge_system_template=PAIRWISE_JUDGE_PROMPT,  # Same prompt as baseline\n",
    "            model_a=\"chosen\",\n",
    "            model_b=\"rejected_1\"\n",
    "        )\n",
    "        \n",
    "        finetuned_evals[model_name] = {\n",
    "            'eval': eval_response,\n",
    "            'output_name': job_info['output_name'],\n",
    "            'base_model_id': job_info['model_id'],\n",
    "            'workflow_id': eval_response.workflow_id\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì Evaluation launched: {eval_response.workflow_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error: {str(e)}\")\n",
    "        finetuned_evals[model_name] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Launched {len([e for e in finetuned_evals.values() if e])} evaluations\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467cedb0",
   "metadata": {},
   "source": [
    "## ‚è≥ Wait for All Fine-Tuned Judge Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚è≥ Waiting for all fine-tuned judge evaluations to complete...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Wait for all evaluations to complete\n",
    "finetuned_eval_statuses = {}\n",
    "\n",
    "for model_name, eval_info in finetuned_evals.items():\n",
    "    if eval_info is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n‚è≥ Waiting for {model_name} evaluation...\")\n",
    "    \n",
    "    try:\n",
    "        status = wait_for_evaluation(eval_info['workflow_id'], check_interval=30)\n",
    "        finetuned_eval_statuses[model_name] = {\n",
    "            'status': status,\n",
    "            'output_name': eval_info['output_name'],\n",
    "            'base_model_id': eval_info['base_model_id']\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì {model_name} evaluation completed!\")\n",
    "        print(f\"  Results: A_wins={status.results.get('A_wins', 0)}, \"\n",
    "              f\"B_wins={status.results.get('B_wins', 0)}, \"\n",
    "              f\"Ties={status.results.get('Ties', 0)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error: {str(e)}\")\n",
    "        finetuned_eval_statuses[model_name] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì {len([s for s in finetuned_eval_statuses.values() if s])} evaluations completed\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd4638",
   "metadata": {},
   "source": [
    "## üì• Download and Process All Fine-Tuned Judge Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79282a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üì• Downloading and processing results for all fine-tuned judges...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Process results for all fine-tuned judges\n",
    "all_finetuned_metrics = {}\n",
    "\n",
    "for model_name, eval_info in finetuned_eval_statuses.items():\n",
    "    if eval_info is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüì• Processing {model_name}...\")\n",
    "    \n",
    "    status = eval_info['status']\n",
    "    result_file_id = status.results.get('result_file_id')\n",
    "    \n",
    "    if not result_file_id:\n",
    "        print(f\"  ‚úó No result file found\")\n",
    "        continue\n",
    "    \n",
    "    # Download results\n",
    "    safe_name = model_name.replace(' ', '_').lower()\n",
    "    output_path = f\"judge_results/finetuned_{safe_name}_test_results.jsonl\"\n",
    "    client.files.retrieve_content(result_file_id, output=output_path)\n",
    "    \n",
    "    # Load and process results\n",
    "    results = []\n",
    "    with open(output_path, 'r') as f:\n",
    "        for line in f:\n",
    "            results.append(json.loads(line))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_examples = len(results)\n",
    "    a_wins = sum(1 for r in results if r.get('final_decision') == 'A')\n",
    "    b_wins = sum(1 for r in results if r.get('final_decision') == 'B')\n",
    "    ties = sum(1 for r in results if r.get('final_decision') == 'Tie')\n",
    "    accuracy = a_wins / total_examples if total_examples > 0 else 0\n",
    "    \n",
    "    # Per-subset accuracy\n",
    "    subset_metrics = defaultdict(lambda: {'correct': 0, 'total': 0, 'ties': 0})\n",
    "    for result in results:\n",
    "        subset = result.get('subset', 'Unknown')\n",
    "        subset_metrics[subset]['total'] += 1\n",
    "        \n",
    "        if result.get('final_decision') == 'A':\n",
    "            subset_metrics[subset]['correct'] += 1\n",
    "        elif result.get('final_decision') == 'Tie':\n",
    "            subset_metrics[subset]['ties'] += 1\n",
    "    \n",
    "    subset_accuracy = {\n",
    "        subset: stats['correct'] / stats['total'] if stats['total'] > 0 else 0\n",
    "        for subset, stats in subset_metrics.items()\n",
    "    }\n",
    "    \n",
    "    # Store metrics\n",
    "    all_finetuned_metrics[model_name] = {\n",
    "        'judge_model': eval_info['output_name'],\n",
    "        'judge_name': f'{model_name} + DPO',\n",
    "        'base_model_id': eval_info['base_model_id'],\n",
    "        'total_examples': total_examples,\n",
    "        'accuracy': accuracy,\n",
    "        'a_wins': a_wins,\n",
    "        'b_wins': b_wins,\n",
    "        'ties': ties,\n",
    "        'subset_accuracy': subset_accuracy,\n",
    "        'results_file': output_path\n",
    "    }\n",
    "    \n",
    "    print(f\"  ‚úì Processed {total_examples} examples\")\n",
    "    print(f\"  Accuracy: {accuracy*100:.2f}% (A: {a_wins}, B: {b_wins}, Ties: {ties})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úì Processed results for {len(all_finetuned_metrics)} fine-tuned judges\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8408db1",
   "metadata": {},
   "source": [
    "## üìä Compare All Fine-Tuned Judges vs. Their Baselines\n",
    "\n",
    "Now let's compare each fine-tuned judge against its baseline version to see improvement from DPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison with baselines and fine-tuned versions\n",
    "comparison_data = []\n",
    "\n",
    "# Map baseline model IDs to friendly names\n",
    "baseline_model_map = {\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\": \"Llama 3.1 8B\",\n",
    "    \"openai/gpt-oss-120b\": \"GPT-OSS 120B\",\n",
    "    \"moonshotai/Kimi-K2-Instruct-0905\": \"Kimi K2 Instruct\",\n",
    "    \"Qwen/Qwen3-235B-A22B-Instruct-2507-tput\": \"Qwen3 235B\",\n",
    "}\n",
    "\n",
    "# Add baseline judges\n",
    "for model_id, model_name in baseline_model_map.items():\n",
    "    if model_id in all_judge_metrics:\n",
    "        metrics = all_judge_metrics[model_id]\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Version': 'Baseline',\n",
    "            'Test Accuracy': metrics['accuracy'] * 100,\n",
    "            'Chosen Wins (A)': metrics['a_wins'],\n",
    "            'Rejected Wins (B)': metrics['b_wins'],\n",
    "            'Ties': metrics['ties'],\n",
    "            'Total Examples': metrics['total_examples']\n",
    "        })\n",
    "\n",
    "# Add fine-tuned judges\n",
    "for model_name, metrics in all_finetuned_metrics.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Version': '+ DPO',\n",
    "        'Test Accuracy': metrics['accuracy'] * 100,\n",
    "        'Chosen Wins (A)': metrics['a_wins'],\n",
    "        'Rejected Wins (B)': metrics['b_wins'],\n",
    "        'Ties': metrics['ties'],\n",
    "        'Total Examples': metrics['total_examples']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values(['Model', 'Version'], ascending=[True, True])\n",
    "\n",
    "print(\"\\n\" + \"=\"*110)\n",
    "print(\"PHASE 3: BASELINE vs. FINE-TUNED JUDGES (TEST SET)\")\n",
    "print(\"=\"*110)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*110)\n",
    "\n",
    "# Calculate improvements for each model\n",
    "print(f\"\\nüìä DPO Improvement Analysis:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in baseline_model_map.values():\n",
    "    # Get baseline metrics\n",
    "    baseline_row = comparison_df[(comparison_df['Model'] == model_name) & (comparison_df['Version'] == 'Baseline')]\n",
    "    finetuned_row = comparison_df[(comparison_df['Model'] == model_name) & (comparison_df['Version'] == '+ DPO')]\n",
    "    \n",
    "    if not baseline_row.empty and not finetuned_row.empty:\n",
    "        baseline_acc = baseline_row['Test Accuracy'].values[0]\n",
    "        finetuned_acc = finetuned_row['Test Accuracy'].values[0]\n",
    "        improvement = finetuned_acc - baseline_acc\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Baseline:    {baseline_acc:5.2f}%\")\n",
    "        print(f\"  Fine-tuned:  {finetuned_acc:5.2f}%\")\n",
    "        print(f\"  Improvement: {improvement:+5.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0802f8",
   "metadata": {},
   "source": [
    "## üìà Visualize Judge Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ebd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grouped bar chart: Baseline vs Fine-tuned for each model\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Get unique models and their baseline/finetuned accuracies\n",
    "models = comparison_df['Model'].unique()\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "baseline_accs = []\n",
    "finetuned_accs = []\n",
    "\n",
    "for model in models:\n",
    "    baseline = comparison_df[(comparison_df['Model'] == model) & (comparison_df['Version'] == 'Baseline')]\n",
    "    finetuned = comparison_df[(comparison_df['Model'] == model) & (comparison_df['Version'] == '+ DPO')]\n",
    "    \n",
    "    baseline_accs.append(baseline['Test Accuracy'].values[0] if not baseline.empty else 0)\n",
    "    finetuned_accs.append(finetuned['Test Accuracy'].values[0] if not finetuned.empty else 0)\n",
    "\n",
    "# Create bars\n",
    "bars1 = ax.bar(x - width/2, baseline_accs, width, label='Baseline', \n",
    "               color='#3498DB', edgecolor='black', alpha=0.8, linewidth=1.5)\n",
    "bars2 = ax.bar(x + width/2, finetuned_accs, width, label='+ DPO Fine-Tuned', \n",
    "               color='#2ECC71', edgecolor='black', alpha=0.8, linewidth=1.5)\n",
    "\n",
    "# Customize plot\n",
    "ax.set_title('Baseline vs. DPO Fine-Tuned Judges on RewardBench 2 TEST SET', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Judge Model', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Test Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=20, ha='right', fontsize=11)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "ax.legend(fontsize=12, loc='upper left')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406c6ce",
   "metadata": {},
   "source": [
    "## üî¨ Per-Category Performance Analysis\n",
    "\n",
    "Let's see how the fine-tuned judge performs across different RewardBench 2 categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create per-subset comparison for all judges\n",
    "subset_comparison_data = []\n",
    "\n",
    "# Add baseline judges\n",
    "for model_id, model_name in baseline_model_map.items():\n",
    "    if model_id in all_judge_metrics:\n",
    "        metrics = all_judge_metrics[model_id]\n",
    "        row = {'Judge': f'{model_name} (Baseline)'}\n",
    "        for subset, acc in metrics['subset_accuracy'].items():\n",
    "            row[subset] = acc * 100\n",
    "        subset_comparison_data.append(row)\n",
    "\n",
    "# Add fine-tuned judges\n",
    "for model_name, metrics in all_finetuned_metrics.items():\n",
    "    row = {'Judge': f'{model_name} + DPO'}\n",
    "    for subset, acc in metrics['subset_accuracy'].items():\n",
    "        row[subset] = acc * 100\n",
    "    subset_comparison_data.append(row)\n",
    "\n",
    "subset_comparison_df = pd.DataFrame(subset_comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"PER-CATEGORY ACCURACY (%) - BASELINE vs. FINE-TUNED JUDGES\")\n",
    "print(\"=\"*120)\n",
    "print(subset_comparison_df.to_string(index=False, float_format='%.1f'))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Calculate category improvements for each model\n",
    "print(f\"\\nüìä Per-Category DPO Improvements:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in baseline_model_map.values():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    \n",
    "    # Get baseline and fine-tuned metrics\n",
    "    baseline_idx = next((i for i, row in enumerate(subset_comparison_data) if row['Judge'] == f'{model_name} (Baseline)'), None)\n",
    "    finetuned_idx = next((i for i, row in enumerate(subset_comparison_data) if row['Judge'] == f'{model_name} + DPO'), None)\n",
    "    \n",
    "    if baseline_idx is not None and finetuned_idx is not None:\n",
    "        baseline_row = subset_comparison_data[baseline_idx]\n",
    "        finetuned_row = subset_comparison_data[finetuned_idx]\n",
    "        \n",
    "        # Get all subsets (excluding \"Judge\" key)\n",
    "        subsets = [k for k in baseline_row.keys() if k != 'Judge']\n",
    "        \n",
    "        for subset in subsets:\n",
    "            if subset in baseline_row and subset in finetuned_row:\n",
    "                baseline_acc = baseline_row[subset]\n",
    "                finetuned_acc = finetuned_row[subset]\n",
    "                improvement = finetuned_acc - baseline_acc\n",
    "                print(f\"  {subset:15s}: {baseline_acc:5.1f}% ‚Üí {finetuned_acc:5.1f}% ({improvement:+5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
