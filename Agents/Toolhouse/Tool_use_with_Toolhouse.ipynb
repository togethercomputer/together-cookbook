{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "73b0444b",
      "metadata": {
        "id": "73b0444b"
      },
      "source": [
        "# Using Toolhouse for Tool Use on Together.AI infrastructure\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/orliesaurus/together-cookbook/blob/toolhouse-tool-use-cookbook/Tool_use_with_Toolhouse.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "626a645c",
      "metadata": {
        "id": "626a645c"
      },
      "source": [
        "This Colab notebook leverages Together's powerful AI models and Toolhouse's streamlined function calling toolkit to efficiently execute complex tasks and generate creative content.\n",
        "\n",
        "[Toolhouse](https://app.toolhouse.ai/) is the first complete infrastructure platform for building, running and managing tool use.\n",
        "\n",
        "With Toolhouse, you can equip your LLM with extra skills (also known as tools).\n",
        "\n",
        "Some of the most popular tools include:\n",
        "- scraping data from the web or social media\n",
        "- generate images\n",
        "- compile or execute code and return the values\n",
        "\n",
        "This cookbook demonstrates how you can **equip LLMs running on** [Together.ai](https://together.ai/) - **with tools**, without the need for your to code or prompt these tools.\n",
        "\n",
        "In this short demo, we'll show how we can equip an LLM to generate images from real-data.\n",
        "\n",
        "We'll use Toolhouse with the model tagged `meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo` and hosted on Together's infrastructure.\n",
        "\n",
        "This model is fine-tuned for effective and precise tool use."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002c27a7",
      "metadata": {
        "id": "002c27a7"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install the dependencies.\n",
        "We will use the OpenAI SDK which is compatible with Together's API - as long as we set the base_url.\n",
        "\n",
        "We're also installing Toolhouse's Python SDK to access pre-built, pre-hosted tools."
      ],
      "metadata": {
        "id": "M11wAhQR2JLu"
      },
      "id": "M11wAhQR2JLu"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install toolhouse openai"
      ],
      "metadata": {
        "id": "LaMYsh-3580q",
        "collapsed": true,
        "outputId": "6cbeffc8-804c-4ecc-d493-9af7d8c1a3b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LaMYsh-3580q",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting toolhouse\n",
            "  Downloading toolhouse-1.3.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
            "Collecting anthropic (from toolhouse)\n",
            "  Downloading anthropic-0.42.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting groq (from toolhouse)\n",
            "  Downloading groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting http-exceptions (from toolhouse)\n",
            "  Downloading http_exceptions-0.2.10-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting llama-index (from toolhouse)\n",
            "  Downloading llama_index-0.12.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting python-dotenv>=1.0.1 (from toolhouse)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from toolhouse) (2.32.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index->toolhouse)\n",
            "  Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index->toolhouse)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.8 (from llama-index->toolhouse)\n",
            "  Downloading llama_index_core-0.12.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index->toolhouse)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index->toolhouse)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index->toolhouse)\n",
            "  Downloading llama_index_llms_openai-0.3.12-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index->toolhouse)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index->toolhouse)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index->toolhouse)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index->toolhouse)\n",
            "  Downloading llama_index_readers_file-0.4.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index->toolhouse)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index->toolhouse) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->toolhouse) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->toolhouse) (2.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (3.11.10)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (1.2.15)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (2024.10.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (11.0.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (1.17.0)\n",
            "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->toolhouse)\n",
            "  Downloading llama_cloud-0.1.7-py3-none-any.whl.metadata (860 bytes)\n",
            "Collecting openai\n",
            "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->toolhouse) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->toolhouse) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->toolhouse)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->toolhouse)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->toolhouse)\n",
            "  Downloading llama_parse-0.5.18-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index->toolhouse) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index->toolhouse) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index->toolhouse) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->toolhouse) (2.6)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse)\n",
            "  Downloading marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->toolhouse) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->toolhouse) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->toolhouse) (2024.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.8->llama-index->toolhouse) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->toolhouse) (1.17.0)\n",
            "Downloading toolhouse-1.3.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading anthropic-0.42.0-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.13.1-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading http_exceptions-0.2.10-py3-none-any.whl (8.8 kB)\n",
            "Downloading llama_index-0.12.8-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.12.8-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_llms_openai-0.3.12-py3-none-any.whl (14 kB)\n",
            "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_multi_modal_llms_openai-0.4.1-py3-none-any.whl (5.8 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.1-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.7-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.18-py3-none-any.whl (15 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, python-dotenv, pypdf, mypy-extensions, marshmallow, http-exceptions, typing-inspect, tiktoken, openai, llama-cloud, groq, dataclasses-json, anthropic, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index, toolhouse\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.57.4\n",
            "    Uninstalling openai-1.57.4:\n",
            "      Successfully uninstalled openai-1.57.4\n",
            "Successfully installed anthropic-0.42.0 dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 groq-0.13.1 http-exceptions-0.2.10 llama-cloud-0.1.7 llama-index-0.12.8 llama-index-agent-openai-0.4.1 llama-index-cli-0.4.0 llama-index-core-0.12.8 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-llms-openai-0.3.12 llama-index-multi-modal-llms-openai-0.4.1 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.1 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.18 marshmallow-3.23.2 mypy-extensions-1.0.0 openai-1.58.1 pypdf-5.1.0 python-dotenv-1.0.1 striprtf-0.0.26 tiktoken-0.8.0 toolhouse-1.3.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d237c86c",
      "metadata": {
        "id": "d237c86c",
        "outputId": "bb89bfca-1ddb-406e-897e-580c02139a57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Packages installed\n"
          ]
        }
      ],
      "source": [
        "# Import packages\n",
        "try:\n",
        "  import os\n",
        "  from openai import OpenAI\n",
        "  from toolhouse import Toolhouse\n",
        "  print(\"Packages installed\")\n",
        "except:\n",
        "  print(\"Packages not installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "458ace84",
      "metadata": {
        "id": "458ace84"
      },
      "source": [
        "To integrate Together and Toolhouse, you'll need to set up two environment variables: `TOGETHER_API_KEY` and `TOOLHOUSE_API_KEY`. Follow these steps to obtain your API keys:\n",
        "\n",
        "* **Together API Key**: Get your key by visiting the [Together Console](https://api.together.ai/).\n",
        "* **Toolhouse API Key**: Sign up for Toolhouse using [this link](https://join.toolhouse.ai) to receive $150 in credits. You will receive your API key as part of the onboarding step, and you can always, navigate to the [Toolhouse API Keys page](https://app.toolhouse.ai/settings/api-keys) to create and get an API key.\n",
        "* **Create a Toolhouse bundle**: We need to create a bundle - a collection of tools - and add 2 specific tools.\n",
        "    *  Navigate to the [Bundle area on Toolhouse](https://app.toolhouse.ai/bundles) and create a new bundle by clicking the Create Bundle blue button in the top right corner. Name your bundle `togethertooluse` and make sure only the `Search X` tool and the `Image Generation` tool are enabled.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up keys in Colab\n",
        "\n",
        "Once you have signe-up and got both API keys, set them as environment variables to start using Together with Toolhouse.\n",
        "\n",
        "To do that click on the Key icon in the left menu on Colab and click **\"Add new secret\".**\n",
        "\n",
        "Create 2 keys and make sure their **name** is spelled like below:\n",
        "```\n",
        "TOGETHER_API_KEY\n",
        "TOOLHOUSE_API_KEY\n",
        "```\n",
        "\n",
        "Enter the values for each key and switch on the **allow note book access\"**"
      ],
      "metadata": {
        "id": "dEPpWhup3Fva"
      },
      "id": "dEPpWhup3Fva"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setting up the SDKs\n",
        "\n",
        "Since we've imported the SDKs already, let's initialize them and authenticate ourselves using the API keys we just set.\n",
        "\n",
        "If you want to check again, your API keys should be now in the left sidebar of Google Colab after clicking the key icon."
      ],
      "metadata": {
        "id": "_kblVg3pXlMy"
      },
      "id": "_kblVg3pXlMy"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bf32deff",
      "metadata": {
        "id": "bf32deff"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "client = OpenAI(base_url = \"https://api.together.xyz/v1\", api_key=userdata.get(\"TOGETHER_API_KEY\"))\n",
        "\n",
        "th = Toolhouse(api_key=userdata.get('TOOLHOUSE_API_KEY'), provider=\"openai\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32220c3f",
      "metadata": {
        "id": "32220c3f"
      },
      "source": [
        "We will use a Meta model specificially the Instruct flavor:\n",
        "- [meta-llama/Llama-3.3-70B-Instruct-Turbo](https://docs.together.ai/docs/function-calling#supported-models).\n",
        "\n",
        "This model is based on the Meta 3.1 70B model and was fine-tuned for tool use and function calling:\n",
        "\n",
        "We can test that we can call the model hosted on Together by running the cell below. It should output some info about NYC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8771ddd7",
      "metadata": {
        "id": "8771ddd7",
        "outputId": "042013b3-2e7f-440d-ad66-5411e0e6d058",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New York, often referred to as the \"City That Never Sleeps,\" is a vibrant and iconic metropolis that embodies the spirit of America. From the bright lights of Times Square to the peaceful green oasis of Central Park, New York City is a melting pot of culture, entertainment, and opportunity. With its rich history, world-class museums, and diverse neighborhoods, such as Chinatown, Little Italy, and Brooklyn, New York has something to offer for everyone, making it one of the most exciting and visited cities in the world.\n"
          ]
        }
      ],
      "source": [
        "MODEL = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"tell me a short blurb about new york\"}],\n",
        ")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ad14fd",
      "metadata": {
        "id": "89ad14fd"
      },
      "source": [
        "Likewise, you can use the `th.get_tools()` function to display all of the Toolhouse tools you have installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d814c0bf",
      "metadata": {
        "id": "d814c0bf",
        "outputId": "4e7ee5c8-85ea-4db1-d531-7b8c2a80a32f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOOLS AVAILABLE:\n",
            "Name: image_generation_flux\n",
            "Type: function\n",
            "Description: This tools enables the LLM to generate images from a given description. The description comes in the format of a prompt. For example: `a photo of a cat, sitting on the couch, looking at the camera, hyper realistic, ultra high definition`. The image generated is returned to the LLM as a JSON containing a string which represents the image in an embeddable markdown format. Only return the markdown embed code to the user. For example: ![](https://example.com/image/url-string)\n",
            "--------\n",
            "Name: search_x\n",
            "Type: function\n",
            "Description: This tool helps your LLM refine and target specific tweets (or posts) from X (formerly Twitter) database. By using advanced search operators, the LLM can compose precise search queries to locate the most relevant posts based on criteria like user, date, media type, and more.\n",
            "--------\n"
          ]
        }
      ],
      "source": [
        "print('TOOLS AVAILABLE:')\n",
        "for tool in th.get_tools(bundle=\"togethertoolhouse\"):\n",
        "    print(f\"Name: {tool['function']['name']}\")\n",
        "    print(f\"Type: {tool['type']}\")\n",
        "    print(f\"Description: {tool['function']['description']}\")\n",
        "    print(\"--------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b2b413",
      "metadata": {
        "id": "b5b2b413"
      },
      "source": [
        "For this demo, we will be using two tools from Toolhouse's store.\n",
        "- Search X\n",
        "- Image generation\n",
        "\n",
        "The first tool lets you search the X social network and the second tool lets you generate an image from a prompt and returns it to the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd565fc7",
      "metadata": {
        "id": "cd565fc7"
      },
      "source": [
        "# 3. Configure Tool Calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09bf2fec",
      "metadata": {
        "id": "09bf2fec"
      },
      "source": [
        "First we'll configure the user prompt to the LLM.\n",
        "This is a basic python list with a dictionary within it that takes `role` and a `content`.\n",
        "\n",
        "We set `role` to `user` because this is a command that we, the user, want the LLM to understand.\n",
        "\n",
        "Doing so will allow us to give a specific command to the LLM. The LLM will look at what tools we have available and attempt to leverage them to achieve the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4b457ee7",
      "metadata": {
        "id": "4b457ee7"
      },
      "outputs": [],
      "source": [
        "# User message to the LLM\n",
        "messages = [\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Find the last three messages from the account named @togethercompute on X/Twitter and summarize them in one sentence. Make it funny!\",\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "396cabd1",
      "metadata": {
        "id": "396cabd1"
      },
      "source": [
        "We'll send this message to our LLM hosted on TogetherAI.\n",
        "\n",
        "Because we have tools provided by Toolhouse we can skip having to code our own Search X parser.\n",
        "The tools are hosted and maintained by the Toolhouse team and are optimized for LLM usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "08148e22",
      "metadata": {
        "id": "08148e22",
        "outputId": "a9ed0185-7edf-4918-99ed-1c3cbc2d7715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLM autonomously decides to use the following tool(s) to achieve its goal of finding data on X/Twitter\n",
            "Tool selected:  search_x\n"
          ]
        }
      ],
      "source": [
        "# let's pull tools from Toolhouse\n",
        "tools = th.get_tools(bundle=\"togethertoolhouse\")\n",
        "# change the value of the description field, when it's 'image_generation_flux'\n",
        "# this is for compatibility reasons\n",
        "custom_tools = [t for t in tools if t['function']['name'] == 'image_generation_flux']\n",
        "custom_tools[0]['function']['description'] = \"an image generation function\"\n",
        "\n",
        "# add the value back into the tools array overwriting the previous\n",
        "tools = [t if t['function']['name'] != 'image_generation_flux' else custom_tools[0] for t in tools]\n",
        "\n",
        "# now that we have fixed the tools array we can continue\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=messages,\n",
        "    tool_choice=\"auto\",\n",
        "    tools=tools\n",
        ")\n",
        "print(\"The LLM autonomously decides to use the following tool(s) to achieve its goal of finding data on X/Twitter\")\n",
        "print(\"Tool selected: \", response.choices[0].message.tool_calls[0].function.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f279224f",
      "metadata": {
        "id": "f279224f"
      },
      "source": [
        "As you can see from the output above, the LLM properly identified that we'd like to invoke the 'search_x' tool\n",
        "\n",
        "You can see the arguments it generated by running the code below, it will break iut down by ID, type and function arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "8505b93d",
      "metadata": {
        "id": "8505b93d",
        "outputId": "ae0b04c6-0751-4b07-9c07-cd1f36d69c6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID: call_ac60hxcgoynrmr6cp1dmyj57\n",
            "Type: function\n",
            "Function: Function(arguments='{\"query\":\"from:togethercompute\",\"max_results\":3}', name='search_x')\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tools_called = response.choices[0].message.tool_calls\n",
        "for tool_called in tools_called:\n",
        "    print(f\"ID: {tool_called.id}\")\n",
        "    print(f\"Type: {tool_called.type}\")\n",
        "    print(f\"Function: {tool_called.function}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13ddcaf",
      "metadata": {
        "id": "e13ddcaf"
      },
      "source": [
        "# 4. Execute Tool Call"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a0978f",
      "metadata": {
        "id": "a0a0978f"
      },
      "source": [
        "Now that we learned how the LLM can generate arguments to pass to a function we'll have to **run the function with those arguments**.\n",
        "\n",
        "The tool gets executed via the `run_tools` command, with the parameters that were identified in the previous LLM call.\n",
        "\n",
        "\n",
        "After that we will get the result, and append it to the context, the `messages` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "355f0f50",
      "metadata": {
        "id": "355f0f50",
        "outputId": "6b51328f-5bbd-42dc-fb6a-8191e4ac158e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'user', 'content': 'Find the last three messages from the account named @togethercompute on X/Twitter and summarize them in one sentence. Make it funny!'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'id': 'call_bvvlys5wgdii8wsbs1xonacx', 'function': {'arguments': '{\"query\":\"from:togethercompute\",\"max_results\":3}', 'name': 'search_x'}, 'type': 'function', 'index': 0}]}, {'role': 'tool', 'tool_call_id': 'call_bvvlys5wgdii8wsbs1xonacx', 'name': 'search_x', 'content': \"<tweet>https://t.co/bcRWhpw9Wd\\nby @togethercompute at 2024-12-19T17:54:02.000Z\\n Conversation ID: 1869803389053521926\\n\\n![github.com/togethercomput…](https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning%26Inference.ipynb\\n<tweet>\\n\\n<tweet>New Cookbook: LoRA Fine-tuning and Inference!\\n\\nLearn how to use LoRA to fine-tune OS LLMs and then perform inference swapping in multiple LoRA adapters!\\n\\nLink below: https://t.co/D5XGITE7FI\\nby @togethercompute at 2024-12-19T17:54:01.000Z\\n Conversation ID: 1869803389053521926\\n\\n![x.com/togethercomput…](https://twitter.com/togethercompute/status/1869465912459604272\\n<tweet>\\n\\n<tweet>Another cool AI app powered by Together AI! https://t.co/uBR2KkbEkg\\nby @togethercompute at 2024-12-19T15:01:58.000Z\\n Conversation ID: 1869760088573121010\\n\\n![x.com/adamuMdankore/…](https://twitter.com/adamuMdankore/status/1869513587699687565\\n<tweet>\\n\\n<tweet>🚀 Don’t miss tomorrow's webinar: Acceptance Testing &amp; Validating Machine Learning Hardware by @RyanLucchese!\\n\\n📅 Thu, 12/19, 9 AM PT\\n💡 Learn to validate GPUs, storage, &amp; networks for peak performance.\\n\\n🔗 RSVP now to secure your spot: https://t.co/Td41XbvmG4\\nby @togethercompute at 2024-12-18T21:38:52.000Z\\n Conversation ID: 1869497583342915943\\n\\n![lu.ma/2lc44kwo](https://lu.ma/2lc44kwo\\n<tweet>\\n\\n<tweet>Get started today: Upload any LoRA adapter (for example from Hugging Face) or create custom ones with our Fine-tuning API.\\n\\nYour LoRA fine-tuned models are automatically deployed for testing in our playground or via API.\\n\\nRead the docs: https://t.co/hP4Iin1m7w https://t.co/boo9N68lt1\\nby @togethercompute at 2024-12-18T19:33:02.000Z\\n Conversation ID: 1869465912459604272\\n\\n![docs.together.ai/docs/lora-infe…](https://docs.together.ai/docs/lora-inference\\n\\n(Animated GIF)[https://pbs.twimg.com/tweet_video_thumb/GfGtOufacAAGzx-.jpg]\\nSources:\\n [video/mp4](https://video.twimg.com/tweet_video/GfGtOufacAAGzx-.mp4)<tweet>\\n\\n<tweet>While running LoRA adapters dynamically at run-time typically adds some inference overhead, we've worked hard to optimize performance: achieving 90% of base model performance through Cross-LoRA Continuous Batching & Adapter Prefetching.\\n\\nThis eliminates the trade-off between cost and speed for your fine-tuned models.\\nby @togethercompute at 2024-12-18T19:33:02.000Z\\n Conversation ID: 1869465912459604272\\n\\n\\n<tweet>\\n\\n<tweet>Multi-LoRA enables serving hundreds of custom adapters alongside one base model, dramatically reducing the cost and complexity of managing multiple fine-tuned models. Perfect for:\\n\\n- Deploying task-specific models at scale e.g. translation, summarization etc\\n- A/B testing different fine-tunes\\n- Rapid experimentation\\n\\nRead more: https://t.co/952rA0kPTH\\nby @togethercompute at 2024-12-18T19:33:01.000Z\\n Conversation ID: 1869465912459604272\\n\\n![together.ai/blog/serverles…](http://www.together.ai/blog/serverless-multi-lora-fine-tune-and-deploy-hundreds-of-adapters-for-model-customization-at-scale\\nPhoto: ![Image](https://pbs.twimg.com/media/GfGtBoeasAA6ATf.jpg)<tweet>\\n\\n<tweet>🚀Announcing Serverless Multi-LoRA: Deploy hundreds of model adapters while paying only base model per-token prices. Starting today you can upload existing LoRA adapters (e.g. from Hugging Face) or fine-tune custom ones, and try them immediately on our serverless inference.\\n\\nSupported on leading models like Llama 3.1 and Qwen 2.5.\\nby @togethercompute at 2024-12-18T19:33:01.000Z\\n Conversation ID: 1869465912459604272\\n\\n\\nPhoto: ![Image](https://pbs.twimg.com/media/GfGsxQtbgAAh2sA.jpg)<tweet>\\n\"}]\n"
          ]
        }
      ],
      "source": [
        "tool_run = th.run_tools(response)\n",
        "messages += tool_run\n",
        "\n",
        "# Here's what the messages variable contains now\n",
        "print(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what our messages list looks like:"
      ],
      "metadata": {
        "id": "aOTqRaFe7xMa"
      },
      "id": "aOTqRaFe7xMa"
    },
    {
      "cell_type": "code",
      "source": [
        "for message in tool_run:\n",
        "  print(f\"Role: {message['role']}\")\n",
        "  if 'tool_calls' in message:\n",
        "    print(f\"Tool Calls: {message['tool_calls']}\")\n",
        "  if 'tool_call_id' in message:\n",
        "    print(f\"Tool Call ID: {message['tool_call_id']}\")\n",
        "    print(f\"Content: {message['content']}\")\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "ZqeSLyYhJRMc",
        "outputId": "3ca504ea-fa9b-4a0a-cb03-62e9d906e717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZqeSLyYhJRMc",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Role: assistant\n",
            "Tool Calls: [{'id': 'call_bvvlys5wgdii8wsbs1xonacx', 'function': {'arguments': '{\"query\":\"from:togethercompute\",\"max_results\":3}', 'name': 'search_x'}, 'type': 'function', 'index': 0}]\n",
            "Role: tool\n",
            "Tool Call ID: call_bvvlys5wgdii8wsbs1xonacx\n",
            "Content: <tweet>https://t.co/bcRWhpw9Wd\n",
            "by @togethercompute at 2024-12-19T17:54:02.000Z\n",
            " Conversation ID: 1869803389053521926\n",
            "\n",
            "![github.com/togethercomput…](https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning%26Inference.ipynb\n",
            "<tweet>\n",
            "\n",
            "<tweet>New Cookbook: LoRA Fine-tuning and Inference!\n",
            "\n",
            "Learn how to use LoRA to fine-tune OS LLMs and then perform inference swapping in multiple LoRA adapters!\n",
            "\n",
            "Link below: https://t.co/D5XGITE7FI\n",
            "by @togethercompute at 2024-12-19T17:54:01.000Z\n",
            " Conversation ID: 1869803389053521926\n",
            "\n",
            "![x.com/togethercomput…](https://twitter.com/togethercompute/status/1869465912459604272\n",
            "<tweet>\n",
            "\n",
            "<tweet>Another cool AI app powered by Together AI! https://t.co/uBR2KkbEkg\n",
            "by @togethercompute at 2024-12-19T15:01:58.000Z\n",
            " Conversation ID: 1869760088573121010\n",
            "\n",
            "![x.com/adamuMdankore/…](https://twitter.com/adamuMdankore/status/1869513587699687565\n",
            "<tweet>\n",
            "\n",
            "<tweet>🚀 Don’t miss tomorrow's webinar: Acceptance Testing &amp; Validating Machine Learning Hardware by @RyanLucchese!\n",
            "\n",
            "📅 Thu, 12/19, 9 AM PT\n",
            "💡 Learn to validate GPUs, storage, &amp; networks for peak performance.\n",
            "\n",
            "🔗 RSVP now to secure your spot: https://t.co/Td41XbvmG4\n",
            "by @togethercompute at 2024-12-18T21:38:52.000Z\n",
            " Conversation ID: 1869497583342915943\n",
            "\n",
            "![lu.ma/2lc44kwo](https://lu.ma/2lc44kwo\n",
            "<tweet>\n",
            "\n",
            "<tweet>Get started today: Upload any LoRA adapter (for example from Hugging Face) or create custom ones with our Fine-tuning API.\n",
            "\n",
            "Your LoRA fine-tuned models are automatically deployed for testing in our playground or via API.\n",
            "\n",
            "Read the docs: https://t.co/hP4Iin1m7w https://t.co/boo9N68lt1\n",
            "by @togethercompute at 2024-12-18T19:33:02.000Z\n",
            " Conversation ID: 1869465912459604272\n",
            "\n",
            "![docs.together.ai/docs/lora-infe…](https://docs.together.ai/docs/lora-inference\n",
            "\n",
            "(Animated GIF)[https://pbs.twimg.com/tweet_video_thumb/GfGtOufacAAGzx-.jpg]\n",
            "Sources:\n",
            " [video/mp4](https://video.twimg.com/tweet_video/GfGtOufacAAGzx-.mp4)<tweet>\n",
            "\n",
            "<tweet>While running LoRA adapters dynamically at run-time typically adds some inference overhead, we've worked hard to optimize performance: achieving 90% of base model performance through Cross-LoRA Continuous Batching & Adapter Prefetching.\n",
            "\n",
            "This eliminates the trade-off between cost and speed for your fine-tuned models.\n",
            "by @togethercompute at 2024-12-18T19:33:02.000Z\n",
            " Conversation ID: 1869465912459604272\n",
            "\n",
            "\n",
            "<tweet>\n",
            "\n",
            "<tweet>Multi-LoRA enables serving hundreds of custom adapters alongside one base model, dramatically reducing the cost and complexity of managing multiple fine-tuned models. Perfect for:\n",
            "\n",
            "- Deploying task-specific models at scale e.g. translation, summarization etc\n",
            "- A/B testing different fine-tunes\n",
            "- Rapid experimentation\n",
            "\n",
            "Read more: https://t.co/952rA0kPTH\n",
            "by @togethercompute at 2024-12-18T19:33:01.000Z\n",
            " Conversation ID: 1869465912459604272\n",
            "\n",
            "![together.ai/blog/serverles…](http://www.together.ai/blog/serverless-multi-lora-fine-tune-and-deploy-hundreds-of-adapters-for-model-customization-at-scale\n",
            "Photo: ![Image](https://pbs.twimg.com/media/GfGtBoeasAA6ATf.jpg)<tweet>\n",
            "\n",
            "<tweet>🚀Announcing Serverless Multi-LoRA: Deploy hundreds of model adapters while paying only base model per-token prices. Starting today you can upload existing LoRA adapters (e.g. from Hugging Face) or fine-tune custom ones, and try them immediately on our serverless inference.\n",
            "\n",
            "Supported on leading models like Llama 3.1 and Qwen 2.5.\n",
            "by @togethercompute at 2024-12-18T19:33:01.000Z\n",
            " Conversation ID: 1869465912459604272\n",
            "\n",
            "\n",
            "Photo: ![Image](https://pbs.twimg.com/media/GfGsxQtbgAAh2sA.jpg)<tweet>\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84ceb341",
      "metadata": {
        "id": "84ceb341"
      },
      "source": [
        "We're now going to pass the whole message chain to the LLM. The LLM will generate a proper response based on our initial prompt:\n",
        "\n",
        "```\n",
        "Find the last three messages from the account named @togethercompute on X/Twitter and summarize them in one sentence. Make it funny!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 The Funny Summary"
      ],
      "metadata": {
        "id": "8rJ0agFQ-v6a"
      },
      "id": "8rJ0agFQ-v6a"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "84f24416",
      "metadata": {
        "id": "84f24416",
        "outputId": "2b35647f-9611-4591-d324-4e6894fc5b09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM RESPONSE:\n",
            "The last three messages from @togethercompute on X/Twitter are about their new cookbook on LoRA fine-tuning and inference, another cool AI app powered by Together AI, and a webinar on acceptance testing and validating machine learning hardware, because who doesn't love a good webinar to spice up their Thursday morning?\n"
          ]
        }
      ],
      "source": [
        "summary_response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "print('LLM RESPONSE:')\n",
        "print(summary_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add this last response - a funny summary of three posts retrieved from X - to the history of messages.\n",
        "\n",
        "To achieve this we're combining two arrays into a new variable called `new_messages`."
      ],
      "metadata": {
        "id": "1sJuKocpNd0_"
      },
      "id": "1sJuKocpNd0_"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "f3b5513b",
      "metadata": {
        "id": "f3b5513b"
      },
      "outputs": [],
      "source": [
        "new_messages = messages + [\n",
        "  {\n",
        "    \"role\": \"assistant\",\n",
        "    \"content\": summary_response.choices[0].message.content,\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we're going to give a new command to the LLM - generate us an image from this funny caption using the tool provided by Toolhouse.\n",
        "\n",
        "This tool uses Flux - an image model hosted on TogetherAI, then hosts it to a storage location on the img.toolhouse.ai subdomain.\n",
        "\n",
        "> This shows you that a tool can be used to perform multiple action at once (image generation and hosting), but it's up to you - the user and creator of tools - how to set up every tool for best agentic use.\n",
        "\n",
        "We've installed this tool in step 1."
      ],
      "metadata": {
        "id": "T9K6hrDkbbIP"
      },
      "id": "T9K6hrDkbbIP"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generate_image_request = new_messages + [\n",
        "  {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Generate an image from the funny summary and return the full URL\"\n",
        "  }\n",
        "]\n",
        "\n",
        "# Let's ask the LLM to generate the image\n",
        "image_response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=generate_image_request,\n",
        "  tools=tools,\n",
        ")\n",
        "tool_run = th.run_tools(image_response)"
      ],
      "metadata": {
        "id": "rQtHdIjObpZA"
      },
      "id": "rQtHdIjObpZA",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the LLM generated an image and stored it, we just want it to let the user know that the image is ready and can be viewed.\n",
        "\n",
        "We're going to - yet again - ask the LLM to take the output of the function call and generate a response."
      ],
      "metadata": {
        "id": "-9RI1IWZbX4I"
      },
      "id": "-9RI1IWZbX4I"
    },
    {
      "cell_type": "code",
      "source": [
        "image_messages = generate_image_request + tool_run\n",
        "\n",
        "last_response = client.chat.completions.create(\n",
        "  model=MODEL,\n",
        "  messages=image_messages,\n",
        "  tools=tools,\n",
        ")\n",
        "\n",
        "print('LLM RESPONSE:', last_response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "Gjk6rNvfXanT",
        "outputId": "293fcfb5-83c7-4600-e3ed-21f97e925685",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Gjk6rNvfXanT",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM RESPONSE: The image is available at https://img.toolhouse.ai/lsx2hf.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And there you have it, if you have managed to run all the cells in the correct order you should see a message with a valid URL. If you click it you will be able to see the generated image! The image is pretty random and truly depends on what \"funny summary\" we generated in step 4.1"
      ],
      "metadata": {
        "id": "TwDJcIG_cN46"
      },
      "id": "TwDJcIG_cN46"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}