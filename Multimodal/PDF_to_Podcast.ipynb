{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Implementation of Notebook LM's PDF to Podcast\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Multimodal/PDF_to_Podcast.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook we will see how to create a podcast like the one below from a PDF input!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\M'\n",
      "/tmp/ipykernel_2275960/3251414585.py:3: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  IPython.display.Video(\"images\\MoA_podcast.mp4\", width=500)\n",
      "/tmp/ipykernel_2275960/3251414585.py:3: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  IPython.display.Video(\"images\\MoA_podcast.mp4\", width=500)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "To embed videos, you must pass embed=True (this may make your notebook files huge)\nConsider passing Video(url='...')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example podcast\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mIPython\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisplay\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVideo\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mMoA_podcast.mp4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages/IPython/core/display.py:1217\u001b[39m, in \u001b[36mVideo.__init__\u001b[39m\u001b[34m(self, data, url, filename, embed, mimetype, width, height, html_attributes)\u001b[39m\n\u001b[32m   1211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m embed:\n\u001b[32m   1212\u001b[39m     msg = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join([\n\u001b[32m   1213\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo embed videos, you must pass embed=True \u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1214\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(this may make your notebook files huge)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1215\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConsider passing Video(url=\u001b[39m\u001b[33m'\u001b[39m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1216\u001b[39m     ])\n\u001b[32m-> \u001b[39m\u001b[32m1217\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1219\u001b[39m \u001b[38;5;28mself\u001b[39m.mimetype = mimetype\n\u001b[32m   1220\u001b[39m \u001b[38;5;28mself\u001b[39m.embed = embed\n",
      "\u001b[31mValueError\u001b[39m: To embed videos, you must pass embed=True (this may make your notebook files huge)\nConsider passing Video(url='...')"
     ]
    }
   ],
   "source": [
    "# Example podcast\n",
    "import IPython\n",
    "IPython.display.Video(\"images\\MoA_podcast.mp4\", width=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to set up the following to run the notebook in Jupyter Lab. Follow the below steps in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Step 1: Create a virtual environment\n",
    "python3.12 -m venv ~/venvs/pdf2podcast\n",
    "\n",
    "# Step 2: Activate it\n",
    "source ~/venvs/pdf2podcast/bin/activate\n",
    "\n",
    "# Step 3: Install ipykernel (and anything else you need)\n",
    "pip install ipykernel\n",
    "\n",
    "# Step 4: Register the kernel with Jupyter\n",
    "python -m ipykernel install --user --name=python312 --display-name \"Python 3.12\"\n",
    "\n",
    "# If you have compatible kernel and prefer to run in code editor, skip steps 5-7\n",
    "\n",
    "# Step 5: Install JupyterLab\n",
    "pip3 install jupyterlab\n",
    "\n",
    "# Step 6: Start JupyterLab\n",
    "jupyter lab\n",
    "\n",
    "# Step 7: Select the kernel you just registered in the JupyterLab interface (top right corner)\n",
    "\n",
    "# Other steps if you haven't done already:\n",
    "- have ffmpeg installed `brew install ffmpeg`\n",
    "- set your TOGETHER_API_KEY os env\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by [Notebook LM's](https://notebooklm.google/) podcast generation feature and a recent open source implementation of [Open Notebook LM](https://github.com/gabrielchua/open-notebooklm). In this cookbook we will implement a walkthrough of how you can build a PDF to podcast pipeline. \n",
    "\n",
    "Given any PDF we will generate a conversation between a host and a guest discussing and explaining the contents of the PDF.\n",
    "\n",
    "In doing so we will learn the following:\n",
    "1. How we can use JSON mode and structured generation with open models like Llama 3 70b to extract a script for the Podcast given text from the PDF.\n",
    "2. How we can use TTS models to bring this script to life as a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN0Tpr76ssM1",
    "outputId": "ae8e04ea-fee7-4496-f752-c8eb972e2eb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (5.4.0)\n",
      "Requirement already satisfied: pydantic in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (2.10.6)\n",
      "Requirement already satisfied: together in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: cartesia in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (1.4.0)\n",
      "Requirement already satisfied: ffmpeg-python in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: alive-progress in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: tqdm in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from pydantic) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from together) (3.11.14)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from together) (8.1.8)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from together) (0.2.2)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from together) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from together) (2.2.4)\n",
      "Requirement already satisfied: pillow<12.0.0,>=11.1.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from together) (11.1.0)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from together) (19.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from together) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.8.1 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from together) (13.9.4)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from together) (0.9.0)\n",
      "Requirement already satisfied: typer<0.16,>=0.9 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from together) (0.15.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from cartesia) (0.28.1)\n",
      "Requirement already satisfied: iterators>=0.2.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from cartesia) (0.2.0)\n",
      "Collecting pydub>=0.25.1 (from cartesia)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from cartesia) (15.0.1)\n",
      "Requirement already satisfied: future in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from ffmpeg-python) (1.0.0)\n",
      "Requirement already satisfied: about-time==4.2.1 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from alive-progress) (4.2.1)\n",
      "Requirement already satisfied: grapheme==0.6.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from alive-progress) (0.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.18.3)\n",
      "Requirement already satisfied: anyio in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from httpx>=0.27.0->cartesia) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from httpx>=0.27.0->cartesia) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from httpx>=0.27.0->cartesia) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from httpx>=0.27.0->cartesia) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/roy/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->cartesia) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->together) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->together) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from rich<14.0.0,>=13.8.1->together) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from rich<14.0.0,>=13.8.1->together) (2.19.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/roy/.local/lib/python3.12/site-packages (from typer<0.16,>=0.9->together) (1.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/roy/miniconda3/envs/pdf2podcast/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/roy/.local/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->cartesia) (1.3.1)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf pydantic together cartesia ffmpeg-python alive-progress tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iWea6go4r72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Literal\n",
    "from together import Together\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "from pydantic import ValidationError\n",
    "from tqdm import tqdm\n",
    "from alive_progress import alive_bar\n",
    "import time\n",
    "import requests\n",
    "\n",
    "import subprocess\n",
    "import ffmpeg\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7GYTmdx_s6QL"
   },
   "outputs": [],
   "source": [
    "# Paste in your Together AI API key or load it. We will also access Together's Cartesia endpoint using this\n",
    "api_key = os.getenv(\"TOGETHER_API_KEY\") #or just set api_key = \"your_api_key\"\n",
    "\n",
    "#note: you need to set this before launching the notebook. otherwise, close nb and relaunch. or just set the key here manually\n",
    "if api_key is None:\n",
    "    raise ValueError(\"TOGETHER_API_KEY is not set in your environment.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dialogue Schema with Pydantic\n",
    "\n",
    "We need a way of telling the LLM what the structure of the podcast script between the guest and host will look like. We will do this using `pydantic` models.\n",
    "\n",
    "Below we define the required classes. \n",
    "\n",
    "- The overall conversation consists of lines said by either the host or the guest. The `DialogueItem` class specifies the structure of these lines.\n",
    "- The full script is a combination of multiple lines performed by the speakers, here we also include a scratchpad field to allow the LLM to ideate and brainstorm the overall flow of the script prior to actually generating the lines. The `Dialogue` class specifies this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zYOq3bdntLgl"
   },
   "outputs": [],
   "source": [
    "class LineItem(BaseModel):\n",
    "    speaker: Literal[\"Host (Jane)\", \"Guest\"]\n",
    "    text: str\n",
    "\n",
    "class Script(BaseModel):\n",
    "    scratchpad: str\n",
    "    name_of_guest: str\n",
    "    script: List[LineItem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6ZzYFsNXuDN0"
   },
   "outputs": [],
   "source": [
    "# Adapted and modified from https://github.com/gabrielchua/open-notebooklm\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a world-class podcast producer tasked with transforming the provided input text into an engaging and informative podcast script. The input may be unstructured or messy, sourced from PDFs or web pages. Your goal is to extract the most interesting and insightful content for a compelling podcast discussion.\n",
    "\n",
    "# Steps to Follow:\n",
    "\n",
    "1. **Analyze the Input:**\n",
    "   Carefully examine the text, identifying key topics, points, and interesting facts or anecdotes that could drive an engaging podcast conversation. Disregard irrelevant information or formatting issues.\n",
    "\n",
    "2. **Brainstorm Ideas:**\n",
    "   In the `<scratchpad>`, creatively brainstorm ways to present the key points engagingly. Consider:\n",
    "   - Analogies, storytelling techniques, or hypothetical scenarios to make content relatable\n",
    "   - Ways to make complex topics accessible to a general audience\n",
    "   - Thought-provoking questions to explore during the podcast\n",
    "   - Creative approaches to fill any gaps in the information\n",
    "\n",
    "3. **Craft the Dialogue:**\n",
    "   Develop a natural, conversational flow between the host (Jane) and the guest speaker (the author or an expert on the topic). Incorporate:\n",
    "   - The best ideas from your brainstorming session\n",
    "   - Clear explanations of complex topics\n",
    "   - An engaging and lively tone to captivate listeners\n",
    "   - A balance of information and entertainment\n",
    "\n",
    "   Rules for the dialogue:\n",
    "   - The host (Jane) always initiates the conversation and interviews the guest\n",
    "   - Include thoughtful questions from the host to guide the discussion\n",
    "   - Incorporate natural speech patterns, including occasional verbal fillers (e.g., \"Uhh\", \"Hmmm\", \"um,\" \"well,\" \"you know\")\n",
    "   - Allow for natural interruptions and back-and-forth between host and guest - this is very important to make the conversation feel authentic\n",
    "   - Ensure the guest's responses are substantiated by the input text, avoiding unsupported claims\n",
    "   - Maintain a PG-rated conversation appropriate for all audiences\n",
    "   - Avoid any marketing or self-promotional content from the guest\n",
    "   - The host concludes the conversation\n",
    "\n",
    "4. **Summarize Key Insights:**\n",
    "   Naturally weave a summary of key points into the closing part of the dialogue. This should feel like a casual conversation rather than a formal recap, reinforcing the main takeaways before signing off.\n",
    "\n",
    "5. **Maintain Authenticity:**\n",
    "   Throughout the script, strive for authenticity in the conversation. Include:\n",
    "   - Moments of genuine curiosity or surprise from the host\n",
    "   - Instances where the guest might briefly struggle to articulate a complex idea\n",
    "   - Light-hearted moments or humor when appropriate\n",
    "   - Brief personal anecdotes or examples that relate to the topic (within the bounds of the input text)\n",
    "\n",
    "6. **Consider Pacing and Structure:**\n",
    "   Ensure the dialogue has a natural ebb and flow:\n",
    "   - Start with a strong hook to grab the listener's attention\n",
    "   - Gradually build complexity as the conversation progresses\n",
    "   - Include brief \"breather\" moments for listeners to absorb complex information\n",
    "   - For complicated concepts, reasking similar questions framed from a different perspective is recommended\n",
    "   - End on a high note, perhaps with a thought-provoking question or a call-to-action for listeners\n",
    "\n",
    "IMPORTANT RULE: Each line of dialogue should be no more than 100 characters (e.g., can finish within 5-8 seconds)\n",
    "\n",
    "Remember: Always reply in valid JSON format, without code blocks. Begin directly with the JSON output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the LLM to Generate Podcast Script\n",
    "\n",
    "Below we call `Llama-3.1-70B` to generate a script for our podcast. We will also be able to read it's `scratchpad` and see how it structured the overall conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Y0RtJZ9VtVut"
   },
   "outputs": [],
   "source": [
    "client = Together(api_key=api_key) #set above \n",
    "\n",
    "def call_llm(system_prompt: str, text: str, schema_class):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "        response_format={\n",
    "            \"type\": \"json_object\",\n",
    "            \"schema\": schema_class.model_json_schema(),\n",
    "        },\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FvW4J7W3tOow"
   },
   "outputs": [],
   "source": [
    "def generate_script(system_prompt: str, input_text: str, output_model):\n",
    "    \"\"\"Get the dialogue from the LLM.\"\"\"\n",
    "    # Load as python object\n",
    "    try:\n",
    "        response = call_llm(system_prompt, input_text, output_model)\n",
    "        dialogue = output_model.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        )\n",
    "    except ValidationError as e:\n",
    "        error_message = f\"Failed to parse dialogue JSON: {e}\"\n",
    "        system_prompt_with_error = f\"{system_prompt}\\n\\nPlease return a VALID JSON object. This was the earlier error: {error_message}\"\n",
    "        response = call_llm(system_prompt_with_error, input_text, output_model)\n",
    "        dialogue = output_model.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        )\n",
    "    return dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in PDF of Choice\n",
    "\n",
    "Here we will load in an academic paper that proposes the use of many open source language models in a collaborative manner together to outperform proprietary models that are much larger!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c2nbb7Hu2jV",
    "outputId": "8b2cfe7e-3b3d-41d6-c8a2-edfa497c0b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1130k  100 1130k    0     0  5733k      0 --:--:-- --:--:-- --:--:-- 5737k\n"
     ]
    }
   ],
   "source": [
    "#https://arxiv.org/abs/2406.04692\n",
    "\n",
    "# !wget https://arxiv.org/pdf/2406.04692\n",
    "# !mv 2406.04692 MoA.pdf\n",
    "# use above if you prefer wget\n",
    "!curl -L https://arxiv.org/pdf/2406.04692 -o MoA.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Rn-lhgqmueWM"
   },
   "outputs": [],
   "source": [
    "def get_PDF_text(file : str):\n",
    "    text = ''\n",
    "\n",
    "    # Read the PDF file and extract text\n",
    "    try:\n",
    "        with Path(file).open(\"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            text = \"\\n\\n\".join([page.extract_text() for page in reader.pages])\n",
    "    except Exception as e:\n",
    "        raise f\"Error reading the PDF file: {str(e)}\"\n",
    "\n",
    "        # Check if the PDF has more than ~131,072 characters\n",
    "        # The context lenght limit of the model is 131,072 tokens and thus the text should be less than this limit\n",
    "    if len(text) > 131072:\n",
    "        raise \"The PDF is too long. Please upload a PDF with fewer than ~131072 characters.\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "D9BzDxmgvS2V",
    "outputId": "8fbd8676-d4d9-4760-9f16-ba00dcff66df"
   },
   "outputs": [],
   "source": [
    "# Helper Functions - PDF processing. Conversts a pdf into text!\n",
    "def get_pdf_text(file_path: str) -> str:\n",
    "    with Path(file_path).open(\"rb\") as f:\n",
    "        reader = PdfReader(f)\n",
    "        text = \"\\n\\n\".join([page.extract_text() or '' for page in reader.pages])\n",
    "    if len(text) > 400000:\n",
    "        raise ValueError(\"PDF is too long\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Script\n",
    "\n",
    "Below we generate the script and print out the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "f5rBXur8vXnP"
   },
   "outputs": [],
   "source": [
    "# we use alive bar to show a progress bar and it looks nice!\n",
    "def generate_script(system_prompt, input_text, schema_class):\n",
    "    try:\n",
    "        with alive_bar(spinner='dots_waves', title='Generating podcast script...') as bar:\n",
    "            response = call_llm(system_prompt, input_text, schema_class)\n",
    "            parsed = schema_class.model_validate_json(response.choices[0].message.content)\n",
    "    except ValidationError as e:\n",
    "        print(\"Retrying due to validation error:\", e)\n",
    "        system_prompt += f\"\\n\\nPrevious error: {e}\"\n",
    "        with alive_bar(spinner='dots_waves', title='Retrying generation...') as bar:\n",
    "            response = call_llm(system_prompt, input_text, schema_class)\n",
    "            parsed = schema_class.model_validate_json(response.choices[0].message.content)\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inFgEVeBtCOR",
    "outputId": "f290d90b-cc5a-409d-b72c-8a2e947038c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating podcast script... |████████████████████████████████████████| 0 in 25.8s (0.00/s) \n",
      "Host (Jane): Welcome to our podcast, where we explore the world of artificial intelligence and its applications. Today, we have Junlin Wang, a researcher from Duke University, who has made significant contributions to the field of large language models. Junlin, welcome to the show!\n",
      "Guest: Thank you, Jane, for having me. I'm excited to share our research on large language models and how we can harness their collective strengths to improve their capabilities.\n",
      "Host (Jane): Let's dive right in. Your research proposes a new approach called Mixture-of-Agents, or MoA. Can you explain what that is and how it works?\n",
      "Guest: MoA is a methodology that leverages the collective strengths of multiple large language models to improve their reasoning and language generation capabilities. We construct a layered MoA architecture, where each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response.\n",
      "Host (Jane): That's fascinating. So, you're essentially creating a collaborative environment where multiple models can work together to generate better responses. How did you come up with this idea?\n",
      "Guest: We discovered that LLMs possess an inherent collaborativeness, where they tend to generate better responses when presented with outputs from other models, even if those outputs are of lower quality. This led us to design MoA, which iteratively enhances the generation quality by leveraging the strengths of multiple models.\n",
      "Host (Jane): I see. And what are the benefits of using MoA compared to traditional large language models?\n",
      "Guest: MoA achieves state-of-the-art performance on various benchmarks, including AlpacaEval 2.0, MT-Bench, and FLASK. Moreover, our approach is cost-effective and can be applied to the latest LLMs regardless of their size or architecture.\n",
      "Host (Jane): That's impressive. Can you walk us through some of the experiments you conducted to evaluate MoA's performance?\n",
      "Guest: We conducted comprehensive evaluations using AlpacaEval 2.0, MT-Bench, and FLASK benchmarks. Our results demonstrate substantial improvements with MoA, achieving a new SOTA win rate of 65.8% on AlpacaEval 2.0 compared to the previous best of 57.5% achieved by GPT-4 Omni.\n",
      "Host (Jane): Wow, those are significant improvements. What do you think are the limitations of MoA, and how do you plan to address them in future research?\n",
      "Guest: One limitation of MoA is that it requires iterative aggregation of model responses, which can result in a high Time to First Token. To mitigate this, we can limit the number of MoA layers or explore chunk-wise aggregation instead of aggregating entire responses at once.\n",
      "Host (Jane): I see. And finally, what are the broader implications of your research, and how do you think MoA can be applied in real-world scenarios?\n",
      "Guest: MoA has the potential to enhance the effectiveness of LLM-driven chat assistants, making AI more accessible. Moreover, since the intermediate outputs are expressed in natural language, MoA improves the interpretability of models, facilitating better alignment with human reasoning.\n",
      "Host (Jane): Thank you, Junlin, for sharing your research with us today. It's been enlightening to learn about MoA and its potential applications.\n",
      "Guest: Thank you, Jane, for having me. It was a pleasure to discuss our research and its implications.\n"
     ]
    }
   ],
   "source": [
    "filepath = 'MoA.pdf' #whatever pdf path you want to use\n",
    "pdf_text = get_pdf_text(filepath)\n",
    "script = generate_script(SYSTEM_PROMPT, pdf_text, Script)\n",
    "# Print the script\n",
    "for line in script.script:\n",
    "    print(f\"{line.speaker}: {line.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Podcast Using TTS\n",
    "\n",
    "Below we read through the script and parse choose the TTS voice depending on the speaker. We define a speaker and guest voice id.\n",
    "\n",
    "We can loop through the lines in the script and generate them by a call to the TTS model with specific voice and lines configurations. The lines all appended to the same buffer and once the script finishes we write this out to a `wav` file, ready to be played.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for actually generating the audio... saves as a bunch of smaller files then concats \n",
    "def generate_audio_with_together(text, voice, model_id):\n",
    "    api_key = os.getenv(\"TOGETHER_API_KEY\")\n",
    "    if api_key is None:\n",
    "        raise ValueError(\"TOGETHER_API_KEY is not set in your environment.\")\n",
    "    \n",
    "    url = \"https://api.together.ai/v1/audio/generations\"\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    \n",
    "    data = {\n",
    "        \"input\": text,\n",
    "        \"voice\": voice,\n",
    "        \"response_format\": \"wav\",  # Changed from mp3 to wav\n",
    "        \"sample_rate\": 44100,\n",
    "        \"stream\": False,\n",
    "        \"model\": model_id,\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        raise Exception(f\"API request failed: {response.text}\")\n",
    "        \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKnQnoYNvx3k",
    "outputId": "f1749a36-6d5d-4d6e-8849-703555b71ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting audio generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating audio: 100%|██████████| 16/16 [01:48<00:00,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podcast generated successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ffmpeg version 9c33b2f Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/home/roy/miniconda3/envs/pdf2podcast --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "[wav @ 0x563e2600dfc0] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2600dfc0] Estimating duration from bitrate, this may be inaccurate\n",
      "Input #0, concat, from 'concat_list.txt':\n",
      "  Duration: N/A, start: 0.000000, bitrate: 1411 kb/s\n",
      "    Stream #0:0: Audio: pcm_f32le ([3][0][0][0] / 0x0003), 44100 Hz, mono, flt, 1411 kb/s\n",
      "Output #0, wav, to 'podcast.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf58.45.100\n",
      "    Stream #0:0: Audio: pcm_f32le ([3][0][0][0] / 0x0003), 44100 Hz, mono, flt, 1411 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (copy)\n",
      "Press [q] to stop, [?] for help\n",
      "[wav @ 0x563e2600e980] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2600e980] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e26030680] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e26030680] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e26030680] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e26030680] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e2602f300] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2602f300] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e2603fe00] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2603fe00] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e2603fe00] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2603fe00] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e2603fe00] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2603fe00] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e2603fe00] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2603fe00] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e2603fe00] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2603fe00] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e2603fe00] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2603fe00] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e2603fe00] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2603fe00] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e2603fe00] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2603fe00] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e2603fe00] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2603fe00] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e2603fe00] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2603fe00] Estimating duration from bitrate, this may be inaccurate\n",
      "[wav @ 0x563e2603fe00] Ignoring maximum wav data size, file may be invalid\n",
      "[wav @ 0x563e2603fe00] Estimating duration from bitrate, this may be inaccurate\n",
      "size=   33624kB time=00:03:15.16 bitrate=1411.4kbits/s speed=4e+03x    \n",
      "video:0kB audio:33624kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000331%\n"
     ]
    }
   ],
   "source": [
    "host_id = \"wise man\" # host ... look at together api for other voices\n",
    "guest_id = \"sarah\" # Guest voice ... look at together api for other voices\n",
    "model_id = \"cartesia/sonic-2\" # look at together api for other models\n",
    "print(\"Starting audio generation...\")\n",
    "audio_files = []\n",
    "\n",
    "for i, line in enumerate(tqdm(script.script, desc=\"Generating audio\")):\n",
    "    voice = host_id if line.speaker == \"Host (Jane)\" else guest_id\n",
    "    \n",
    "    audio_content = generate_audio_with_together(line.text, voice, model_id)\n",
    "    \n",
    "    # save each line as separate wav file\n",
    "    filename = f\"temp_audio_{i}.wav\"  # Changed extension to .wav\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(audio_content)\n",
    "    audio_files.append(filename)\n",
    "\n",
    "concat_file = \"concat_list.txt\"\n",
    "with open(concat_file, \"w\") as f:\n",
    "    for file in audio_files:\n",
    "        f.write(f\"file '{file}'\\n\")\n",
    "\n",
    "# Save as wav \n",
    "subprocess.run([\n",
    "    \"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\",\n",
    "    \"-i\", \"concat_list.txt\", \"-c\", \"copy\", \"podcast.wav\"\n",
    "])\n",
    "\n",
    "# Clean up temp files\n",
    "for file in audio_files:\n",
    "    os.remove(file)\n",
    "os.remove(concat_file)\n",
    "print(\"Podcast generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: There are many options for playing the audio file. Here we use `pygame` to play the file since it avoids a lot of compiling and installation issues. Make sure your system also has an audio output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "STWaJf_ySctY",
    "outputId": "8abab8cd-a1a0-4a03-8f96-27c197e947ac"
   },
   "outputs": [],
   "source": [
    "# Play the podcast \n",
    "# Play audio (optional - only if on host device) -> if your jupyter kernel is being weird you can run this script locally\n",
    "play_audio = True #set true if you want to play the audio\n",
    "if play_audio:\n",
    "    try:\n",
    "        import pygame\n",
    "        pygame.init()\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(\"podcast.wav\")\n",
    "        pygame.mixer.music.play()\n",
    "\n",
    "        # Wait until playback finishes\n",
    "        while pygame.mixer.music.get_busy():\n",
    "            pygame.time.Clock().tick(10)\n",
    "        print(\"Podcast finished playing...\")\n",
    "    except ImportError:\n",
    "        print(\"pygame not available - try another audio output method\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pdf2podcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
