{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3D Grounding with Qwen3-VL (Together AI)\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Multimodal/Vision/3D_Grounding.ipynb)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we'll explore Qwen3-VL's 3D spatial understanding capabilities using Together AI's API. We'll cover:\n",
        "\n",
        "1. Detecting objects with 3D bounding boxes\n",
        "2. Using camera parameters for accurate projections\n",
        "3. Multi-object 3D localization\n",
        "\n",
        "3D bounding boxes are represented as: `[x_center, y_center, z_center, x_size, y_size, z_size, roll, pitch, yaw]`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openai pillow numpy matplotlib opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cv2'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mbase64\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import base64\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import openai\n",
        "from PIL import Image\n",
        "\n",
        "# Together AI Configuration\n",
        "client = openai.OpenAI(\n",
        "    api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
        "    base_url=\"https://api.together.xyz/v1\",\n",
        ")\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen3-VL-32B-Instruct\"\n",
        "\n",
        "print(f\"Using model: {MODEL_ID}\")\n",
        "print(f\"API Key configured: {bool(os.environ.get('TOGETHER_API_KEY'))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "\n",
        "def encode_image(image_path):\n",
        "    \"\"\"Encode image to base64.\"\"\"\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "def inference_with_api(image_path, prompt, max_tokens=4096):\n",
        "    \"\"\"Run inference with Together AI API.\"\"\"\n",
        "    base64_image = encode_image(image_path)\n",
        "    ext = image_path.split(\".\")[-1].lower()\n",
        "    mime_type = \"jpeg\" if ext in [\"jpg\", \"jpeg\"] else ext\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_ID,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/{mime_type};base64,{base64_image}\"}},\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ],\n",
        "        }],\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def parse_bbox_3d_from_text(text):\n",
        "    \"\"\"Parse 3D bounding box information from assistant response.\"\"\"\n",
        "    try:\n",
        "        if \"```json\" in text:\n",
        "            start_idx = text.find(\"```json\")\n",
        "            end_idx = text.find(\"```\", start_idx + 7)\n",
        "            json_str = text[start_idx + 7:end_idx].strip() if end_idx != -1 else text[start_idx + 7:].strip()\n",
        "        else:\n",
        "            start_idx = text.find('[')\n",
        "            end_idx = text.rfind(']')\n",
        "            json_str = text[start_idx:end_idx + 1] if start_idx != -1 and end_idx != -1 else \"\"\n",
        "        \n",
        "        bbox_data = json.loads(json_str)\n",
        "        return bbox_data if isinstance(bbox_data, list) else [bbox_data]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def convert_3dbbox(point, cam_params):\n",
        "    \"\"\"Convert 3D bounding box to 2D image coordinates.\"\"\"\n",
        "    x, y, z, x_size, y_size, z_size, pitch, yaw, roll = point\n",
        "    hx, hy, hz = x_size / 2, y_size / 2, z_size / 2\n",
        "    local_corners = [\n",
        "        [hx, hy, hz], [hx, hy, -hz], [hx, -hy, hz], [hx, -hy, -hz],\n",
        "        [-hx, hy, hz], [-hx, hy, -hz], [-hx, -hy, hz], [-hx, -hy, -hz]\n",
        "    ]\n",
        "\n",
        "    def rotate_xyz(pt, _pitch, _yaw, _roll):\n",
        "        x0, y0, z0 = pt\n",
        "        x1, y1 = x0, y0 * math.cos(_pitch) - z0 * math.sin(_pitch)\n",
        "        z1 = y0 * math.sin(_pitch) + z0 * math.cos(_pitch)\n",
        "        x2 = x1 * math.cos(_yaw) + z1 * math.sin(_yaw)\n",
        "        y2, z2 = y1, -x1 * math.sin(_yaw) + z1 * math.cos(_yaw)\n",
        "        x3 = x2 * math.cos(_roll) - y2 * math.sin(_roll)\n",
        "        y3, z3 = x2 * math.sin(_roll) + y2 * math.cos(_roll), z2\n",
        "        return [x3, y3, z3]\n",
        "    \n",
        "    img_corners = []\n",
        "    for corner in local_corners:\n",
        "        rotated = rotate_xyz(corner, np.deg2rad(pitch), np.deg2rad(yaw), np.deg2rad(roll))\n",
        "        X, Y, Z = rotated[0] + x, rotated[1] + y, rotated[2] + z\n",
        "        if Z > 0:\n",
        "            x_2d = cam_params['fx'] * (X / Z) + cam_params['cx']\n",
        "            y_2d = cam_params['fy'] * (Y / Z) + cam_params['cy']\n",
        "            img_corners.append([x_2d, y_2d])\n",
        "    return img_corners\n",
        "\n",
        "def draw_3dbboxes(image_path, cam_params, bbox_3d_list):\n",
        "    \"\"\"Draw multiple 3D bounding boxes on the image.\"\"\"\n",
        "    annotated_image = cv2.imread(image_path)\n",
        "    if annotated_image is None:\n",
        "        print(f\"Error reading image: {image_path}\")\n",
        "        return None\n",
        "\n",
        "    edges = [[0,1], [2,3], [4,5], [6,7], [0,2], [1,3], [4,6], [5,7], [0,4], [1,5], [2,6], [3,7]]\n",
        "    \n",
        "    for bbox_data in bbox_3d_list:\n",
        "        bbox_3d = bbox_data['bbox_3d'] if isinstance(bbox_data, dict) and 'bbox_3d' in bbox_data else bbox_data\n",
        "        bbox_3d = list(bbox_3d)\n",
        "        bbox_3d[-3:] = [_x * 180 for _x in bbox_3d[-3:]]\n",
        "        bbox_2d = convert_3dbbox(bbox_3d, cam_params)\n",
        "\n",
        "        if len(bbox_2d) >= 8:\n",
        "            box_color = [random.randint(0, 255) for _ in range(3)]\n",
        "            for start, end in edges:\n",
        "                try:\n",
        "                    pt1 = tuple([int(_pt) for _pt in bbox_2d[start]])\n",
        "                    pt2 = tuple([int(_pt) for _pt in bbox_2d[end]])\n",
        "                    cv2.line(annotated_image, pt1, pt2, box_color, 2)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "    annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "    ax.imshow(annotated_image_rgb)\n",
        "    ax.axis('off')\n",
        "    return fig\n",
        "\n",
        "def load_camera_params(image_name):\n",
        "    \"\"\"Load camera parameters from JSON file.\"\"\"\n",
        "    try:\n",
        "        with open('../assets/spatial_understanding/cam_infos.json', 'r') as f:\n",
        "            cam_infos = json.load(f)\n",
        "        return cam_infos.get(image_name, None)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def generate_camera_params(image_path, fov=60):\n",
        "    \"\"\"Generate camera parameters if not available.\"\"\"\n",
        "    image = Image.open(image_path)\n",
        "    w, h = image.size\n",
        "    fx = round(w / (2 * np.tan(np.deg2rad(fov) / 2)), 2)\n",
        "    fy = round(h / (2 * np.tan(np.deg2rad(fov) / 2)), 2)\n",
        "    cx, cy = round(w / 2, 2), round(h / 2, 2)\n",
        "    return {'fx': fx, 'fy': fy, 'cx': cx, 'cy': cy}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Detect Objects of Specific Categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Detect all cars in autonomous driving scene\n",
        "image_path = \"../assets/spatial_understanding/autonomous_driving.jpg\"\n",
        "prompt = 'Find all cars in this image. For each car, provide its 3D bounding box. The output format required is JSON: `[{\"bbox_3d\":[x_center, y_center, z_center, x_size, y_size, z_size, roll, pitch, yaw],\"label\":\"category\"}]`.'\n",
        "\n",
        "cam_params = load_camera_params(\"autonomous_driving.jpg\")\n",
        "if cam_params is None:\n",
        "    cam_params = generate_camera_params(image_path)\n",
        "    print(\"Using generated camera params:\", cam_params)\n",
        "\n",
        "response = inference_with_api(image_path, prompt)\n",
        "bbox_3d_results = parse_bbox_3d_from_text(response)\n",
        "print(\"Parsed bbox_3d_results:\", bbox_3d_results)\n",
        "\n",
        "fig = draw_3dbboxes(image_path, cam_params, bbox_3d_results)\n",
        "if fig:\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Detect a Specific Object Using Descriptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Detect a specific object using descriptions\n",
        "image_path = \"../assets/spatial_understanding/office.jpg\"\n",
        "prompt = \"Locate the black chair in image and provide 3D bounding boxes results in JSON format.\"\n",
        "\n",
        "cam_params = load_camera_params(\"office.jpg\")\n",
        "if cam_params is None:\n",
        "    cam_params = generate_camera_params(image_path)\n",
        "\n",
        "response = inference_with_api(image_path, prompt)\n",
        "bbox_3d_results = parse_bbox_3d_from_text(response)\n",
        "print(\"Parsed bbox_3d_results:\", bbox_3d_results)\n",
        "\n",
        "fig = draw_3dbboxes(image_path, cam_params, bbox_3d_results)\n",
        "if fig:\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Detect Multiple Objects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Detect multiple objects simultaneously\n",
        "image_path = \"../assets/spatial_understanding/lounge.jpg\"\n",
        "prompt = 'Locate tables, chairs, and sofas in the image and output their 3D bounding boxes. Format: [{\"bbox_3d\":[x_center, y_center, z_center, x_size, y_size, z_size, roll, pitch, yaw],\"label\":\"category\"}].'\n",
        "\n",
        "cam_params = load_camera_params(\"lounge.jpg\")\n",
        "if cam_params is None:\n",
        "    cam_params = generate_camera_params(image_path)\n",
        "\n",
        "response = inference_with_api(image_path, prompt)\n",
        "bbox_3d_results = parse_bbox_3d_from_text(response)\n",
        "print(\"Parsed bbox_3d_results:\", bbox_3d_results)\n",
        "\n",
        "fig = draw_3dbboxes(image_path, cam_params, bbox_3d_results)\n",
        "if fig:\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Using Custom Camera Parameters\n",
        "\n",
        "When you don't have access to the original camera intrinsic parameters, you can generate parameters with a field of view of 60\u00b0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Using custom camera parameters\n",
        "image_path = \"../assets/spatial_understanding/manipulation.jpg\"\n",
        "prompt = 'Detect the bottle in the image and predict the 3D box. Output JSON: [{\"bbox_3d\":[x_center, y_center, z_center, x_size, y_size, z_size, roll, pitch, yaw],\"label\":\"category\"}].'\n",
        "\n",
        "# Generate camera parameters with 60\u00b0 FOV\n",
        "cam_params = generate_camera_params(image_path, fov=60)\n",
        "print(\"Generated camera params:\", cam_params)\n",
        "\n",
        "response = inference_with_api(image_path, prompt)\n",
        "bbox_3d_results = parse_bbox_3d_from_text(response)\n",
        "print(\"Parsed bbox_3d_results:\", bbox_3d_results)\n",
        "\n",
        "fig = draw_3dbboxes(image_path, cam_params, bbox_3d_results)\n",
        "if fig:\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}