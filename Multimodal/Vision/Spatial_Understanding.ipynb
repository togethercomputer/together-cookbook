{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spatial Understanding with Qwen3-VL (Together AI)\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Multimodal/Vision/Spatial_Understanding.ipynb)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we'll explore Qwen3-VL's spatial reasoning capabilities using Together AI's API. We'll cover:\n",
        "\n",
        "1. Understanding spatial relationships between objects\n",
        "2. Perceiving object affordances (what actions are possible)\n",
        "3. Integrating spatial reasoning with action planning\n",
        "\n",
        "These capabilities enable embodied AI applications like robotics and navigation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openai pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import base64\n",
        "import openai\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageColor\n",
        "from IPython.display import display\n",
        "\n",
        "# Together AI Configuration\n",
        "client = openai.OpenAI(\n",
        "    api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
        "    base_url=\"https://api.together.xyz/v1\",\n",
        ")\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen3-VL-32B-Instruct\"\n",
        "\n",
        "print(f\"Using model: {MODEL_ID}\")\n",
        "print(f\"API Key configured: {bool(os.environ.get('TOGETHER_API_KEY'))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
        "\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "def get_mime_type(image_path):\n",
        "    ext = image_path.split(\".\")[-1].lower()\n",
        "    return \"jpeg\" if ext in [\"jpg\", \"jpeg\"] else ext\n",
        "\n",
        "def inference_with_api(image_path, prompt, max_tokens=4096):\n",
        "    base64_image = encode_image(image_path)\n",
        "    mime_type = get_mime_type(image_path)\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_ID,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/{mime_type};base64,{base64_image}\"}},\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ],\n",
        "        }],\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def parse_json(text):\n",
        "    if \"```json\" in text:\n",
        "        text = text.split(\"```json\")[1].split(\"```\")[0]\n",
        "    return text\n",
        "\n",
        "def plot_points(im, text):\n",
        "    img = im.copy()\n",
        "    width, height = img.size\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    colors = ['red', 'green', 'blue', 'yellow', 'orange', 'pink', 'purple'] + additional_colors\n",
        "    \n",
        "    try:\n",
        "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", size=14)\n",
        "    except:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    try:\n",
        "        data = json.loads(parse_json(text))\n",
        "    except:\n",
        "        print(\"Could not parse JSON\")\n",
        "        display(img)\n",
        "        return\n",
        "    \n",
        "    for i, item in enumerate(data):\n",
        "        if \"point_2d\" in item:\n",
        "            point = item[\"point_2d\"]\n",
        "            label = item.get(\"label\", f\"point_{i}\")\n",
        "            color = colors[i % len(colors)]\n",
        "            x, y = int(point[0] / 1000 * width), int(point[1] / 1000 * height)\n",
        "            radius = 5\n",
        "            draw.ellipse([(x - radius, y - radius), (x + radius, y + radius)], fill=color)\n",
        "            draw.text((x + 2*radius, y + 2*radius), label, fill=color, font=font)\n",
        "    \n",
        "    display(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understand Spatial Relationships Between Objects\n",
        "\n",
        "After identifying objects, the model can understand their relative spatial positions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = \"../assets/spatial_understanding/spatio_case1.jpg\"\n",
        "prompt = \"\"\"Which object, in relation to your current position, holds the farthest placement in the image?\n",
        "Answer options:\n",
        "A. chair\n",
        "B. plant\n",
        "C. window\n",
        "D. tv stand.\"\"\"\n",
        "\n",
        "response = inference_with_api(image_path, prompt)\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nAnswer:\", response)\n",
        "\n",
        "img = Image.open(image_path)\n",
        "display(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Perceive Object Affordances\n",
        "\n",
        "The model can understand what actions are enabled by specific parts of objects or empty space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = \"../assets/spatial_understanding/spatio_case2_aff.png\"\n",
        "prompt = \"Locate the free space on the white table on the right in this image. Output the point coordinates in JSON format.\"\n",
        "\n",
        "response = inference_with_api(image_path, prompt)\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nAnswer:\", response)\n",
        "\n",
        "plot_points(Image.open(image_path), response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = \"../assets/spatial_understanding/spatio_case2_aff2.png\"\n",
        "prompt = \"Can the speaker fit behind the guitar?\"\n",
        "\n",
        "response = inference_with_api(image_path, prompt)\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nAnswer:\", response)\n",
        "\n",
        "img = Image.open(image_path)\n",
        "img = img.resize((img.width//4, img.height//4))\n",
        "display(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Integrate Spatial Reasoning and Action Planning\n",
        "\n",
        "The model can synthesize spatial relationships and affordances to select correct actions, reasoning like an embodied agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = \"../assets/spatial_understanding/spatio_case2_plan.png\"\n",
        "prompt = \"What color arrow should the robot follow to move the apple in between the green can and the orange? Choices: A. Red. B. Blue. C. Green. D. Orange.\"\n",
        "\n",
        "response = inference_with_api(image_path, prompt)\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nAnswer:\", response)\n",
        "\n",
        "img = Image.open(image_path)\n",
        "display(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = \"../assets/spatial_understanding/spatio_case2_plan2.png\"\n",
        "prompt = \"Which motion can help change the coffee pod? Choices: A. A. B. B. C. C. D. D.\"\n",
        "\n",
        "response = inference_with_api(image_path, prompt)\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nAnswer:\", response)\n",
        "\n",
        "img = Image.open(image_path)\n",
        "display(img)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}