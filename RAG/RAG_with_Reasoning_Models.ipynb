{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Reasoning Models\n",
    "\n",
    "Author: [Zain Hasan](https://x.com/zainhasan6)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdJJyFNcgNhp"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook we will use a reasoning model to answer questions based on a set of context.\n",
    "\n",
    "The interesting part is that the reasoning model will not only answer the question but also provide a justification for the answer.\n",
    "\n",
    "Additionally if no relevant context is found, the model will say that it does not know the answer and provide a justification for this in the thinking tokens!\n",
    "\n",
    "The notebook is structured as follows:\n",
    "\n",
    "- Install the relevant libraries\n",
    "- Download the full document for SB1047  \n",
    "- Process and chunk the document\n",
    "- Embed and index the chunks\n",
    "- Retrieve the top 5 chunks\n",
    "- Call the generative model with the retrieved chunks and the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvgfJHr4Z8tQ",
    "outputId": "15456b74-ed09-4950-f490-48039ac3d81b"
   },
   "outputs": [],
   "source": [
    "!pip install -qU together beautifulsoup4 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from together import Together\n",
    "\n",
    "# Paste in your Together AI API Key or load it\n",
    "\n",
    "client = Together(api_key = os.environ.get(\"TOGETHER_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Full Document for SB1047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJtvFPIAbZVr",
    "outputId": "ea5ba13c-95f5-499f-a733-967eb7820db3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California-2023-SB1047-Amended\n",
      "                Amended\n",
      "               IN \n",
      "                Senate\n",
      "               May 16, 2024\n",
      "                Amended\n",
      "               IN \n",
      "                Senate\n",
      "               April 30, 2024\n",
      "                Amended\n",
      "               IN \n",
      "                Senate\n",
      "               April 16, 2024\n",
      "                Amended\n",
      "               IN \n",
      "                Senate\n",
      "               April 08, 2024\n",
      "                Amended\n",
      "               IN \n",
      "                Senate\n",
      "               March 20, 2024\n",
      "                    CALIFORNIA LEGISLATURE—\n",
      "                    2023–2024 REGULAR SESSION\n",
      "                Senate Bill\n",
      "              No. 1047Introduced by Senator Wiener(Coauthors: Senators Roth, Rubio, and Stern)February 07, 2024An act to add Chapter 22.6 (commencing with Section 22602) to Division 8 of the Business and Professions Code, and to add Sections 11547.6 and 11547.7 to the Government Code, relating to artificial intelligence.LEGISLATIVE COUNSEL'S DIGESTSB 1047, as amended, Wi\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_legiscan_text(url):\n",
    "    \"\"\"\n",
    "    Fetches and returns the text content from a given LegiScan URL.\n",
    "    Args:\n",
    "        url (str): The URL of the LegiScan page to fetch.\n",
    "    Returns:\n",
    "        str: The text content of the page.\n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If there is an issue with the HTTP request.\n",
    "    \"\"\"\n",
    "    # Basic headers to mimic a browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Get text content\n",
    "    content = soup.get_text()\n",
    "\n",
    "    return content\n",
    "\n",
    "url = \"https://legiscan.com/CA/text/SB1047/id/2999979/California-2023-SB1047-Amended.html\"\n",
    "text = get_legiscan_text(url)\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing and Chunking\n",
    "\n",
    "We will RAG over the recent [**SB1047**](https://legiscan.com/CA/text/SB1047/id/2999979/California-2023-SB1047-Amended.html) bill in California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get away with naive fixed sized chunking as the context generation will add meaning to these chunks\n",
    "\n",
    "def create_chunks(document, chunk_size=300, overlap=50):\n",
    "    return [document[i : i + chunk_size] for i in range(0, len(document), chunk_size - overlap)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: California-2023-SB1047-Amended\n",
      "                Amended\n",
      "               IN \n",
      "                Senate\n",
      "               May 16, 2024\n",
      "                Amended\n",
      "               IN \n",
      "                Senate\n",
      "               April 30, 2024\n",
      "                Amended\n",
      "               IN \n",
      "                Senate\n",
      "               April 16, 2024\n",
      "                Amended\n",
      "         \n",
      "Chunk 2: , 2024\n",
      "                Amended\n",
      "               IN \n",
      "                Senate\n",
      "               April 08, 2024\n",
      "                Amended\n",
      "               IN \n",
      "                Senate\n",
      "               March 20, 2024\n",
      "                    CALIFORNIA LEGISLATURE—\n",
      "                    2023–2024 REGULAR SESSION\n",
      "                Senate Bill\n",
      "              No. 1047Introduced \n",
      "Chunk 3: e Bill\n",
      "              No. 1047Introduced by Senator Wiener(Coauthors: Senators Roth, Rubio, and Stern)February 07, 2024An act to add Chapter 22.6 (commencing with Section 22602) to Division 8 of the Business and Professions Code, and to add Sections 11547.6 and 11547.7 to the Government Code, relating to artificial intelligence.LEGISLATIVE COUNSEL'S\n"
     ]
    }
   ],
   "source": [
    "chunks = create_chunks(text, chunk_size=350, overlap=40)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i + 1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Indexing and Embedding Generation\n",
    "\n",
    "We will now use `bge-large-en-v1.5` to embed the augmented chunks above into a vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings from Together python library.\n",
    "\n",
    "    Args:\n",
    "        input_texts: a list of string input texts.\n",
    "        model_api_string: str. An API string for a specific embedding model of your choice.\n",
    "\n",
    "    Returns:\n",
    "        embeddings_list: a list of embeddings. Each element corresponds to the each input text.\n",
    "    \"\"\"\n",
    "    outputs = client.embeddings.create(\n",
    "        input=input_texts,\n",
    "        model=model_api_string,\n",
    "    )\n",
    "    return np.array([x.embedding for x in outputs.data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = generate_embeddings(list(chunks), \"BAAI/bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each vector is 1024 dimensional\n",
    "\n",
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the vector embeddings for the query\n",
    "query = \"what is the maximum allowable floating point operation per second this bill allows?\"\n",
    "\n",
    "query_embedding = generate_embeddings([query], 'BAAI/bge-large-en-v1.5')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between the query embedding and each movie embedding\n",
    "\n",
    "dot_product = np.dot(query_embedding, np.array(embeddings).T)\n",
    "query_norm = np.linalg.norm(query_embedding)\n",
    "embeddings_norm = np.linalg.norm(embeddings, axis=1)\n",
    "similarity_scores = dot_product / (query_norm * embeddings_norm)\n",
    "indices = np.argsort(-similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30,   2,  34, 135,  33])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_indices = indices[:5]\n",
    "top_5_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' duty exemption.(4)\\xa0Unauthorized use of the hazardous capability of a covered model.(d)\\xa0“Computing cluster” means a set of machines transitively connected by data center networking of over 100 gigabits per second that has a theoretical maximum computing capacity of at least 10^20 integer or floating-point operations per second and can be used for t',\n",
       " \"e Bill\\n              No. 1047Introduced by Senator Wiener(Coauthors: Senators Roth, Rubio, and Stern)February\\xa007,\\xa02024An act to add Chapter 22.6 (commencing with Section 22602) to Division 8 of the Business and Professions Code, and to add Sections 11547.6 and 11547.7 to the Government Code, relating to artificial intelligence.LEGISLATIVE COUNSEL'S\",\n",
       " 'ficial intelligence model was trained using a quantity of computing power sufficiently large that it could reasonably be expected to have similar or greater performance as an artificial intelligence model trained using a quantity of computing power greater than 10^26 integer or floating-point operations in 2024 as assessed using benchmarks commonly',\n",
       " 'n by the Legislature, for purposes of carrying out the provisions of this section.SEC. 5.\\xa0Section 11547.7 is added to the Government Code, to read:11547.7.\\xa0(a)\\xa0The Department of Technology shall commission consultants, pursuant to subdivision (b), to create a public cloud computing cluster, to be known as CalCompute, with the primary focus of condu',\n",
       " 't may possess hazardous capabilities.(f)\\xa0“Covered model” means an artificial intelligence model that meets either of the following criteria:(1)\\xa0The artificial\\n\\t\\t\\t\\t\\t\\t  intelligence model was trained using a quantity of computing power greater than 10^26 integer or floating-point\\n\\t\\t\\t\\t\\t\\t  operations.(2)\\xa0The artificial intelligence model was trained us']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_chunks = [chunks[index] for index in indices][:5]\n",
    "\n",
    "top_5_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compact function to retrieve the top k chunks\n",
    "\n",
    "def vector_retrieval(query: str, top_k: int = 5, vector_index: np.ndarray = None, chunks: List[str] = None) -> List[int]:\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most similar items from an index based on a query.\n",
    "    Args:\n",
    "        query (str): The query string to search for.\n",
    "        top_k (int, optional): The number of top similar items to retrieve. Defaults to 5.\n",
    "        index (np.ndarray, optional): The index array containing embeddings to search against. Defaults to None.\n",
    "    Returns:\n",
    "        List[int]: A list of indices corresponding to the top-k most similar items in the index.\n",
    "    \"\"\"\n",
    "\n",
    "    query_embedding = generate_embeddings([query], 'BAAI/bge-large-en-v1.5')[0]\n",
    "    \n",
    "    \n",
    "    dot_product = np.dot(query_embedding, np.array(vector_index).T)\n",
    "    query_norm = np.linalg.norm(query_embedding)\n",
    "    vector_index_norm = np.linalg.norm(vector_index, axis=1)\n",
    "    \n",
    "    similarity_scores = dot_product / (query_norm * vector_index_norm)\n",
    "\n",
    "    return [chunks[index] for index in np.argsort(-similarity_scores)[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' duty exemption.(4)\\xa0Unauthorized use of the hazardous capability of a covered model.(d)\\xa0“Computing cluster” means a set of machines transitively connected by data center networking of over 100 gigabits per second that has a theoretical maximum computing capacity of at least 10^20 integer or floating-point operations per second and can be used for t',\n",
       " \"e Bill\\n              No. 1047Introduced by Senator Wiener(Coauthors: Senators Roth, Rubio, and Stern)February\\xa007,\\xa02024An act to add Chapter 22.6 (commencing with Section 22602) to Division 8 of the Business and Professions Code, and to add Sections 11547.6 and 11547.7 to the Government Code, relating to artificial intelligence.LEGISLATIVE COUNSEL'S\",\n",
       " 'ficial intelligence model was trained using a quantity of computing power sufficiently large that it could reasonably be expected to have similar or greater performance as an artificial intelligence model trained using a quantity of computing power greater than 10^26 integer or floating-point operations in 2024 as assessed using benchmarks commonly',\n",
       " 'n by the Legislature, for purposes of carrying out the provisions of this section.SEC. 5.\\xa0Section 11547.7 is added to the Government Code, to read:11547.7.\\xa0(a)\\xa0The Department of Technology shall commission consultants, pursuant to subdivision (b), to create a public cloud computing cluster, to be known as CalCompute, with the primary focus of condu',\n",
       " 't may possess hazardous capabilities.(f)\\xa0“Covered model” means an artificial intelligence model that meets either of the following criteria:(1)\\xa0The artificial\\n\\t\\t\\t\\t\\t\\t  intelligence model was trained using a quantity of computing power greater than 10^26 integer or floating-point\\n\\t\\t\\t\\t\\t\\t  operations.(2)\\xa0The artificial intelligence model was trained us']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_retrieval(query = \"what is the maximum allowable floating point operation per second this bill allows?\", top_k = 5, vector_index = embeddings, chunks = chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a way to retrieve from the vector index given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_chunks = vector_retrieval(query = \"what is the maximum allowable floating point operation per second this bill allows?\", top_k = 5, vector_index = embeddings, chunks = chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context 1:  duty exemption.(4) Unauthorized use of the hazardous capability of a covered model.(d) “Computing cluster” means a set of machines transitively connected by data center networking of over 100 gigabits per second that has a theoretical maximum computing capacity of at least 10^20 integer or floating-point operations per second and can be used for t\n",
      "Context 2: e Bill\n",
      "              No. 1047Introduced by Senator Wiener(Coauthors: Senators Roth, Rubio, and Stern)February 07, 2024An act to add Chapter 22.6 (commencing with Section 22602) to Division 8 of the Business and Professions Code, and to add Sections 11547.6 and 11547.7 to the Government Code, relating to artificial intelligence.LEGISLATIVE COUNSEL'S\n",
      "Context 3: ficial intelligence model was trained using a quantity of computing power sufficiently large that it could reasonably be expected to have similar or greater performance as an artificial intelligence model trained using a quantity of computing power greater than 10^26 integer or floating-point operations in 2024 as assessed using benchmarks commonly\n",
      "Context 4: n by the Legislature, for purposes of carrying out the provisions of this section.SEC. 5. Section 11547.7 is added to the Government Code, to read:11547.7. (a) The Department of Technology shall commission consultants, pursuant to subdivision (b), to create a public cloud computing cluster, to be known as CalCompute, with the primary focus of condu\n",
      "Context 5: t may possess hazardous capabilities.(f) “Covered model” means an artificial intelligence model that meets either of the following criteria:(1) The artificial\n",
      "\t\t\t\t\t\t  intelligence model was trained using a quantity of computing power greater than 10^26 integer or floating-point\n",
      "\t\t\t\t\t\t  operations.(2) The artificial intelligence model was trained us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets add the top 5 documents to a string\n",
    "\n",
    "formatted_chunks = ''\n",
    "\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    formatted_chunks += f\"Context {i+1}: {chunk}\\n\"\n",
    "\n",
    "print(formatted_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Call Reasoning Generative Model - DeepSeek R1\n",
    "\n",
    "We will pass the finalized 5 chunks into an LLM to get our final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's tackle this question. The user is asking for the maximum allowable floating point operations per second (FLOPs) allowed for model training under the bill mentioned. The answer needs to be based on the provided contexts, and each claim must have a source from those contexts.\n",
      "\n",
      "First, I'll go through each context to find relevant mentions of FLOPs. \n",
      "\n",
      "Context 1 talks about a \"computing cluster\" defined as having a theoretical maximum capacity of at least 10^20 integer or floating-point operations per second. But this seems to be about the cluster's capacity, not the training limit. \n",
      "\n",
      "Context 2 is the bill introduction, no numbers here. \n",
      "\n",
      "Context 3 mentions training with computing power greater than 10^26 FLOPs in 2024. This looks like a threshold for determining if a model is covered. \n",
      "\n",
      "Context 4 discusses CalCompute, but no FLOPs number. \n",
      "\n",
      "Context 5 defines a \"covered model\" under two criteria. The first is training with more than 10^26 FLOPs. The second is similar performance to models trained with more than 10^26 FLOPs. \n",
      "\n",
      "Putting this together, the bill sets 10^26 FLOPs as the threshold. If a model is trained above this, it's a covered model and subject to regulations. So the maximum allowable without being a covered model would be just under 10^26. The answer should state that models trained with more than 10^26 are covered, hence the allowable maximum is 10^26. The sources are Context 3 and 5.\n",
      "</think>\n",
      "\n",
      "The maximum allowable floating point operations per second (FLOPs) for model training under the bill is **10^26 FLOPs**. Models trained with computing power exceeding this threshold are classified as \"covered models\" and subject to regulation. \n",
      "\n",
      "**Sources:**\n",
      "- Context 3 explicitly states that a model trained with \"greater than 10^26 integer or floating-point operations\" in 2024 qualifies as a covered model.  \n",
      "- Context 5 reiterates this threshold, defining a covered model as one trained with \"greater than 10^26 integer or floating-point operations\" (subdivision (f)(1)).  \n",
      "\n",
      "Thus, the bill prohibits training models above 10^26 FLOPs without compliance (§11547.7 and §22602)."
     ]
    }
   ],
   "source": [
    "query = \"What is the maximum allowable floating point operation per second this bill allows for model training?\"\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "Answer the question: {query}. \n",
    "IMPORTANT RULE: Use the information provided to answer the question. For each claim in the answer provide a source from the information provided. \n",
    "Here is relevant information: {formatted_chunks} \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n",
    "      {\"role\": \"user\", \"content\": PROMPT.format(query=query, formatted_chunks=formatted_chunks)},\n",
    "    ],\n",
    "      stream=True,\n",
    ")\n",
    "\n",
    "response = ''\n",
    "\n",
    "for chunk in stream:\n",
    "  response += chunk.choices[0].delta.content or \"\"\n",
    "  print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if the retreived chunks are not relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's tackle this question. The user is asking for the circumference of the moon. But wait, the provided contexts don't seem to mention anything about the moon, its size, or circumference. Let me check each context again to be sure.\n",
      "\n",
      "Context 1 talks about duty exemptions and computing clusters, mentioning 10^20 operations. Context 2 is about a bill related to AI. Context 3 again discusses AI models and computing power of 10^26 operations. Context 4 mentions creating a cloud computing cluster called CalCompute. Context 5 defines a covered model based on computing power. \n",
      "\n",
      "None of these contexts provide any data about the moon's physical characteristics. The user specified to use only the given information and cite sources from the provided contexts. Since there's no relevant information here about the moon's circumference, I need to state that the answer isn't available in the given contexts. I should also list the contexts checked to show I reviewed them all.\n",
      "</think>\n",
      "\n",
      "The question about the circumference of the moon cannot be answered using the provided information. The contexts focus on legislative bills, definitions of computing clusters, and regulations for artificial intelligence models (e.g., computing power thresholds like 10^26 operations in Context 3 and 5). There is no mention of astronomical data, lunar measurements, or related topics in any of the provided contexts.  \n",
      "\n",
      "**Sources Checked:**  \n",
      "- Context 1: Computing clusters and hazardous capability definitions.  \n",
      "- Context 2: Legislative bill details (SB 1047).  \n",
      "- Context 3: AI model training benchmarks (10^26 operations).  \n",
      "- Context 4: CalCompute cloud cluster creation.  \n",
      "- Context 5: Criteria for \"covered models\" (computing power thresholds).  \n",
      "\n",
      "No relevant lunar data is included in these sources."
     ]
    }
   ],
   "source": [
    "query = \"What is the circumference of the moon?\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n",
    "      {\"role\": \"user\", \"content\": PROMPT.format(query=query, formatted_chunks=formatted_chunks)},\n",
    "    ],\n",
    "      stream=True,\n",
    ")\n",
    "\n",
    "response = ''\n",
    "\n",
    "for chunk in stream:\n",
    "  response += chunk.choices[0].delta.content or \"\"\n",
    "  print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
