{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bRskMbY8L-YQ",
   "metadata": {
    "id": "bRskMbY8L-YQ"
   },
   "source": [
    "## Finetuning and Inference using Low-Rank Adaptations(LoRA)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20d3d9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we demonstrate how to perform LoRA finetuning and inference using the Together AI API!\n",
    "\n",
    "LoRA is a very useful fine-tuning technique, here is how it works: \n",
    "\n",
    "Instead of updating all model parameters(blue parameters in the figure below) during fine-tuning (which is computationally expensive), LoRA adds a small ammount of trainable parameters (orange matrices A and B) alongside the original model weights. \n",
    "\n",
    "These smaller matrices get updated during the fine-tuning phase and get added to the main weights. This dramatically reduces the time it takes to fine-tune the model and the compute resources required while maintaining good performance.\n",
    "\n",
    "When paired with fast LoRA inference you can swap betweeen multiple LoRA adapters and run inference with different fine-tunes - all while using the same base model!\n",
    "\n",
    "In this notebook we demonstrate:\n",
    "1. How to perform LoRA fine-tuning on Together AI\n",
    "2. How to perform LoRA inference on the trained model\n",
    "3. Hwo to swap and perform inference using various LoRA fine-tunes!\n",
    "\n",
    "\n",
    "<img src=\"images/lora.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3b2e7",
   "metadata": {},
   "source": [
    "## Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Ula3Kb-MMLgG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ula3Kb-MMLgG",
    "outputId": "16a0dfdd-e5f1-4321-f25c-278be065140a"
   },
   "outputs": [],
   "source": [
    "!pip install -qU together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h-9strcjMki_",
   "metadata": {
    "id": "h-9strcjMki_"
   },
   "outputs": [],
   "source": [
    "from together import Together\n",
    "import os\n",
    "\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "client = Together(api_key = TOGEHER_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gp6dFzt-OK5p",
   "metadata": {
    "id": "gp6dFzt-OK5p"
   },
   "source": [
    "## Perform LoRA Fine-tune\n",
    "\n",
    "Below we upload a file that can be used to fine-tune Llama 3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "jxEdLIyNMCet",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxEdLIyNMCet",
    "outputId": "7d1e97f0-6e24-4264-d3fc-11726a01f023"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file small_coqa_10.jsonl: 100%|██████████| 33.4k/33.4k [00:00<00:00, 53.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='file-e7c382f4-c2ed-4ef7-a9a7-269278c799f4' object=<ObjectType.File: 'file'> created_at=1734472962 type=None purpose=<FilePurpose.FineTune: 'fine-tune'> filename='small_coqa_10.jsonl' bytes=0 line_count=0 processed=False FileType='jsonl'\n"
     ]
    }
   ],
   "source": [
    "# Upload dataset to Together AI\n",
    "\n",
    "train_file_resp = client.files.upload(\"datasets/small_coqa_10.jsonl\", check=True)\n",
    "print(train_file_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4wsSmVCxMUm4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wsSmVCxMUm4",
    "outputId": "609b6796-30ee-40fd-bbb1-0133d6fd020e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message='Starting from together>=1.3.0, the default batch size is set to the maximum allowed value for each model.'\n",
      "message='Starting from together>=1.3.0, the default batch size is set to the maximum allowed value for each model.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft-8f5d2d92-6616-491c-8891-218043c99c3a\n"
     ]
    }
   ],
   "source": [
    "ft_resp = client.fine_tuning.create(\n",
    "    training_file = train_file_resp.id,\n",
    "    model = 'meta-llama/Meta-Llama-3.1-8B-Instruct-Reference',\n",
    "    train_on_inputs= \"auto\",\n",
    "    n_epochs = 3,\n",
    "    n_checkpoints = 1,\n",
    "    wandb_api_key = WANDB_API_KEY,\n",
    "    lora = True,\n",
    "    warmup_ratio=0,\n",
    "    learning_rate = 1e-5,\n",
    "    suffix = 'my-lora-finetune',\n",
    ")\n",
    "\n",
    "print(ft_resp.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "yKbK8rh6NKlt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yKbK8rh6NKlt",
    "outputId": "6b351651-480d-49d0-a705-e95778677626"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zainhas/Meta-Llama-3.1-8B-Instruct-Reference-my-lora-finetune-cef0d8dd'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output model name\n",
    "ft_resp.output_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039cd41f",
   "metadata": {},
   "source": [
    "## LoRA Inference\n",
    "\n",
    "Once the fine-tuning job finishes you can directly perform inference.\n",
    "\n",
    "To check the status of the finetuning job you can check the `Jobs` page: https://api.together.ai/jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd4d8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = ft_resp.output_name\n",
    "user_prompt = \"What is the capital of the France?\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=124,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qs4-HPVcQb3m",
   "metadata": {
    "id": "qs4-HPVcQb3m"
   },
   "source": [
    "## Swap between different LoRA adapters on the go!\n",
    "\n",
    "If you have trained multiple LoRA adapters you can loop through and use them all. This can be quite useful to evaluate multiple fine-tunes togehter.\n",
    "\n",
    "The first time you run LoRA inference with an adapter it might take some time - however following inference calls to the same LoRA adapter will be alot faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2z--ZqDyNKeV",
   "metadata": {
    "id": "2z--ZqDyNKeV"
   },
   "outputs": [],
   "source": [
    "# List of LoRA fine-tunes\n",
    "\n",
    "LoRA_adapters = [\"zainhas/Meta-Llama-3.1-8B-Instruct-Reference-my-demo-finetune-4224205a\",\n",
    "                 \"zainhas/Meta-Llama-3.1-8B-Instruct-Reference-30b975fd\",\n",
    "                 \"zainhas/Meta-Llama-3.1-8B-Instruct-Reference-f9ef93c8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd2715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from zainhas/Meta-Llama-3.1-8B-Instruct-Reference-my-demo-finetune-4224205a:\n",
      "\n",
      "Here is a short haiku about elephants:\n",
      "\n",
      "Gray giants roam free\n",
      "Trunk entwined in ancient dance\n",
      "Wisdom's gentle soul\n",
      "\n",
      "########################################################################################################################\n",
      "\n",
      "Response from zainhas/Meta-Llama-3.1-8B-Instruct-Reference-30b975fd:\n",
      "\n",
      "Here is a short haiku about elephants:\n",
      "\n",
      "Gray giants roam free\n",
      "Tusks lift spirits high above\n",
      "Nature's gentle king\n",
      "\n",
      "########################################################################################################################\n",
      "\n",
      "Response from zainhas/Meta-Llama-3.1-8B-Instruct-Reference-f9ef93c8:\n",
      "\n",
      "Tusks gently unfold\n",
      "Memories in wrinkled grey\n",
      "Wisdom's ancient steps\n",
      "\n",
      "########################################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over different LoRA fine-tunes and call with same query\n",
    "\n",
    "for adapter in LoRA_adapters:\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "    model = adapter,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a short haiku about elephants.\",\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=124,\n",
    "    temperature=0.7,\n",
    "    )\n",
    "\n",
    "    print(f\"Response from {adapter}:\\n\")\n",
    "\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "    print('\\n'+20*'######'+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d3c40",
   "metadata": {},
   "source": [
    "## Learn more about LoRA Inference\n",
    "\n",
    "You can also bring you own adapters or source LoRA adapters from HugginFace. To learn more refer to the [docs here](https://docs.together.ai/docs/lora-inference)!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
