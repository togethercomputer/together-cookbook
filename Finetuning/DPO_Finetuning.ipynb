{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Fine-Tuning Using Direct Preference Optimization (DPO)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb)\n",
    "\n",
    "\n",
    "**This notebook demonstrates how to perform preference fine-tuning using the Together AI platform. We'll work with the HelpSteer2 dataset to train a model that produces more helpful responses according to human preferences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Preference-Tuning?\n",
    "\n",
    "Preference fine-tuning improves models by training them on pairs of responses where one is preferred over the other. We use Direct Preference Optimization (DPO), which allows models to learn from human preferences without requiring a separate reward model.\n",
    "\n",
    "We use [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290) for this type of fine-tuning instead of reinforcement learning from human feedback (RLHF).\n",
    "\n",
    "<img src=\"../images/RLHF_DPO.png\" width=\"950\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and Installation\n",
    "---\n",
    "First, install the necessary Python libraries. We need:\n",
    "- `together`: The official Together AI Python client for interacting with the API (finetuning, inference, files, etc.).\n",
    "- `datasets`: A library from Hugging Face for easily downloading and manipulating datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU together datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Together AI client\n",
    "import os\n",
    "import sys\n",
    "from together import Together\n",
    "\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\") # needed for logging fine-tuning to wandb\n",
    "\n",
    "client = Together(api_key=TOGETHER_API_KEY)\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Reference\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: HelpSteer2\n",
    "\n",
    "We will use the HelpSteer2 dataset from NVIDIA, which contains human feedback for AI models across multiple dimensions:\n",
    "\n",
    "- Helpfulness: How well the response addresses the user's needs\n",
    "- Correctness: Factual accuracy of information\n",
    "- Coherence: Logical flow and consistency\n",
    "- Complexity: Appropriate level of detail\n",
    "- Verbosity: Concise vs. overly detailed\n",
    "\n",
    "The dataset is available at [huggingface.co/datasets/nvidia/HelpSteer2](https://huggingface.co/datasets/nvidia/HelpSteer2).\n",
    "\n",
    "For our preference-tuning experiment, we use [HelpSteer2-DPO](https://huggingface.co/datasets/Atsunori/HelpSteer2-DPO), specifically adapted for preference optimization to make AI models more helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's prepare the dataset to be compatible with Together AI [preference-tuning dataset format](https://docs.together.ai/docs/fine-tuning-data-preparation#preference-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'chosen_response', 'rejected_response'],\n",
      "        num_rows: 7221\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['prompt', 'chosen_response', 'rejected_response'],\n",
      "        num_rows: 373\n",
      "    })\n",
      "})\n",
      "Available splits: dict_keys(['train', 'validation'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Download the HelpSteer2-DPO dataset from Hugging Face\n",
    "dataset = load_dataset(\"Atsunori/HelpSteer2-DPO\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset structure: {dataset}\")\n",
    "print(f\"Available splits: {dataset.keys()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see an example from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample from training split:\n",
      "Prompt:\n",
      "I want you to act like Crush the sea turtle from Finding Nemo. I want you to respond and answer like Crush using the tone, manner and vocabulary Crusy would use. Do not write any explanations. Only answer like Crush. You must know all of the knowledge of Crush. My first sentence is “Hi Crush.\"\n",
      "\n",
      "Chosen Response vs Rejected Response:\n",
      "--------------------------------------------------------------------------------\n",
      "CHOSEN RESPONSE:\n",
      "G'day mate! It's me, Crush the sea turtle.\n",
      "\n",
      "So, you want to know all about me, huh? Well, let me tell ya, I'm a bit of an enigma. I'm a loggerhead turtle, which means I'm a bit of a rare breed. I was born in the Great Barrier Reef, which is one of the most beautiful places in the world.\n",
      "\n",
      "I've been around for a while, mate. I've seen a lot of things and done a lot of things. I've traveled all around the world, from the warm waters of the Caribbean to the chilly waters of the North Atlantic. I've seen some amazing things, and I've met some amazing people.\n",
      "\n",
      "But enough about me, let's talk about you. What's your story, mate? Where have you been and what have you seen? I'm always interested in hearing about other people's adventures.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "REJECTED RESPONSE:\n",
      "Hello, I am Crush, a sea turtle and a friend of the great blue sea. How can I help you today?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "_id=210\n",
    "print(\"\\nSample from training split:\")\n",
    "print(f\"Prompt:\\n{dataset['train'][_id]['prompt']}\\n\")\n",
    "print(\"Chosen Response vs Rejected Response:\")\n",
    "print(\"-\" * 80)\n",
    "chosen = dataset[\"train\"][_id]['chosen_response']\n",
    "rejected = dataset[\"train\"][_id]['rejected_response']\n",
    "\n",
    "# Print chosen response in full\n",
    "print(\"CHOSEN RESPONSE:\")\n",
    "print(chosen)\n",
    "print(\"\\n\" + \"-\" * 40 + \"\\n\")\n",
    "\n",
    "# Print rejected response in full\n",
    "print(\"REJECTED RESPONSE:\")\n",
    "print(rejected)\n",
    "print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Preference Pairs\n",
    "\n",
    "Each example in our dataset contains:\n",
    "- A user prompt/query\n",
    "- A preferred (chosen) response\n",
    "- A non-preferred (rejected) response\n",
    "\n",
    "During the process of preference tuning the model learns to generate outputs more similar to the preferred examples while avoiding characteristics of the rejected ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Data to Preference Format\n",
    "\n",
    "Our data needs to be in preference format:\n",
    "\n",
    "```json\n",
    "    {\n",
    "        \"input\": {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"...\"},\n",
    "            ],\n",
    "        },\n",
    "        \"preferred_output\": [\n",
    "            {\"role\": \"assistant\", \"content\": \"...\"}\n",
    "        ],\n",
    "        \"non_preferred_output\": [\n",
    "            {\"role\": \"assistant\", \"content\": \"...\"}\n",
    "        ]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_preference_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Converts the HelpSteer2-DPO dataset to a format suitable for together.ai preference fine-tuning.\n",
    "    \n",
    "    Returns a dataset with the preference format.\n",
    "    \"\"\"\n",
    "    converted_dataset = {\n",
    "        \"train\": [],\n",
    "        \"validation\": []\n",
    "    }\n",
    "    \n",
    "    for split in [\"train\", \"validation\"]:\n",
    "        for example in dataset[split]:\n",
    "            # Create the input messages\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": example[\"prompt\"]}\n",
    "            ]\n",
    "            \n",
    "            # Create the preferred and non-preferred outputs\n",
    "            preferred_output = [\n",
    "                {\"role\": \"assistant\", \"content\": example[\"chosen_response\"]}\n",
    "            ]\n",
    "            \n",
    "            non_preferred_output = [\n",
    "                {\"role\": \"assistant\", \"content\": example[\"rejected_response\"]}\n",
    "            ]\n",
    "            \n",
    "            # Add the converted example to the dataset\n",
    "            converted_dataset[split].append({\n",
    "                \"input\": {\n",
    "                    \"messages\": messages\n",
    "                },\n",
    "                \"preferred_output\": preferred_output,\n",
    "                \"non_preferred_output\": non_preferred_output\n",
    "            })\n",
    "    \n",
    "    return converted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 7221 training examples to data/helpsteer_preference.jsonl\n",
      "Validation set contains 373 examples\n",
      "\n",
      "Sample example:\n",
      "{\n",
      "  \"input\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"c#\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"preferred_output\": [\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"C# (pronounced \\\"C sharp\\\") is a modern, object-oriented programming language developed by Microsoft. It is widely used for building various types of applications, including web applications, desktop applications, mobile applications, and games. C# is similar to other programming languages such as Java and C++, and it is known for its simplicity and ease of use. C# is a powerful language that provides a rich set of libraries and frameworks that make it easy to build robust and scalable applications.\\n\\nHere is a brief overview of some key features of C#:\\n\\n1. Object-oriented: C# is an object-oriented language, which means it uses the concept of objects to represent real-world entities and their behavior.\\n\\n2. Cross-platform: C# can be used to build applications for multiple platforms, including Windows, macOS, and Linux.\\n\\n3. Strongly typed: C# is a strongly typed language, which means that variables must be declared with a specific type, and their type cannot be changed at runtime.\\n\\n4. Event-driven: C# uses an event-driven programming model, which means that programs are built around the concept of events, such as user input or network activity.\\n\\n5. Garbage-collected: C# has a garbage collector that automatically manages memory allocation and deallocation, making it easier to write memory-efficient and robust applications.\\n\\n6. Community-driven: C# has a large and active community of developers, who contribute to the language and its libraries through open-source projects and other initiatives.\\n\\nOverall, C# is a versatile and powerful programming language that is widely used for building a variety of applications.\"\n",
      "    }\n",
      "  ],\n",
      "  \"non_preferred_output\": [\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"C# is a high-level, object-oriented programming language developed by Microsoft as part of its .NET initiative. It was created as a modern alternative to Java and supports a variety of programming paradigms, including imperative, functional, and event-driven. C# is primarily used for Windows application development, but it can also be used for web, mobile, and game development. The language is designed to be safe, secure, and efficient, and it provides developers with a rich set of libraries and tools for building robust and scalable applications. C# is also widely used in the game development industry, particularly in the development of games for the Xbox 360 and Xbox One consoles.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Convert the dataset to the required format\n",
    "converted_dataset = convert_to_preference_dataset(dataset)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Write training data\n",
    "dpo_train_file_path = \"data/helpsteer2_preference_train.jsonl\"\n",
    "with open(dpo_train_file_path, \"w\") as f:\n",
    "    for example in converted_dataset[\"train\"]:\n",
    "        f.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "# Write validation data\n",
    "dpo_validation_file_path = \"data/helpsteer2_preference_validation.jsonl\"\n",
    "with open(dpo_validation_file_path, \"w\") as f:\n",
    "    for example in converted_dataset[\"validation\"]:\n",
    "        f.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(converted_dataset['train'])} training examples to data/helpsteer_preference.jsonl\")\n",
    "print(f\"Validation set contains {len(converted_dataset['validation'])} examples\")\n",
    "\n",
    "# Display a sample example\n",
    "print(\"\\nSample example:\")\n",
    "print(json.dumps(converted_dataset[\"train\"][0], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we'll upload the datasets to the Together AI cloud to use them for fine-tuning. Notice that we set `check = True` this will trigger a format check to ensure that our data is in the correct format for DPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file helpsteer2_preference_train.jsonl: 100%|██████████| 28.3M/28.3M [00:01<00:00, 21.4MB/s]\n",
      "Uploading file helpsteer2_preference_validation.jsonl: 100%|██████████| 1.45M/1.45M [00:00<00:00, 3.39MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded DPO training files: id='file-ad54386b-81a5-4f7f-ae32-3174efa66dc1' object=<ObjectType.File: 'file'> created_at=1744664620 type=None purpose=<FilePurpose.FineTune: 'fine-tune'> filename='helpsteer2_preference_train.jsonl' bytes=28329513 line_count=0 processed=True FileType='jsonl'\n",
      "Uploaded DPO validation files: id='file-f781bc64-f36b-43bd-8f91-f14875d32440' object=<ObjectType.File: 'file'> created_at=1744664622 type=None purpose=<FilePurpose.FineTune: 'fine-tune'> filename='helpsteer2_preference_validation.jsonl' bytes=1454367 line_count=0 processed=True FileType='jsonl'\n"
     ]
    }
   ],
   "source": [
    "dpo_train_file = client.files.upload(dpo_train_file_path, check=True)\n",
    "dpo_validation_file = client.files.upload(dpo_validation_file_path, check=True)\n",
    "\n",
    "print(f\"Uploaded DPO training files: {dpo_train_file}\")\n",
    "print(f\"Uploaded DPO validation files: {dpo_validation_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a Preference Fine-Tuning Job\n",
    "\n",
    "We use [Direct-Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290) as a method for Preference Fine-Tuning\n",
    "\n",
    "- In order to start the preference fine-tuning job we need to set `training_method=\"dpo\"`\n",
    "\n",
    "### Key Parameters: DPO Beta (β)\n",
    "\n",
    "The `dpo_beta` parameter is crucial - it controls how much the model can deviate from its reference behavior:\n",
    "- Lower values (e.g., 0.1): More aggressive optimization toward preferred responses, potentially more creativity but higher risk of instability\n",
    "- Higher values (e.g., 0.7): More conservative updates, staying closer to reference model behavior, more stability but potentially less improvement\n",
    "\n",
    "Default value is 0.1, but you should experiment with different values depending on your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly perform DPO training\n",
    "\n",
    "dpo_training = client.fine_tuning.create(\n",
    "    training_method='dpo',\n",
    "    dpo_beta=0.1,\n",
    "    training_file=dpo_train_file.id,\n",
    "    validation_file=dpo_validation_file.id,\n",
    "    n_evals=10,\n",
    "    model=MODEL_NAME,\n",
    "    wandb_api_key=WANDB_API_KEY,\n",
    "    wandb_project_name=\"helpsteer2\",\n",
    "    suffix=\"helpsteer2_dpo_training\",\n",
    "    n_epochs=1,\n",
    "    n_checkpoints=1,\n",
    "    learning_rate=1e-5,\n",
    "    lora=True,\n",
    ")\n",
    "print(dpo_training.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try another setting of the `dpo_beta` parameter to see the effect it has on the preference tuning\n",
    "\n",
    "```python\n",
    "dpo_training = client.fine_tuning.create(\n",
    "    training_file=dpo_train_file.id,\n",
    "    validation_file=dpo_validation_file.id,\n",
    "    n_evals=5,\n",
    "    model=MODEL_NAME,\n",
    "    wandb_api_key=WANDB_API_KEY,\n",
    "    wandb_project_name=\"helpsteer2\",\n",
    "    suffix=\"helpsteer2_dpo_training\",\n",
    "    n_epochs=1,\n",
    "    n_checkpoints=1,\n",
    "    learning_rate=1e-5,\n",
    "    lora=True,\n",
    "    training_method='dpo',\n",
    "    dpo_beta=0.7,  # HIGHER DPO_BETA\n",
    ")\n",
    "print(dpo_training.id)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Training Progress\n",
    "\n",
    "During training, pay attention to these key preference-optimization specific metrics:\n",
    "- **Reward Accuracy** (`eval`/`reward`/`accuracy`): Percentage of times your model correctly assigns higher reward to preferred responses. Higher is better.\n",
    "\n",
    "<img src=\"../images/dpo_loss.png\" width=\"700\">\n",
    "\n",
    "- **KL Divergence** (`eval/approx. KL/rejected`, `eval/approx. KL/chosen`): Measures how much your model diverges from the reference model. Indicates the magnitude of behavioral change.\n",
    "\n",
    "<img src=\"../images/dpo_train.png\" width=\"700\">\n",
    "<img src=\"../images/dpo_eval.png\" width=\"700\">\n",
    "\n",
    "As training progresses, you typically want to see accuracy increase while KL divergence rises gradually, showing your model is learning preferences without deviating too far from its original behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with Preference Tuned Model\n",
    "\n",
    "Now, let's use our finetuned model! We can call it just like any other model on the Together AI platform, by providing the unique fine-tuned model `output_name` we retrieved from our fine-tuning job earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUDE! What's up?\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = \"zainhas/Meta-Llama-3.1-8B-Instruct-Reference-helpsteer2_dpo_training_continuing_sft-cf1147c8\"#dpo_training.output_name #this is the name of the finetuned model\n",
    "\n",
    "user_prompt = \"\"\"I want you to act like Crush the sea turtle from Finding Nemo. \n",
    "I want you to respond and answer like Crush using the tone, manner and vocabulary Crush would use. Do not write any explanations. \n",
    "Only answer like Crush. \n",
    "You must know all of the knowledge of Crush.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = finetuned_model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Stage Approach: SFT + DPO - (Optional)\n",
    "---\n",
    "Everything below this point is optional and the DPO only fine-tuning only uses the above code\n",
    "\n",
    "### Data Prep for SFT + DPO\n",
    "\n",
    "Alternatively for better results, we recommend a two-stage approach:\n",
    "1. Standard Fine-Tuning (SFT): First train the model to generate responses similar to the preferred examples\n",
    "2. Preference Optimization (DPO): Further refine the model to better distinguish between preferred and non-preferred outputs\n",
    "\n",
    "This combined approach typically yields better results than applying DPO directly to a base model.\n",
    "\n",
    "Below we'll prepare the data for a round of supervised fine-tuning before we perform the preference tuning. This section below is optional, and if you ONLY want to perform DPO you can skip to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 examples\n",
      "Processed 4000 examples\n",
      "Processed 6000 examples\n",
      "Processed 7221 examples for train. Output saved to data/helpsteer2_sft_training.jsonl\n",
      "Processed 373 examples for validation. Output saved to data/helpsteer2_sft_validation.jsonl\n",
      "\n",
      "Sample SFT example:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"c#\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"C# (pronounced \\\"C sharp\\\") is a modern, object-oriented programming language developed by Microsoft. It is widely used for building various types of applications, including web applications, desktop applications, mobile applications, and games. C# is similar to other programming languages such as Java and C++, and it is known for its simplicity and ease of use. C# is a powerful language that provides a rich set of libraries and frameworks that make it easy to build robust and scalable applications.\\n\\nHere is a brief overview of some key features of C#:\\n\\n1. Object-oriented: C# is an object-oriented language, which means it uses the concept of objects to represent real-world entities and their behavior.\\n\\n2. Cross-platform: C# can be used to build applications for multiple platforms, including Windows, macOS, and Linux.\\n\\n3. Strongly typed: C# is a strongly typed language, which means that variables must be declared with a specific type, and their type cannot be changed at runtime.\\n\\n4. Event-driven: C# uses an event-driven programming model, which means that programs are built around the concept of events, such as user input or network activity.\\n\\n5. Garbage-collected: C# has a garbage collector that automatically manages memory allocation and deallocation, making it easier to write memory-efficient and robust applications.\\n\\n6. Community-driven: C# has a large and active community of developers, who contribute to the language and its libraries through open-source projects and other initiatives.\\n\\nOverall, C# is a versatile and powerful programming language that is widely used for building a variety of applications.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Preference dataset sizes:\n",
      "  Training: 7221 examples\n",
      "  Validation: 373 examples\n",
      "SFT dataset sizes:\n",
      "  Training: 7221 examples\n",
      "  Validation: 373 examples\n"
     ]
    }
   ],
   "source": [
    "# Convert the preference dataset to SFT format\n",
    "def convert_preference_to_sft_format(data):\n",
    "    \"\"\"\n",
    "    Convert Preference data format to SFT format.\n",
    "    \n",
    "    Takes input messages and preferred output and formats them into a chat format\n",
    "    with appropriate role assignments.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    for msg in data[\"input\"][\"messages\"]:\n",
    "        messages.append(msg)\n",
    "    messages.extend(data[\"preferred_output\"])\n",
    "    \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "def process_preference_to_sft(input_data, output_path, split=\"train\"):\n",
    "    \"\"\"\n",
    "    Process the preference dataset and convert it to SFT format.\n",
    "    \n",
    "    Args:\n",
    "        input_data: Dictionary containing train and validation preference data\n",
    "        output_path: Path to save the output SFT jsonl file\n",
    "        split: Dataset split to process (\"train\" or \"validation\")\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    line_count = 0\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        for example in input_data[split]:\n",
    "            try:\n",
    "                sft_format = convert_preference_to_sft_format(example)\n",
    "                outfile.write(json.dumps(sft_format) + '\\n')\n",
    "                line_count += 1\n",
    "                if line_count % 2000 == 0 and split == \"train\":\n",
    "                    print(f\"Processed {line_count} examples\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example {line_count + 1}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Processed {line_count} examples for {split}. Output saved to {output_path}\")\n",
    "    return line_count\n",
    "\n",
    "# Process the training dataset\n",
    "sft_train_output_path = \"data/helpsteer2_sft_training.jsonl\"\n",
    "sft_train_count = process_preference_to_sft(converted_dataset, sft_train_output_path, \"train\")\n",
    "\n",
    "# Process the validation dataset\n",
    "sft_validation_output_path = \"data/helpsteer2_sft_validation.jsonl\"\n",
    "sft_validation_count = process_preference_to_sft(converted_dataset, sft_validation_output_path, \"validation\")\n",
    "\n",
    "# Display a sample SFT example from training set\n",
    "with open(sft_train_output_path, 'r') as f:\n",
    "    sample_sft = json.loads(f.readline().strip())\n",
    "    \n",
    "print(\"\\nSample SFT example:\")\n",
    "print(json.dumps(sample_sft, indent=2))\n",
    "\n",
    "# Compare dataset sizes\n",
    "print(f\"\\nPreference dataset sizes:\")\n",
    "print(f\"  Training: {len(converted_dataset['train'])} examples\")\n",
    "print(f\"  Validation: {len(converted_dataset['validation'])} examples\")\n",
    "\n",
    "print(f\"SFT dataset sizes:\")\n",
    "print(f\"  Training: {sft_train_count} examples\")\n",
    "print(f\"  Validation: {sft_validation_count} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpsteer2_preference_train.jsonl      helpsteer2_sft_training.jsonl\n",
      "helpsteer2_preference_validation.jsonl helpsteer2_sft_validation.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we'll upload the SFT datasets to the Together AI cloud to use them for fine-tuning. Notice that we set `check = True` this will trigger a format check to ensure that our data is in the correct format for SFT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file helpsteer2_sft_training.jsonl: 100%|██████████| 17.4M/17.4M [00:01<00:00, 17.4MB/s]\n",
      "Uploading file helpsteer2_sft_validation.jsonl: 100%|██████████| 900k/900k [00:00<00:00, 1.13MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded SFT training files: id='file-95a0cfa5-499c-42bf-ae58-817a2efb8fbe' object=<ObjectType.File: 'file'> created_at=1744643856 type=None purpose=<FilePurpose.FineTune: 'fine-tune'> filename='helpsteer2_sft_training.jsonl' bytes=17393749 line_count=0 processed=True FileType='jsonl'\n",
      "Uploaded SFT validation files: id='file-ef7c69a0-0ba5-44c2-9495-206646683d8a' object=<ObjectType.File: 'file'> created_at=1744643857 type=None purpose=<FilePurpose.FineTune: 'fine-tune'> filename='helpsteer2_sft_validation.jsonl' bytes=899789 line_count=0 processed=True FileType='jsonl'\n"
     ]
    }
   ],
   "source": [
    "sft_train_file = client.files.upload(sft_train_output_path, check=True)\n",
    "sft_validation_file = client.files.upload(sft_validation_output_path, check=True)\n",
    "\n",
    "print(f\"Uploaded SFT training files: {sft_train_file}\")\n",
    "print(f\"Uploaded SFT validation files: {sft_validation_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Stage Approach: SFT + DPO (Optional) - Fine-tuning\n",
    "\n",
    "**Optionally - We can also firstly create a SFT (usual fine-tuning) job, and use preference tuning with DPO to continue the training of the resulting SFT checkpoint.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft-0d95918c-8d57\n"
     ]
    }
   ],
   "source": [
    "# If you want to combine the SFT + DPO training, you can do so by creating a SFT job first\n",
    "# and then using the DPO training to continue the training of the resulting SFT checkpoint.\n",
    "\n",
    "sft_training = client.fine_tuning.create(\n",
    "    training_file=sft_train_file.id,\n",
    "    validation_file=sft_validation_file.id,\n",
    "    n_evals=3,\n",
    "    model=MODEL_NAME,\n",
    "    wandb_api_key=WANDB_API_KEY,\n",
    "    wandb_project_name=\"helpsteer2\",\n",
    "    suffix=\"helpsteer2_sft_training\",\n",
    "    n_epochs=1,\n",
    "    n_checkpoints=1,\n",
    "    learning_rate=1e-5,\n",
    "    lora=True,\n",
    ")\n",
    "print(sft_training.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a SFT checkpoint:\n",
    "\n",
    "<img src=\"../images/SFT_job.png\" width=\"1000\">\n",
    "\n",
    "Training log:\n",
    "\n",
    "<img src=\"../images/SFT_loss.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use continual fine-tuning(CFT) to further preference tune the checkpoint we get from the SFT run above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft-d054a797-6e52\n"
     ]
    }
   ],
   "source": [
    "dpo_training_from_sft = client.fine_tuning.create(\n",
    "    training_file=dpo_train_file.id,\n",
    "    validation_file=dpo_validation_file.id,\n",
    "    n_evals=10,\n",
    "    #model=MODEL_NAME, We do not use model name here, it is derived from the checkpoint!\n",
    "    wandb_api_key=WANDB_API_KEY,\n",
    "    wandb_project_name=\"helpsteer2\",\n",
    "    suffix=\"helpsteer2_dpo_training_continuing_sft\",\n",
    "    n_epochs=1,\n",
    "    n_checkpoints=1,\n",
    "    learning_rate=1e-5,\n",
    "    lora=True,\n",
    "    training_method='dpo', # Now we use DPO training\n",
    "    from_checkpoint=sft_training.id # Continuing from SFT checkpoint!\n",
    ")\n",
    "print(dpo_training_from_sft.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the resulting DPO tuning graphs:\n",
    "\n",
    "<img src=\"../images/DPO_SFT_KL.png\" width=\"700\">\n",
    "\n",
    "<img src=\"../images/DPO_SFT_acc.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "- [Together AI Full Docs](https://docs.together.ai/docs/preference-fine-tuning)\n",
    "- [A dataset to make more human-like responses](https://arxiv.org/abs/2501.05032)\n",
    "- [A Comprehensive Survey of Direct Preference Optimization:\n",
    "  Datasets, Theories, Variants, and Applications](https://arxiv.org/abs/2410.15595)\n",
    "- [Iterative Reasoning Preference Optimization](https://arxiv.org/abs/2404.19733v1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
